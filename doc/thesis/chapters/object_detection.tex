	\chapter{Detecting \ac{EWFO} on \ac{MAV}}
	\label{sec:object_detection}
	
	\ac{CNN}-based Object Detectors offer the advantage that they can be trained on any object given labelled training data is available. However, typically the architectures of \acp{CNN} are optimized for solid feature rich objects, such as planes, cars or faces. The predominant metrics optimized are precision and recall rather than inference speed. This work investigates the detection of \ac{EWFO} on \acp{MAV} as part of a control loop. Hence, inference speed and computational requirements can be more important than detection performance. Furthermore, \acp{EWFO} differ substantially from objects usually used in Object Detection. This chapter investigates the detection of \acp{EWFO} with a \acp{CNN}. An off-the-shelve \acp{CNN}-based detector is optimized for the detection of \acp{EWFO}. Furthermore, the trade-off in detection performance and inference speed is studied in the example of the hardware used in this thesis. The research questions are summarized in the following:
	
\begin{enumerate}
	\item[\textbf{RQ2}]How can the architecture be optimized to detect \acp{EWFO} more efficiently?
	\item[\textbf{RQ3}]What are the trade-offs in detection performance and inference time when the detection model for \acp{EWFO} is deployed on a \ac{MAV}?
%	\item[\textbf{RQ4}]Can the gained insights be used to build a lightweight and robust detection model for racing gates in the \ac{IROS} Autonomous Drone Race?
\end{enumerate}

	\section{Methodology}
		
	This work uses a typical one-stage detector with anchor boxes as baseline, namely the \textit{YoloV3} detector with the \textit{TinyYoloV3} network. The fundamental concept of one-stage detectors with anchor boxes is illustrated in \Cref{fig:anchors}. In this section the concrete implementation with the \textit{TinyYoloV3} network and its training goal are explained.
	
	On a high level basis \textit{YoloV3} maps the input image to a predefined set of bounding boxes. For each box the network predicts an object probability $\hat o$  and a class probability $\hat c$. $\hat o$ expresses how likely the box contains an object, $\hat c$ expresses what kind of object it contains. This way a box can be either background or object but the object can have multiple classes. For example a box can be a human and face but not a face and background. As this work considers the single class case, we remove $\hat c$ from the prediction.
	
	Defining a set of boxes that covers only approximately the full range of possible object locations and bounding box dimensions would lead to an intractable amount of parameters/computations. Instead \textit{YoloV3} defines anchor boxes that contain the most common aspect ratios and predicts how to adapt these anchor boxes to better fit the predicted object. These are the bounding box center $\hat x,\hat y$, its width $w$ and height $h$. In total this leads to 5 output nodes for each bounding box.
	
	In an image objects can appear at different scales. For small objects more fine grain features are relevant and it is important to predict locations at a high resolution. However, for larger objects a too fine discretization is not only unnecessary but can even confuse the detector. Hence, \textit{TinyYoloV3} uses $G=2$ output grids with $S_1=13$ and $S_2=26$ grid cells to predict objects at various scales. For each grid cell $B_1 = B_2 = 3$ bounding boxes are predicted. In total this leads to 12675 output nodes that predict 2535 boxes. In a last step boxes that contain the same class and have a high overlap are filtered such that only the boxes with the highest confidences remain.
	
	The predicted outputs are interpreted as offset to the predefined anchor box dimensions. Hence, the output for the anchor at grid $i$, cell $j$, anchor $k$  is decoded as follows:
	
	\begin{equation}
	b_x = \hat x_{i,j,k} + t^x_{i,j,k}\quad
	b_y = \hat y_{i,j,k} + t^y_{i,j,k}\quad
	b_w = \hat w_{i,j,k} \cdot t^w_{i,j,k}\quad
	b_h = \hat h_{i,j,k} \cdot t^h_{i,j,k}
	\end{equation}
	
	where $t^x$,$t^y$,$t^w$,$t^h$ are the coordinates of the predefined anchor boxes and $b_x,b_y,b_w,b_h$ are the final bounding box coordinates normalized to the image resolution.
		
	This mapping is implemented with a \acp{CNN} that contains 10 convolutional, 6 pooling and 1 deconvolutional layer(s). After each convolutional layer batch normalization normalizes the output in order to simplify the training process.
	
	The architecture is illustrated in \Cref{fig:tinyyolov3_arch}. The input image with a resolution of 416x416x3 is processed by 5 layers that stepwise decrease the spatial resolution (max pooling) while increasing the width, leading to a intermediate volume of 26x26x512. This part can be seen as a common part that extracts features for objects at all scales. From layer 5 the network splits to two branches responsible for smaller and larger objects. The lower branch extracts features for larger objects leading to a final grid of 13x13. The higher branch extracts features for smaller objects leading to a grid of 26x26. For each grid cell 15 output nodes predict 3 bounding boxes. Thereby the nodes responsible for $\hat o$ have a sigmoid-activation such that the response gets squashed into a value between zero and one and can be interpreted as a probability. Similarly the nodes responsible for $\hat x$ and $\hat y$ have a sigmoid activation such that the output can be interpreted as coordinate normalized to the image size. For the output nodes of $w$ and $h$ \textit{YoloV3} uses $e^x$-activation. This way the output is always larger than one while no adaption in bounding box width/height ($w=1$) corresponds to no activation. Furthermore, the network can predict a large range of scales in a small range of activation.
			
	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=0.8\textwidth]{fig/tinyyolov3_arch}
		\caption{The architecture of the baseline \textit{TinyYoloV3}. For each layer the amount of filters are displayed. The height of the boxes correspond to their spatial dimension. Arrows correspond to the forward pass in a network inference. In the common part the spatial resolution decreases each layer down to 26x26, while the width increases from 16 to 512. From layer 5 two branches focus on objects corresponding to different scales. }
		\label{fig:tinyyolov3_arch}
	\end{figure}
	

	\subsection{Training Goal}
	
	In order to train a \ac{CNN} to predict the desired properties \textit{YoloV3} quantifies the Object Detection task as the combined goal between localization and a classification:
	\begin{equation}
	\mathcal{L} = \lambda_{loc}\mathcal{L}_{loc} + \lambda_{obj}\mathcal{L}_{obj} + \lambda_{noobj}\mathcal{L}_{noobj} + \lambda_{class}\mathcal{L}_{class}
	\end{equation}
	where $\mathcal{L}_{loc}$ is the loss for bounding box dimensions, $\mathcal{L}_{obj}$ the loss where a object is present, $\mathcal{L}_{noobj}$ the loss where there is no object and $\mathcal{L}_{class}$ the classification loss. $\lambda$ are trade-off parameters between the multiple targets.
	
	For a single class prediction this can be simplified to:
	
	\begin{equation}
	\mathcal{L} = \lambda_{loc}\mathcal{L}_{loc} + \lambda_{obj}\mathcal{L}_{obj} + \lambda_{noobj}\mathcal{L}_{noobj}
	\end{equation}
	
	The object loss quantifies a binary classification loss. Hence, it is the difference between a predicted probability $\hat o$ and an actual class label $c$. Where $o \in {0,1}$ and $\hat o \in (0,1)$. In order to learn such a goal it is desirable that the weights of the network get updated significantly when the difference between truth and prediction are high. However, when prediction and truth are already close to each other the updates to the weights should be smaller otherwise the training might miss the optimal solution. A loss function that contains the desired properties and is used by \textit{YoloV3} is the logarithmic loss which can be formulated as follows:
	
	\begin{equation}
		\mathcal{L}_{log} = -(o_{ij}\log(\hat o_{ijk}) + (1 - o_{ij})\log(1 - \hat o_{ijk}))
	\end{equation}
	
	where $\hat o_{ij}$ is an output node with sigmoid activation assigned to anchor box $i$,$j$,$k$ and $ o_{ij}$ the ground truth label assigned to that box. The logarithmic loss is calculated for each output grid $G_i$, for each grid cell $S_j$ and each anchor box $B_k$. However, only the loss of the responsible anchor boxes are summed in the total loss calculation:
	
	\begin{equation}
		\mathcal{L}_{obj} = \sum_{i=0}^{G}\sum_{j=0}^{S_i^2}\sum_{k=0}^{B_i} \mathbb{1}_{ijk}^{obj}(-(c_{ijk}\log(\hat c_{ijk}) + (1 - c_{ijk})\log(1 - \hat c_{ijk})))
	\end{equation}
	
	Thereby the  binary variable $\mathbb{1}_{ijk}^{obj}$ is 1 if anchor at $i,j,k$ is responsible to predict an object and 0 otherwise. The responsibility is determined by the \ac{IoU} of an anchor box with a ground truth box. The box with the highest \ac{IoU} with the ground truth box gets assigned responsible. $\mathcal{L}_{obj}$ is defined vice versa but controlled by the $\mathbb{1}_{ijk}^{noobj}$ binary variable.

	For the localization loss, similar to the classification loss, the weights should be updated significantly when the difference is high but less strongly when the difference is small. Furthermore, the loss should be invariant to direction. A loss that contains these properties is the squared distance between each bounding box parameter. However the squared distance does not make a difference between large and small boxes. For example, a deviation of 5 px for a small ground truth box with a width of 1 is treated equally to a deviation from a ground truth box with width 100. Therefore \textit{YoloV3}, applies the square root on width and height to scale down very large box dimensions and thus balance the loss calculation. The localization loss is summarized in:
	
	\begin{equation}
		\mathcal{L}_{loc} = \sum_{i=0}^{G} \sum_{j=0}^{S_i^2}\sum_{k=0}^{B_i} \mathbb{1}_{ijk}^{obj}[(x_{ijk}-\hat{x}_{ijk})^2 + (y_{ijk}-\hat{y}_{ijk})^2  + (\sqrt{w_{ijk}}-\sqrt{\hat{w}_{ijk}})^2 +(\sqrt{h_{ijk}}-\sqrt{\hat{h}_{ijk}})^2 ]
	\end{equation}
	where $x_{ijk}$,$y_{ijk}$ are the ground truth center coordinates of anchor box $i,j,k$ and $w_{ijk}$,$h_{ijk}$ the corresponding width and height. $\hat x_{ijk}$,$\hat y_{ijk}, \hat w_{ijk}$,$\hat h_{ijk}$ are the predicted bounding box coordinates. $\mathbb{1}_{ijk}^{obj}$ is 1 if the set of output nodes at $i$,$j$,$k$ is responsible to predict a certain object and 0 otherwise. 
	
	The model is implemented using \textit{keras} with \textit{tensorflow} backend. 
	
	\subsection{Datasets}
	
	Throughout the experiments the same training and test set is used. For training 20 000 samples from the dataset created in \Cref{sec:datagen} are used. For testing we use the test set described in \Cref{sec:datagen:method}.

	For all trainings the Adam optimizer is applied using a learning rate of 0.001 for the first 60 epochs and a learning rate 0.0001 afterwards. A validation set containing 1\% randomly sampled images from the training set is used. The training is stopped if the validation error does not decrease for more than 5 epochs or 100 epochs are reached.
	
	\section{Depth and Width}
	
	State of the art results of Computer Vision benchmarks are achieved by particularly deep/wide models. The vast amount of parameters and layers enables to model highly non-linear functions and complex image features. However, \acp{EWFO} do not contain particularly complex shapes. Instead the object parts are of thin structure and are spread across large parts of the image. Hence, we hypothesize that a very deep/wide network will not perform better than a shallower and thinner counterpart. Also, the baseline architecture is designed to detect complex objects of multiple classes. In this work we investigate the single class case. We hypothesize that this requires less filters and thus that a thinner and shallower counterpart can detect the object with equal performance.
	
	\subsection{Experiment}
	
	In order to evaluate our hypothesis an experiment is conducted. The network is trained with different architectures and the performance is evaluated. In a first experiment the TinyYoloV3 architecture is used as baseline and the number of filters is decreased stepwise by a factor of 2. The number of layers is held constant. In a second experiment the width is held constant and the depth is varied. The exact scheme to change depth is visualized in \Cref{fig:depth_changes}. In order to remove layers the convolutional layers are removed while the pooling layers are kept. For the increase of depth 2 layers are inserted stepwise at the end of both branches.  
	
		\begin{figure}[hbtp]
			\centering
			\includegraphics[width=0.8\textwidth]{fig/depth_changes}
			\caption{Architectural Changes when varying the depth. The upper graph shows in which order layers are removed. The lower graph shows how layers are added. Depth is increased by inserting two layers on each branch (green). }
			\label{fig:depth_changes}
		\end{figure}
	
	\subsection{Results}
	
	\Cref{fig:perf_width} shows the performance for thinner and wider networks. It is apparent how the performance only undergoes slight variations despite reducing the total number of weights by a factor 1000. \todo{retrain one in the middle to get variance, it should be more linear}
	
	
	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/perf_width}
		\caption{Performance in simulation when varying the amount of filters per layer. Starting from the baseline architecture with approximately 9 Mio. weights, the amount of filters per layer are decreased stepwise by a factor of 2. Only minor effects on performance can be seen, despite reducing the flexibility of the model.}
		\label{fig:perf_width}
	\end{figure}
	
	\Cref{fig:perf_depth} shows the performance for different amount of layers. It can be seen how the performance increases significantly up to a depth of 8. Further increase in depth does not lead to better overall performance. However, depth has a larger influence on the performance for different object sizes as displayed in \Cref{fig:depth_ap_size}. It can be seen how the deeper models perform better at larger object sizes. The deepest model however looses performance for the smaller object sizes.

	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/perf_depth}
		\caption{Performance in simulation when varying the amount of layers. The exact architectures are shown in \Cref{fig:depth_changes}. The overall performance does not increase further with more than 8 layers.}
		\label{fig:perf_depth}
	\end{figure}


	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/depth_ap_size}
		\caption{Performance in simulation of models with varying depth and for bins of different size. It can be seen that the performance for larger objects increases with the amount of layers.}
		\label{fig:depth_ap_size}
	\end{figure}
	
	\Cref{fig:size_bins} shows the distribution of object sizes in the bins used for evaluation. It can be seen that most objects in the test set are further away. \todo{put some examples to show what each size actuall means}
	
	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/distr_size}
		\caption{Label distribution in bins of different object size. As the field of view is larger for objects further away, the proportion of small objects is higher.}
		\label{fig:size_bins}
	\end{figure}
	
	\subsection{Discussion}
	
	The overall performance is only slightly affected when reducing the number of weights in terms of width and height. We can assume that this is because the objects we investigate are of quite simple structure. Intuitively the features to be considered are color, lines and edges in certain combinations. Hence, it seems logical that only a few filters are necessary to represent these shapes.
	
	The performance in terms of different object sizes varies significantly for models with varying depth. Only deeper networks are able to detect larger objects. However, the complexity of the object does not increase for closer objects. In contrary the closer the objects are, the less context is visible. A very close object consists "only" of an orange square. Hence, it is unlikely that increased flexibility is the reason for the increase in performance. 
	
	Instead it is more likely that the increased receptive field is the reason for the improved performance. In fact only the network with 14 layers has a receptive field of 414 an thus covers the whole image. Yet even this network cannot detect the largest objects. Thus the receptive field can not be the only reason for the lower performance on larger objects.
	
	 The current structure combines features distributed across space in a pyramid fashion. So 3-by-3 convolutions are performed layer by layer such until the whole image is covered. For \acp{EWFO} many of these steps are unnecessary as the objects are empty and the network should learn to ignore the empty part in the centre. It is possible that this structure gets confused by the parts that are present in the image centre.
	
	\subsection{Conclusion}
	
	We investigated how width affects the performance for the detection of \acp{EWFO}. We hypothesized that due to the low variance in the investigated object and the simple features, less filters are required than in the baseline architecture. We can confirm this hypothesis as we see that the width can be decreased by a factor of 10 without loosing performance. This leads to a reduction of weights by a factor of 1000. 
	
	Furthermore, we investigated how depth affects the performance of the model. We hypothesized that a shallow network should be able to detect the object as it only consists of relatively simple features. We can see how depth is required in order to cover the whole input image. Hence, we cannot fully conclude whether depth is required for the increased flexibility or simply due to the receptive field. 
	
	\section{Receptive Field}
	
	In shallow fully convolutional networks the receptive field of the last layer might not cover the whole image. This is of particular effect for \ac{EWFO} as thus it is possible that the output layer which gets assigned responsible to predict the object does not see any feature of its parts.
	
	A deeper network is one way to increase receptive field but comes at cost of computation. In this section we investigate whether the same performance can be reached by using alternative ways to increase the receptive field.
	
	\subsection{Experiment}
	
	 A 7-by-7 max-pooling layer is inserted on the lower branch after layer 6. The model is compared to the architecture with 14 layers and a model where we replace the 3-by-3 kernel of layer 6 with a 9-by-9 kernel.
	 
	 \subsection{Results}
	 
	\section{Anchors and Output Grid}
		
	\section{Reducing Inference Time}
	
	To this end we investigated how to incorporate 
	Next to performance, inference time is a crucial parameter for the detection of \ac{EWFO} on \ac{MAV}. 
	
	 It is determined by the number of computations but also how these computations can be executed. Theoretically the convolutions of one layer in a \acp{CNN} can be executed fully in parallel, however this operation has to be supported by the computational platform. The execution time further depends on how fast a computational platform can execute multiplications, how large the memory is and how fast it can be accessed, but also on the particular low level software implementation. So far we created a model that is 
	 
	 \section{What did we learn?}
	
	\section{Implementation on the \ac{MAV}}