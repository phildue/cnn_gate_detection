@misc{keras,
title = {{Keras Documentation}},
url = {https://keras.io/},
urldate = {2018-11-26}
}
@misc{tensorflow,
title = {{TensorFlow}},
url = {https://www.tensorflow.org/},
urldate = {2018-11-26}
}
@misc{airsim,
	title = {{Microsoft/AirSim: Open source simulator for autonomous vehicles built on Unreal Engine / Unity, from Microsoft AI {\&} Research}},
	url = {https://github.com/Microsoft/AirSim},
	urldate = {2018-11-26}
}
@misc{unreal,
	title = {{What is Unreal Engine 4}},
	url = {https://www.unrealengine.com/en-US/what-is-unreal-engine-4},
	urldate = {2018-11-26}
}

@article{He2014b,
	title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={37},
	number={9},
	pages={1904--1916},
	year={2015},
	publisher={IEEE}
}

@article{Hou2018,
archivePrefix = {arXiv},
arxivId = {1802.08635},
author = {Hou, Lu and Kwok, James T.},
eprint = {1802.08635},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou, Kwok - 2018 - Loss-aware Weight Quantization of Deep Networks.pdf:pdf},
month = {feb},
title = {{Loss-aware Weight Quantization of Deep Networks}},
url = {https://arxiv.org/abs/1802.08635},
year = {2018}
}

@article{Creswell2017,
	title={Generative adversarial networks: An overview},
	author={Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
	journal={IEEE Signal Processing Magazine},
	volume={35},
	number={1},
	pages={53--65},
	year={2018},
	publisher={IEEE}
}

@inproceedings{Goodfellow2014,
	title={Generative adversarial nets},
	author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle={Advances in neural information processing systems},
	pages={2672--2680},
	year={2014}
}
@inproceedings{Li2017c,
author = {Li, Quanquan and Jin, Shengying and Yan, Junjie},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.776},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {7341--7349},
publisher = {IEEE},
title = {{Mimicking Very Efficient Network for Object Detection}},
url = {http://ieeexplore.ieee.org/document/8100259/},
year = {2017}
}
@article{Itti1998,
author = {Itti, L. and Koch, C. and Niebur, E.},
doi = {10.1109/34.730558},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1254--1259},
title = {{A model of saliency-based visual attention for rapid scene analysis}},
url = {http://ieeexplore.ieee.org/document/730558/},
volume = {20},
year = {1998}
}
@article{Ba2014,
archivePrefix = {arXiv},
arxivId = {1412.7755},
author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
eprint = {1412.7755},
month = {dec},
title = {{Multiple Object Recognition with Visual Attention}},
url = {https://arxiv.org/abs/1412.7755},
year = {2014}
}
@article{Ablavatski2017a,
abstract = {We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an improved attention-based architecture for multiple object recognition. The proposed model is a fully differentiable unit that can be optimized end-to-end by using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was employed as visual attention mechanism which allows to learn the geometric transformation of objects within images. With the combination of the Spatial Transformer and the powerful recurrent architecture, the proposed EDRAM can localize and recognize objects simultaneously. EDRAM has been evaluated on two publicly available datasets including MNIST Cluttered (with 70K cluttered digits) and SVHN (with up to 250k real world images of house numbers). Experiments show that it obtains superior performance as compared with the state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {1706.03581},
author = {Ablavatski, Artsiom and Lu, Shijian and Cai, Jianfei},
doi = {10.1109/WACV.2017.113},
eprint = {1706.03581},
month = {jun},
title = {{Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition}},
url = {http://arxiv.org/abs/1706.03581 http://dx.doi.org/10.1109/WACV.2017.113},
year = {2017}
}
@inproceedings{Yim2017,
author = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.754},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {7130--7138},
publisher = {IEEE},
title = {{A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning}},
url = {http://ieeexplore.ieee.org/document/8100237/},
year = {2017}
}
@article{Hinton2015a,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513 http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
volume = {18},
year = {2006}
}
@inproceedings{Papageorgiou,
author = {Papageorgiou, C.P. and Oren, M. and Poggio, T.},
booktitle = {Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)},
doi = {10.1109/ICCV.1998.710772},
isbn = {81-7319-221-9},
pages = {555--562},
publisher = {Narosa Publishing House},
title = {{A general framework for object detection}},
url = {http://ieeexplore.ieee.org/document/710772/}
}
@article{Andreopoulos2013,
author = {Andreopoulos, Alexander and Tsotsos, John K.},
doi = {10.1016/j.cviu.2013.04.005},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = {aug},
number = {8},
pages = {827--891},
title = {{50 Years of object recognition: Directions forward}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S107731421300091X},
volume = {117},
year = {2013}
}
@article{Uijlings2013,
author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
doi = {10.1007/s11263-013-0620-5},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {sep},
number = {2},
pages = {154--171},
publisher = {Springer US},
title = {{Selective Search for Object Recognition}},
url = {http://link.springer.com/10.1007/s11263-013-0620-5},
volume = {104},
year = {2013}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}

@inproceedings{Law2018,
	title={Cornernet: Detecting objects as paired keypoints},
	author={Law, Hei and Deng, Jia},
	booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
	volume={6},
	year={2018}
}

@INPROCEEDINGS{Johnson-Roberson2016, 
	author={M. Johnson-Roberson and C. Barto and R. Mehta and S. N. Sridhar and K. Rosaen and R. Vasudevan}, 
	booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={746-753}, 
	keywords={learning (artificial intelligence);pattern classification;virtual world;human-generated annotation;deep learning;photo-realistic computer image;machine learning algorithm;data classification;sensor-based classification problems;Engines;Training;Machine learning;Data models;Robots;Training data;Automobiles;deep learning;simulation;object detection;autonomous driving}, 
	doi={10.1109/ICRA.2017.7989092}, 
	ISSN={}, 
	month={May},}

@article{Gaidon2016,
abstract = {Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.},
archivePrefix = {arXiv},
arxivId = {1605.06457},
author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
eprint = {1605.06457},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaidon et al. - 2016 - Virtual Worlds as Proxy for Multi-Object Tracking Analysis.pdf:pdf},
month = {may},
title = {{Virtual Worlds as Proxy for Multi-Object Tracking Analysis}},
url = {http://arxiv.org/abs/1605.06457},
year = {2016}
}

@inproceedings{Dodge2016a,
	title={Understanding how image quality affects deep neural networks},
	author={Dodge, Samuel and Karam, Lina},
	booktitle={Quality of Multimedia Experience (QoMEX), 2016 Eighth International Conference on},
	pages={1--6},
	year={2016},
	organization={IEEE}
}


@article{Andreopoulos2012,
author = {Andreopoulos, A. and Tsotsos, J. K.},
doi = {10.1109/TPAMI.2011.91},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {110--126},
title = {{On Sensor Bias in Experimental Methods for Comparing Interest-Point, Saliency, and Recognition Algorithms}},
url = {http://ieeexplore.ieee.org/document/5765998/},
volume = {34},
year = {2012}
}
@inproceedings{Ros2016a,
author = {Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M.},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.352},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {3234--3243},
publisher = {IEEE},
title = {{The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes}},
url = {http://ieeexplore.ieee.org/document/7780721/},
year = {2016}
}
@inproceedings{Ros2016,
author = {Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M.},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.352},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {3234--3243},
publisher = {IEEE},
title = {{The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes}},
url = {http://ieeexplore.ieee.org/document/7780721/},
year = {2016}
}
@article{Sadeghi2016,
abstract = {Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep RL to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train vision-based navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call CAD{\$}{\^{}}2{\$}RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Our method uses single RGB images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s},
archivePrefix = {arXiv},
arxivId = {1611.04201},
author = {Sadeghi, Fereshteh and Levine, Sergey},
eprint = {1611.04201},
month = {nov},
title = {{CAD2RL: Real Single-Image Flight without a Single Real Image}},
url = {http://arxiv.org/abs/1611.04201},
year = {2016}
}
@article{Hinterstoisser2017,
abstract = {Deep Learning methods usually require huge amounts of training data to perform at their full potential, and often require expensive manual labeling. Using synthetic images is therefore very attractive to train object detectors, as the labeling comes for free, and several approaches have been proposed to combine synthetic and real images for training. In this paper, we show that a simple trick is sufficient to train very effectively modern object detectors with synthetic images only: We freeze the layers responsible for feature extraction to generic layers pre-trained on real images, and train only the remaining layers with plain OpenGL rendering. Our experiments with very recent deep architectures for object recognition (Faster-RCNN, R-FCN, Mask-RCNN) and image feature extractors (InceptionResnet and Resnet) show this simple approach performs surprisingly well.},
archivePrefix = {arXiv},
arxivId = {1710.10710},
author = {Hinterstoisser, Stefan and Lepetit, Vincent and Wohlhart, Paul and Konolige, Kurt},
eprint = {1710.10710},
month = {oct},
title = {{On Pre-Trained Image Features and Synthetic Images for Deep Learning}},
url = {http://arxiv.org/abs/1710.10710},
year = {2017}
}
@misc{Krizhevsky2012a,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{Howard2013,
abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55{\%} using no external data which is over a 20{\%} relative improvement on the previous year's winner.},
archivePrefix = {arXiv},
arxivId = {1312.5402},
author = {Howard, Andrew G.},
eprint = {1312.5402},
month = {dec},
title = {{Some Improvements on Deep Convolutional Neural Network Based Image Classification}},
url = {http://arxiv.org/abs/1312.5402},
year = {2013}
}
@article{Gupta2014,
abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3{\%}, which is a 56{\%} relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24{\%} relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
archivePrefix = {arXiv},
arxivId = {1407.5736},
author = {Gupta, Saurabh and Girshick, Ross and Arbel{\'{a}}ez, Pablo and Malik, Jitendra},
eprint = {1407.5736},
month = {jul},
title = {{Learning Rich Features from RGB-D Images for Object Detection and Segmentation}},
url = {http://arxiv.org/abs/1407.5736},
year = {2014}
}
@article{Fischer2015,
abstract = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
archivePrefix = {arXiv},
arxivId = {1504.06852},
author = {Fischer, Philipp and Dosovitskiy, Alexey and Ilg, Eddy and H{\"{a}}usser, Philip and Hazırbaş, Caner and Golkov, Vladimir and van der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
eprint = {1504.06852},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer et al. - 2015 - FlowNet Learning Optical Flow with Convolutional Networks.pdf:pdf},
month = {apr},
title = {{FlowNet: Learning Optical Flow with Convolutional Networks}},
url = {http://arxiv.org/abs/1504.06852},
year = {2015}
}
@article{Levine2018,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 ro...},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
doi = {10.1177/0278364917710318},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
month = {apr},
number = {4-5},
pages = {421--436},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
url = {http://journals.sagepub.com/doi/10.1177/0278364917710318},
volume = {37},
year = {2018}
}
@misc{Handa2016,
author = {Handa, Ankur and Patraucean, Viorica and Badrinarayanan, Vijay and Stent, Simon and Cipolla, Roberto},
pages = {4077--4085},
title = {{Understanding Real World Indoor Scenes With Synthetic Data}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/html/Handa{\_}Understanding{\_}Real{\_}World{\_}CVPR{\_}2016{\_}paper.html},
year = {2016}
}
@article{Tschannen2017,
abstract = {A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) consists of matrix multiplications, in both convolution and fully connected layers. We perform end-to-end learning of low-cost approximations of matrix multiplications in DNN layers by casting matrix multiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits) and learning their (ternary) edge weights from data. The SPNs disentangle multiplication and addition operations and enable us to impose a budget on the number of multiplication operations. Combining our method with knowledge distillation and applying it to image classification DNNs (trained on ImageNet) and language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction in number of multiplications (over 99.5{\%}) while maintaining the predictive performance of the full-precision models. Finally, we demonstrate that the proposed framework is able to rediscover Strassen's matrix multiplication algorithm, learning to multiply {\$}2 \backslashtimes 2{\$} matrices using only 7 multiplications instead of 8.},
archivePrefix = {arXiv},
arxivId = {1712.03942},
author = {Tschannen, Michael and Khanna, Aran and Anandkumar, Anima},
eprint = {1712.03942},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tschannen, Khanna, Anandkumar - 2017 - StrassenNets Deep Learning with a Multiplication Budget.pdf:pdf},
month = {dec},
title = {{StrassenNets: Deep Learning with a Multiplication Budget}},
url = {http://arxiv.org/abs/1712.03942},
year = {2017}
}

@inproceedings{Szegedy2014,
	title={Going deeper with convolutions},
	author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={1--9},
	year={2015}
}

@inproceedings{He2015,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@article{Mohamed2014,
abstract = {The small scale and portability of fixed-wing Micro Aerial Vehicles lend them to many unique applications, however their utility is often limited by ineffective attitude control in turbulent environments. The performance of attitude control systems themselves are affected by a variety of factors. Assessment of this system's performance needs to be viewed in relation to the MAVs' unique constraints. Certain aspects and limitations of MAV attitude control related issues are addressed in the literature, but to fully address the degradation of utility, the entire system must be examined. These issues can only be fully addressed when considering them concurrently. There is no framework for defining the attitude control problem explicitly for MAVs. This paper attempts to (1) Define the MAV attitude control problem with respect to the unique constraints imposed by this class of Unmanned Aircraft; (2) Review current design trends of MAVs with respect to vulnerability to atmospheric turbulence.},
author = {Mohamed, Abdulghani and Massey, Kevin and Watkins, Simon and Clothier, Reece},
doi = {10.1016/J.PAEROSCI.2013.12.003},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohamed et al. - 2014 - The attitude control of fixed-wing MAVS in turbulent environments.pdf:pdf},
issn = {0376-0421},
journal = {Progress in Aerospace Sciences},
month = {apr},
pages = {37--48},
publisher = {Pergamon},
title = {{The attitude control of fixed-wing MAVS in turbulent environments}},
url = {https://www.sciencedirect.com/science/article/pii/S0376042113000912},
volume = {66},
year = {2014}
}
@article{Jenkins2001,
author = {Jenkins, David A. and Jenkins, David A. and Ifju, Peter G. and Abdulrahim, Mujahid and Olipra, Scott},
journal = {PROC. SIXTEENTH INT. CONF. ON UNMANNED AIR VEHICLE SYSTEMS},
title = {{Assessment of controllability of Micro Air Vehicles}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.199.6606},
year = {2001}
}
@incollection{Murphy2016,
address = {Cham},
author = {Murphy, Robin R. and Tadokoro, Satoshi and Kleiner, Alexander},
booktitle = {Springer Handbook of Robotics},
doi = {10.1007/978-3-319-32552-1_60},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy, Tadokoro, Kleiner - 2016 - Disaster Robotics.pdf:pdf},
pages = {1577--1604},
publisher = {Springer International Publishing},
title = {{Disaster Robotics}},
url = {http://link.springer.com/10.1007/978-3-319-32552-1{\_}60},
year = {2016}
}
@article{Wilson2007,
abstract = {Objective: We show that psychophysiologically driven real-time adaptive aiding significantly enhances performance in a complex aviation task. Afurther goal was to assess the importance of individual operator capabilities when providing adaptive aiding. Background: Psychophysiological measures are useful for monitoring cognitive workload in laboratory and real-world settings. They can be recorded without intruding into task performance and can be analyzed in real time, making them candidates for providing operator functional state estimates. These estimates could be used to determine if and when system intervention should be provided to assist the operator to improve system performance. Methods: Adaptive automation was implemented while operators performed an uninhabited aerial vehicle task. Psychophysiological data were collected and an artificial neural network was used to detect periods of high and low mental workload in real time. The high-difficulty task levels used to initiate the adaptive automation...},
author = {Wilson, Glenn F. and Russell, Christopher A.},
doi = {10.1518/001872007X249875},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
month = {dec},
number = {6},
pages = {1005--1018},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Performance Enhancement in an Uninhabited Air Vehicle Task Using Psychophysiologically Determined Adaptive Aiding}},
url = {http://journals.sagepub.com/doi/10.1518/001872007X249875},
volume = {49},
year = {2007}
}
@article{Elbanhawi2017,
abstract = {The utility of micro air vehicles (MAVs) has expanded significantly in the last decade, and there are now numerous commercial systems available at relatively low cost. This expansion has arisen mainly due to the miniaturisation of flight control systems and advances in energy storage and propulsion technologies. Several emerging applications involve routine operation of MAVs in complex urban environments such as parcel delivery, communications relay and environmental monitoring. However, MAVs currently rely on one or more operators-in-the-loop and, whilst desirable, full autonomous operation has not yet been achieved. In this review paper, autonomous MAV operation in complex environments is explored with conceptualisation for future MAV operation in urban environments. Limitations of current technologies are systematically examined through consideration of the state-of-the-art and future trends. The main limitations challenging the realisation of fully autonomous MAVs are mainly attributed to: computational power, communication and energy storage. These limitations lead to poor sensing and planning capabilities, which are essential components of autonomous MAVs. Possible solutions are explored with goal of enabling MAVs to reliably operate autonomously in urban environments.},
author = {Elbanhawi, M. and Mohamed, A. and Clothier, R. and Palmer, J.L. and Simic, M. and Watkins, S.},
doi = {10.1016/J.PAEROSCI.2017.03.002},
issn = {0376-0421},
journal = {Progress in Aerospace Sciences},
month = {may},
pages = {27--52},
publisher = {Pergamon},
title = {{Enabling technologies for autonomous MAV operations}},
url = {https://www.sciencedirect.com/science/article/pii/S0376042116300367},
volume = {91},
year = {2017}
}
@article{Webb2007,
author = {Webb, Thomas P. and Prazenica, Richard J. and Kurdila, Andrew J. and Lind, Rick},
doi = {10.2514/1.22398},
issn = {0731-5090},
journal = {Journal of Guidance, Control, and Dynamics},
month = {may},
number = {3},
pages = {816--826},
title = {{Vision-Based State Estimation for Autonomous Micro Air Vehicles}},
url = {http://arc.aiaa.org/doi/10.2514/1.22398},
volume = {30},
year = {2007}
}
@article{Webb2007a,
author = {Webb, Thomas P. and Prazenica, Richard J. and Kurdila, Andrew J. and Lind, Rick},
doi = {10.2514/1.22398},
issn = {0731-5090},
journal = {Journal of Guidance, Control, and Dynamics},
month = {may},
number = {3},
pages = {816--826},
title = {{Vision-Based State Estimation for Autonomous Micro Air Vehicles}},
url = {http://arc.aiaa.org/doi/10.2514/1.22398},
volume = {30},
year = {2007}
}
@inproceedings{Ettinger,
author = {Ettinger, S.M. and Nechyba, M.C. and Ifju, P.G. and Waszak, M.},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and System},
doi = {10.1109/IRDS.2002.1041582},
isbn = {0-7803-7398-7},
pages = {2134--2140},
publisher = {IEEE},
title = {{Vision-guided flight stability and control for micro air vehicles}},
url = {http://ieeexplore.ieee.org/document/1041582/},
volume = {3}
}
@incollection{Schafroth,
address = {Dordrecht},
author = {Schafroth, Dario and Bouabdallah, Samir and Bermes, Christian and Siegwart, Roland},
booktitle = {Unmanned Aircraft Systems},
doi = {10.1007/978-1-4020-9137-7_14},
pages = {245--260},
publisher = {Springer Netherlands},
title = {{From the Test Benches to the First Prototype of the muFly Micro Helicopter}},
url = {http://link.springer.com/10.1007/978-1-4020-9137-7{\_}14}
}
@inproceedings{Bills2011,
author = {Bills, Cooper and Chen, Joyce and Saxena, Ashutosh},
booktitle = {2011 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980136},
isbn = {978-1-61284-386-5},
month = {may},
pages = {5776--5783},
publisher = {IEEE},
title = {{Autonomous MAV flight in indoor environments using single image perspective cues}},
url = {http://ieeexplore.ieee.org/document/5980136/},
year = {2011}
}
@article{Elbanhawi2017a,
abstract = {The utility of micro air vehicles (MAVs) has expanded significantly in the last decade, and there are now numerous commercial systems available at relatively low cost. This expansion has arisen mainly due to the miniaturisation of flight control systems and advances in energy storage and propulsion technologies. Several emerging applications involve routine operation of MAVs in complex urban environments such as parcel delivery, communications relay and environmental monitoring. However, MAVs currently rely on one or more operators-in-the-loop and, whilst desirable, full autonomous operation has not yet been achieved. In this review paper, autonomous MAV operation in complex environments is explored with conceptualisation for future MAV operation in urban environments. Limitations of current technologies are systematically examined through consideration of the state-of-the-art and future trends. The main limitations challenging the realisation of fully autonomous MAVs are mainly attributed to: computational power, communication and energy storage. These limitations lead to poor sensing and planning capabilities, which are essential components of autonomous MAVs. Possible solutions are explored with goal of enabling MAVs to reliably operate autonomously in urban environments.},
author = {Elbanhawi, M. and Mohamed, A. and Clothier, R. and Palmer, J.L. and Simic, M. and Watkins, S.},
doi = {10.1016/J.PAEROSCI.2017.03.002},
issn = {0376-0421},
journal = {Progress in Aerospace Sciences},
month = {may},
pages = {27--52},
publisher = {Pergamon},
title = {{Enabling technologies for autonomous MAV operations}},
url = {https://www.sciencedirect.com/science/article/pii/S0376042116300367},
volume = {91},
year = {2017}
}
@article{Smolyanskiy2017,
abstract = {We present a micro aerial vehicle (MAV) system, built with inexpensive off-the-shelf hardware, for autonomously following trails in unstructured, outdoor environments such as forests. The system introduces a deep neural network (DNN) called TrailNet for estimating the view orientation and lateral offset of the MAV with respect to the trail center. The DNN-based controller achieves stable flight without oscillations by avoiding overconfident behavior through a loss function that includes both label smoothing and entropy reward. In addition to the TrailNet DNN, the system also utilizes vision modules for environmental awareness, including another DNN for object detection and a visual odometry component for estimating depth for the purpose of low-level obstacle detection. All vision systems run in real time on board the MAV via a Jetson TX1. We provide details on the hardware and software used, as well as implementation details. We present experiments showing the ability of our system to navigate forest trails more robustly than previous techniques, including autonomous flights of 1 km.},
archivePrefix = {arXiv},
arxivId = {1705.02550},
author = {Smolyanskiy, Nikolai and Kamenev, Alexey and Smith, Jeffrey and Birchfield, Stan},
eprint = {1705.02550},
month = {may},
title = {{Toward Low-Flying Autonomous MAV Trail Navigation using Deep Neural Networks for Environmental Awareness}},
url = {http://arxiv.org/abs/1705.02550},
year = {2017}
}
@misc{JoshuaBateman2017,
author = {{Joshua Bateman}},
booktitle = {Wired},
pages = {1},
title = {{China Uses Drones for Earthquake Search and Rescue Missions | WIRED}},
url = {https://www.wired.com/2017/01/chinas-launching-drones-fight-back-earthquakes/},
urldate = {2018-08-23},
year = {2017}
}
@misc{KateBaggaley2017,
author = {{Kate Baggaley}},
booktitle = {NBCNews},
pages = {1},
title = {{Drones are fighting wildfires in some very surprising ways}},
url = {https://www.nbcnews.com/mach/science/drones-are-fighting-wildfires-some-very-surprising-ways-ncna820966},
urldate = {2018-08-23},
year = {2017}
}
@misc{Shankland2018,
author = {Shankland, Stephen},
booktitle = {Sci-Tech},
pages = {1},
title = {{Watch out, Amazon. Zipline's new medical delivery drones go farther, faster - CNET}},
url = {https://www.cnet.com/news/zipline-new-delivery-drones-fly-medical-supplies-faster-farther/},
urldate = {2018-08-23},
year = {2018}
}
@article{Guo2018,
abstract = {Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. For all its popularity, deep neural networks are also criticized for consuming a lot of memory and draining battery life of devices during training and inference. This makes it hard to deploy these models on mobile or embedded devices which have tight resource constraints. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. Instead of adopting 32-bit floating point format to represent weights, quantized representations store weights using more compact formats such as integers or even binary numbers. Despite a possible degradation in predictive performance, quantization provides a potential solution to greatly reduce the model size and the energy consumption. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed.},
archivePrefix = {arXiv},
arxivId = {1808.04752},
author = {Guo, Yunhui},
eprint = {1808.04752},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo - 2018 - A Survey on Methods and Theories of Quantized Neural Networks.pdf:pdf},
month = {aug},
title = {{A Survey on Methods and Theories of Quantized Neural Networks}},
url = {http://arxiv.org/abs/1808.04752},
year = {2018}
}
@article{He2014,
abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8{\%} top-5 error, 10-view test), yet is 20{\%} faster than "AlexNet" (16.0{\%} top-5 error, 10-view test).},
archivePrefix = {arXiv},
arxivId = {1412.1710},
author = {He, Kaiming and Sun, Jian},
eprint = {1412.1710},
month = {dec},
title = {{Convolutional Neural Networks at Constrained Time Cost}},
url = {http://arxiv.org/abs/1412.1710},
year = {2014}
}
@article{Zagoruyko2016,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
archivePrefix = {arXiv},
arxivId = {1605.07146},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
eprint = {1605.07146},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zagoruyko, Komodakis - 2016 - Wide Residual Networks.pdf:pdf},
month = {may},
title = {{Wide Residual Networks}},
url = {http://arxiv.org/abs/1605.07146},
year = {2016}
}
@article{Peng2017a,
abstract = {Domain adaptation is an important tool to transfer knowledge about a task (e.g. classification) learned in a source domain to a second, or target domain. Current approaches assume that task-relevant target-domain data is available during training. We demonstrate how to perform domain adaptation when no such task-relevant target-domain data is available. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses privileged information from task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest but also close to the target-domain representation. Therefore, the source-domain task of interest solution (e.g. a classifier for classification tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform domain adaptation in classification tasks without access to task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating task-relevant target-domain representations with task-relevant source-domain data. To the best of our knowledge, ZDDA is the first domain adaptation and sensor fusion method which requires no task-relevant target-domain data. The underlying principle is not particular to computer vision data, but should be extensible to other domains.},
archivePrefix = {arXiv},
arxivId = {1707.01922},
author = {Peng, Kuan-Chuan and Wu, Ziyan and Ernst, Jan},
eprint = {1707.01922},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Wu, Ernst - 2017 - Zero-Shot Deep Domain Adaptation(2).pdf:pdf},
month = {jul},
title = {{Zero-Shot Deep Domain Adaptation}},
url = {http://arxiv.org/abs/1707.01922},
year = {2017}
}
@article{Rozantsev2014,
abstract = {We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances. A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available.},
archivePrefix = {arXiv},
arxivId = {1411.7911},
author = {Rozantsev, Artem and Lepetit, Vincent and Fua, Pascal},
doi = {10.1016/j.cviu.2014.12.006},
eprint = {1411.7911},
month = {nov},
title = {{On Rendering Synthetic Images for Training an Object Detector}},
url = {http://arxiv.org/abs/1411.7911 http://dx.doi.org/10.1016/j.cviu.2014.12.006},
year = {2014}
}
@article{Liu2018a,
abstract = {This paper addresses the problem of unsupervised domain adaptation on the task of pedestrian detection in crowded scenes. First, we utilize an iterative algorithm to iteratively select and auto-annotate positive pedestrian samples with high confidence as the training samples for the target domain. Meanwhile, we also reuse negative samples from the source domain to compensate for the imbalance between the amount of positive samples and negative samples. Second, based on the deep network we also design an unsupervised regularizer to mitigate influence from data noise. More specifically, we transform the last fully connected layer into two sub-layers - an element-wise multiply layer and a sum layer, and add the unsupervised regularizer to further improve the domain adaptation accuracy. In experiments for pedestrian detection, the proposed method boosts the recall value by nearly 30{\%} while the precision stays almost the same. Furthermore, we perform our method on standard domain adaptation benchmarks on both supervised and unsupervised settings and also achieve state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1802.03269},
author = {Liu, Lihang and Lin, Weiyao and Wu, Lisheng and Yu, Yong and Yang, Michael Ying},
eprint = {1802.03269},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - Unsupervised Deep Domain Adaptation for Pedestrian Detection.pdf:pdf},
month = {feb},
title = {{Unsupervised Deep Domain Adaptation for Pedestrian Detection}},
url = {http://arxiv.org/abs/1802.03269},
year = {2018}
}
@article{Ben-David2010,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
doi = {10.1007/s10994-009-5152-4},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2010 - A theory of learning from different domains.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {may},
number = {1-2},
pages = {151--175},
publisher = {Springer US},
title = {{A theory of learning from different domains}},
url = {http://link.springer.com/10.1007/s10994-009-5152-4},
volume = {79},
year = {2010}
}

@inproceedings{Tobin2017,
title={Domain randomization for transferring deep neural networks from simulation to the real world},
author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
booktitle={Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on},
pages={23--30},
year={2017},
organization={IEEE}
}

@article{Liu2017a,
abstract = {Current top-performing object detectors depend on deep CNN backbones, such as ResNet-101 and Inception, benefiting from their powerful feature representation but suffering from high computational cost. Conversely, some lightweight model based detectors fulfil real time processing, while their accuracies are often criticized. In this paper, we explore an alternative to build a fast and accurate detector by strengthening lightweight features using a crafting mechanism. Inspired by the structure of Receptive Fields (RFs) in human visual systems, we propose a novel RF Block (RFB) module, which takes the relationship between the size and eccentricity of RFs into account, to enhance the discriminability and robustness of features. We further assemble the RFB module to the top of SSD with a lightweight CNN model, constructing the RFB Net detector. To evaluate its effectiveness, experiments are conducted on two major benchmarks and the results show that RFB Net is able to reach the accuracy of advanced very deep backbone network based detectors while keeping the real-time speed. Code is available at https://github.com/ruinmessi/RFBNet},
archivePrefix = {arXiv},
arxivId = {1711.07767},
author = {Liu, Songtao and Huang, Di and Wang, Yunhong},
eprint = {1711.07767},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Huang, Wang - 2017 - Receptive Field Block Net for Accurate and Fast Object Detection.pdf:pdf},
month = {nov},
title = {{Receptive Field Block Net for Accurate and Fast Object Detection}},
url = {http://arxiv.org/abs/1711.07767},
year = {2017}
}
@article{Kyrkou2018,
abstract = {Unmanned Aerial Vehicles (drones) are emerging as a promising technology for both environmental and infrastructure monitoring, with broad use in a plethora of applications. Many such applications require the use of computer vision algorithms in order to analyse the information captured from an on-board camera. Such applications include detecting vehicles for emergency response and traffic monitoring. This paper therefore, explores the trade-offs involved in the development of a single-shot object detector based on deep convolutional neural networks (CNNs) that can enable UAVs to perform vehicle detection under a resource constrained environment such as in a UAV. The paper presents a holistic approach for designing such systems; the data collection and training stages, the CNN architecture, and the optimizations necessary to efficiently map such a CNN on a lightweight embedded processing platform suitable for deployment on UAVs. Through the analysis we propose a CNN architecture that is capable of detecting vehicles from aerial UAV images and can operate between 5-18 frames-per-second for a variety of platforms with an overall accuracy of {\~{}}95{\%}. Overall, the proposed architecture is suitable for UAV applications, utilizing low-power embedded processors that can be deployed on commercial UAVs.},
archivePrefix = {arXiv},
arxivId = {1807.06789},
author = {Kyrkou, Christos and Plastiras, George and Venieris, Stylianos and Theocharides, Theocharis and Bouganis, Christos-Savvas},
doi = {10.23919/DATE.2018.8342149},
eprint = {1807.06789},
month = {jul},
title = {{DroNet: Efficient convolutional neural network detector for real-time UAV applications}},
url = {http://arxiv.org/abs/1807.06789 http://dx.doi.org/10.23919/DATE.2018.8342149},
year = {2018}
}
@article{Tommasi2016,
abstract = {In this paper we focus on the spatial nature of visual domain shift, attempting to learn where domain adaptation originates in each given image of the source and target set. We borrow concepts and techniques from the CNN visualization literature, and learn domainnes maps able to localize the degree of domain specificity in images. We derive from these maps features related to different domainnes levels, and we show that by considering them as a preprocessing step for a domain adaptation algorithm, the final classification performance is strongly improved. Combined with the whole image representation, these features provide state of the art results on the Office dataset.},
archivePrefix = {arXiv},
arxivId = {1607.06144},
author = {Tommasi, Tatiana and Lanzi, Martina and Russo, Paolo and Caputo, Barbara},
eprint = {1607.06144},
month = {jul},
title = {{Learning the Roots of Visual Domain Shift}},
url = {http://arxiv.org/abs/1607.06144},
year = {2016}
}
@article{Gupta2018a,
abstract = {We present here, a novel network architecture called MergeNet for discovering small obstacles for on-road scenes in the context of autonomous driving. The basis of the architecture rests on the central consideration of training with less amount of data since the physical setup and the annotation process for small obstacles is hard to scale. For making effective use of the limited data, we propose a multi-stage training procedure involving weight-sharing, separate learning of low and high level features from the RGBD input and a refining stage which learns to fuse the obtained complementary features. The model is trained and evaluated on the Lost and Found dataset and is able to achieve state-of-art results with just 135 images in comparison to the 1000 images used by the previous benchmark. Additionally, we also compare our results with recent methods trained on 6000 images and show that our method achieves comparable performance with only 1000 training samples.},
archivePrefix = {arXiv},
arxivId = {1803.06508},
author = {Gupta, Krishnam and Javed, Syed Ashar and Gandhi, Vineet and Krishna, K. Madhava},
eprint = {1803.06508},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2018 - MergeNet A Deep Net Architecture for Small Obstacle Discovery.pdf:pdf},
month = {mar},
title = {{MergeNet: A Deep Net Architecture for Small Obstacle Discovery}},
url = {http://arxiv.org/abs/1803.06508},
year = {2018}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
eprint = {1506.01497},
month = {jun},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
year = {2015}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
eprint = {1504.08083},
month = {apr},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
year = {2015}
}

@inproceedings{Sandler2018,
	title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
	author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={4510--4520},
	year={2018}
}

@techreport{AruniRoyChowdhuryPrakharSharma2018,
abstract = {This paper investigates the presence of duplicate neurons or filters in neural networks. This phenomenon is prevalent in networks and increases with the number of filters in a layer. We observe the emergence of duplicate filters over training iterations, study the factors that affect their concentration and compare existing network reducing operations. We validate our findings using convolutional and fully-connected networks on the CIFAR-10 dataset.},
author = {{Aruni RoyChowdhury, Prakhar Sharma}, Erik G. Learned-Miller},
title = {{Reducing Duplicate Filters in Deep Neural Networks - Semantic Scholar}},
url = {https://www.semanticscholar.org/paper/Reducing-Duplicate-Filters-in-Deep-Neural-Networks-RoyChowdhury-Sharma/05769ee8d0731819a6f4213e3987899a06204cb9},
year = {2018}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{He2014a,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-10578-9_23},
eprint = {1406.4729},
month = {jun},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729 http://dx.doi.org/10.1007/978-3-319-10578-9{\_}23},
year = {2014}
}
@inproceedings{Razavian2014,
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.131},
isbn = {978-1-4799-4308-1},
month = {jun},
pages = {512--519},
publisher = {IEEE},
title = {{CNN Features Off-the-Shelf: An Astounding Baseline for Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910029},
year = {2014}
}
@article{Lowe2004,
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {nov},
number = {2},
pages = {91--110},
publisher = {Kluwer Academic Publishers},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@inproceedings{Dalal,
author = {Dalal, N. and Triggs, B.},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
doi = {10.1109/CVPR.2005.177},
isbn = {0-7695-2372-2},
pages = {886--893},
publisher = {IEEE},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://ieeexplore.ieee.org/document/1467360/},
volume = {1}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
eprint = {1703.06870},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf},
month = {mar},
title = {{Mask R-CNN}},
url = {http://arxiv.org/abs/1703.06870},
year = {2017}
}
@article{Soviany2018,
abstract = {There are mainly two types of state-of-the-art object detectors. On one hand, we have two-stage detectors, such as Faster R-CNN (Region-based Convolutional Neural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to generate regions of interests in the first stage and (ii) send the region proposals down the pipeline for object classification and bounding-box regression. Such models reach the highest accuracy rates, but are typically slower. On the other hand, we have single-stage detectors, such as YOLO (You Only Look Once) and SSD (Singe Shot MultiBox Detector), that treat object detection as a simple regression problem which takes an input image and learns the class probabilities and bounding box coordinates. Such models reach lower accuracy rates, but are much faster than two-stage object detectors. In this paper, we propose to use an image difficulty predictor to achieve an optimal trade-off between accuracy and speed in object detection. The image difficulty predictor is applied on the test images to split them into easy versus hard images. Once separated, the easy images are sent to the faster single-stage detector, while the hard images are sent to the more accurate two-stage detector. Our experiments on PASCAL VOC 2007 show that using image difficulty compares favorably to a random split of the images. Our method is flexible, in that it allows to choose a desired threshold for splitting the images into easy versus hard.},
archivePrefix = {arXiv},
arxivId = {1803.08707},
author = {Soviany, Petru and Ionescu, Radu Tudor},
eprint = {1803.08707},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soviany, Ionescu - 2018 - Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction.pdf:pdf},
month = {mar},
title = {{Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction}},
url = {http://arxiv.org/abs/1803.08707},
year = {2018}
}
@article{Huang2016a,
abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
archivePrefix = {arXiv},
arxivId = {1611.10012},
author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
eprint = {1611.10012},
month = {nov},
title = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
url = {http://arxiv.org/abs/1611.10012},
year = {2016}
}
@article{Wei2018a,
abstract = {In this paper, we propose a simple and general framework for training very tiny CNNs for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic and quantization. Mimic improves the performance of a student network by transfering knowledge from a teacher network. Quantization converts a full-precision network to a quantized one without large degradation of performance. If the teacher network is quantized, the search scope of the student network will be smaller. Using this property of quantization, we propose Quantization Mimic. It first quantizes the large network, then mimic a quantized small network. We suggest the operation of quantization can help student network to match the feature maps from teacher network. To evaluate the generalization of our hypothesis, we carry out experiments on various popular CNNs including VGG and Resnet, as well as different detection frameworks including Faster R-CNN and R-FCN. Experiments on Pascal VOC and WIDER FACE verify our Quantization Mimic algorithm can be applied on various settings and outperforms state-of-the-art model acceleration methods given limited computing resouces.},
archivePrefix = {arXiv},
arxivId = {1805.02152},
author = {Wei, Yi and Pan, Xinyu and Qin, Hongwei and Ouyang, Wanli and Yan, Junjie},
eprint = {1805.02152},
month = {may},
title = {{Quantization Mimic: Towards Very Tiny CNN for Object Detection}},
url = {http://arxiv.org/abs/1805.02152},
year = {2018}
}
@article{Zhuang2018,
abstract = {Recently, facial attribute classification (FAC) has attracted significant attention in the computer vision community. Great progress has been made along with the availability of challenging FAC datasets. However, conventional FAC methods usually firstly pre-process the input images (i.e., perform face detection and alignment) and then predict facial attributes. These methods ignore the inherent dependencies among these tasks (i.e., face detection, facial landmark localization and FAC). Moreover, some methods using convolutional neural network are trained based on the fixed loss weights without considering the differences between facial attributes. In order to address the above problems, we propose a novel multi-task learning of cas- caded convolutional neural network method, termed MCFA, for predicting multiple facial attributes simultaneously. Specifically, the proposed method takes advantage of three cascaded sub-networks (i.e., S{\_}Net, M{\_}Net and L{\_}Net corresponding to the neural networks under different scales) to jointly train multiple tasks in a coarse-to-fine manner, which can achieve end-to-end optimization. Furthermore, the proposed method automatically assigns the loss weight to each facial attribute based on a novel dynamic weighting scheme, thus making the proposed method concentrate on predicting the more difficult facial attributes. Experimental results show that the proposed method outperforms several state-of-the-art FAC methods on the challenging CelebA and LFWA datasets.},
archivePrefix = {arXiv},
arxivId = {1805.01290},
author = {Zhuang, Ni and Yan, Yan and Chen, Si and Wang, Hanzi},
eprint = {1805.01290},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhuang et al. - 2018 - Multi-task Learning of Cascaded CNN for Facial Attribute Classification.pdf:pdf},
month = {may},
title = {{Multi-task Learning of Cascaded CNN for Facial Attribute Classification}},
url = {http://arxiv.org/abs/1805.01290},
year = {2018}
}
@article{Lu2017,
abstract = {{Vision based player detection is important in sports applications. Accuracy, efficiency, and low memory consumption are desirable for real-time tasks such as intelligent broadcasting and automatic event classification. In this paper, we present a cascaded convolutional neural network (CNN) that satisfies all three of these requirements. Our method first trains a binary (player/non-player) classification network from labeled image patches. Then, our method efficiently applies the network to a whole image in testing. We conducted experiments on basketball and soccer games. Experimental results demonstrate that our method can accurately detect players under challenging conditions such as varying illumination, highly dynamic camera movements and motion blur. Comparing with conventional CNNs, our approach achieves state-of-the-art accuracy on both games with 1000x fewer parameters (i.e., it is light{\}}.},
archivePrefix = {arXiv},
arxivId = {1709.10230},
author = {Lu, Keyu and Chen, Jianhui and Little, James J. and He, Hangen},
eprint = {1709.10230},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2017 - Light Cascaded Convolutional Neural Networks for Accurate Player Detection.pdf:pdf},
month = {sep},
title = {{Light Cascaded Convolutional Neural Networks for Accurate Player Detection}},
url = {http://arxiv.org/abs/1709.10230},
year = {2017}
}
@inproceedings{WeiShen2015,
author = {{Wei Shen} and {Xinggang Wang} and {Yan Wang} and {Xiang Bai} and Zhang, Zhijiang},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7299024},
isbn = {978-1-4673-6964-0},
month = {jun},
pages = {3982--3991},
publisher = {IEEE},
title = {{DeepContour: A deep convolutional feature learned by positive-sharing loss for contour detection}},
url = {http://ieeexplore.ieee.org/document/7299024/},
year = {2015}
}
@article{Li2017b,
author = {Li, Xiaobin and Wang, Shengjin},
doi = {10.1109/LGRS.2017.2749478},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
month = {nov},
number = {11},
pages = {2037--2041},
title = {{Object Detection Using Convolutional Neural Networks in a Coarse-to-Fine Manner}},
url = {http://ieeexplore.ieee.org/document/8051277/},
volume = {14},
year = {2017}
}
@article{Bengio2013,
author = {Bengio, Y. and Courville, A. and Vincent, P.},
doi = {10.1109/TPAMI.2013.50},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {aug},
number = {8},
pages = {1798--1828},
title = {{Representation Learning: A Review and New Perspectives}},
url = {http://ieeexplore.ieee.org/document/6472238/},
volume = {35},
year = {2013}
}
@article{Bengio2009,
abstract = {Learning Deep Architectures for AI},
author = {Bengio, Y.},
doi = {10.1561/2200000006},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
keywords = {Dimensionality reduction,Machine Learning},
month = {nov},
number = {1},
pages = {1--127},
publisher = {Now Publishers, Inc.},
title = {{Learning Deep Architectures for AI}},
url = {http://www.nowpublishers.com/article/Details/MAL-006},
volume = {2},
year = {2009}
}
@article{Voulodimos2018,
abstract = {{\textless}p{\textgreater}Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.{\textless}/p{\textgreater}},
author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
doi = {10.1155/2018/7068349},
issn = {1687-5265},
journal = {Computational Intelligence and Neuroscience},
month = {feb},
pages = {1--13},
publisher = {Hindawi},
title = {{Deep Learning for Computer Vision: A Brief Review}},
url = {https://www.hindawi.com/journals/cin/2018/7068349/},
volume = {2018},
year = {2018}
}
@article{Sinha2018,
abstract = {Deep learning has recently become one of the most popular sub-fields of machine learning owing to its distributed data representation with multiple levels of abstraction. A diverse range of deep learning algorithms are being employed to solve conventional artificial intelligence problems. This paper gives an overview of some of the most widely used deep learning algorithms applied in the field of computer vision. It first inspects the various approaches of deep learning algorithms, followed by a description of their applications in image classification, object identification, image extraction and semantic segmentation in the presence of noise. The paper concludes with the discussion of the future scope and challenges for construction and training of deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1804.03928},
author = {Sinha, Rajat Kumar and Pandey, Ruchi and Pattnaik, Rohan},
eprint = {1804.03928},
month = {apr},
title = {{Deep Learning For Computer Vision Tasks: A review}},
url = {http://arxiv.org/abs/1804.03928},
year = {2018}
}
@article{Li2017a,
author = {Li, Xiaobin and Wang, Shengjin},
doi = {10.1109/LGRS.2017.2749478},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
month = {nov},
number = {11},
pages = {2037--2041},
title = {{Object Detection Using Convolutional Neural Networks in a Coarse-to-Fine Manner}},
url = {http://ieeexplore.ieee.org/document/8051277/},
volume = {14},
year = {2017}
}
@inproceedings{Castrejon2017,
author = {Castrejon, Lluis and Kundu, Kaustav and Urtasun, Raquel and Fidler, Sanja},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.477},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {4485--4493},
publisher = {IEEE},
title = {{Annotating Object Instances with a Polygon-RNN}},
url = {http://ieeexplore.ieee.org/document/8099960/},
year = {2017}
}
@inproceedings{Zhu2016,
author = {Zhu, Gao and Porikli, Fatih and Li, Hongdong},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.160},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {1265--1272},
publisher = {IEEE},
title = {{Robust Visual Tracking with Deep Convolutional Neural Network Based Object Proposals on PETS}},
url = {http://ieeexplore.ieee.org/document/7789650/},
year = {2016}
}
@inproceedings{Zhu2016a,
author = {Zhu, Gao and Porikli, Fatih and Li, Hongdong},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.160},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {1265--1272},
publisher = {IEEE},
title = {{Robust Visual Tracking with Deep Convolutional Neural Network Based Object Proposals on PETS}},
url = {http://ieeexplore.ieee.org/document/7789650/},
year = {2016}
}
@article{Gordon2017,
abstract = {Robust object tracking requires knowledge and understanding of the object being tracked: its appearance, its motion, and how it changes over time. A tracker must be able to modify its underlying model and adapt to new observations. We present Re3, a real-time deep object tracker capable of incorporating temporal information into its model. Rather than focusing on a limited set of objects or training a model at test-time to track a specific instance, we pretrain our generic tracker on a large variety of objects and efficiently update on the fly; Re3 simultaneously tracks and updates the appearance model with a single forward pass. This lightweight model is capable of tracking objects at 150 FPS, while attaining competitive results on challenging benchmarks. We also show that our method handles temporary occlusion better than other comparable trackers using experiments that directly measure performance on sequences with occlusion.},
archivePrefix = {arXiv},
arxivId = {1705.06368},
author = {Gordon, Daniel and Farhadi, Ali and Fox, Dieter},
eprint = {1705.06368},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordon, Farhadi, Fox - 2017 - Re3 Real-Time Recurrent Regression Networks for Visual Tracking of Generic Objects.pdf:pdf},
month = {may},
title = {{Re3 : Real-Time Recurrent Regression Networks for Visual Tracking of Generic Objects}},
url = {http://arxiv.org/abs/1705.06368},
year = {2017}
}
@book{Shilpa12016,
abstract = {Moving object detection and tracking are the more important and challenging task in video surveillance and computer vision applications. Object detection is the procedure of finding the non-stationary entities in the image sequences. Detection is the first step towards tracking the moving object in the video. Object representation is the next important step to track.    Tracking is the method of identifying, the position of the moving object in the video. Identifying the position is much more challenging task then detecting the moving object in a video.  Object tracking is applied in numerous applications like in robot vision, monitoring the traffic, Video surveillance, Video in-painting and Simulation. Here we are going to present a brief review of numerous object detection, object classification and object tracking algorithms available.},
author = {{Shilpa {\#}1}, Prathap H.L{\#}2 Sunitha M.R {\#}3},
booktitle = {International Journal Of Engineering And Computer Science},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shilpa {\#}1 - 2016 - International journal of engineering and computer science IJECS.pdf:pdf},
issn = {2319-7242},
number = {4},
title = {{International journal of engineering and computer science IJECS.}},
url = {http://ijecs.in/index.php/ijecs/article/view/943},
volume = {5},
year = {2016}
}
@inproceedings{Han2017,
author = {Han, Zhongxing and Zhang, Hui and Zhang, Jinfang and Hu, Xiaohui},
booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2017.8296691},
isbn = {978-1-5090-2175-8},
month = {sep},
pages = {2294--2298},
publisher = {IEEE},
title = {{Fast aircraft detection based on region locating network in large-scale remote sensing images}},
url = {http://ieeexplore.ieee.org/document/8296691/},
year = {2017}
}
@misc{Mnih2014,
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and koray Kavukcuoglu},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:pdf},
pages = {2204--2212},
title = {{Recurrent Models of Visual Attention}},
url = {https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention},
year = {2014}
}
@article{Feichtenhofer2017,
abstract = {Recent approaches for high accuracy detection and tracking of object categories in video consist of complex multistage solutions that become more cumbersome each year. In this paper we propose a ConvNet architecture that jointly performs detection and tracking, solving the task in a simple and effective way. Our contributions are threefold: (i) we set up a ConvNet architecture for simultaneous detection and tracking, using a multi-task objective for frame-based object detection and across-frame track regression; (ii) we introduce correlation features that represent object co-occurrences across time to aid the ConvNet during tracking; and (iii) we link the frame level detections based on our across-frame tracklets to produce high accuracy detections at the video level. Our ConvNet architecture for spatiotemporal object detection is evaluated on the large-scale ImageNet VID dataset where it achieves state-of-the-art results. Our approach provides better single model performance than the winning method of the last ImageNet challenge while being conceptually much simpler. Finally, we show that by increasing the temporal stride we can dramatically increase the tracker speed.},
archivePrefix = {arXiv},
arxivId = {1710.03958},
author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
eprint = {1710.03958},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feichtenhofer, Pinz, Zisserman - 2017 - Detect to Track and Track to Detect.pdf:pdf},
month = {oct},
title = {{Detect to Track and Track to Detect}},
url = {http://arxiv.org/abs/1710.03958},
year = {2017}
}
@inproceedings{Madaan2017,
author = {Madaan, Ratnesh and Maturana, Daniel and Scherer, Sebastian},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2017.8206190},
isbn = {978-1-5386-2682-5},
month = {sep},
pages = {3487--3494},
publisher = {IEEE},
title = {{Wire detection using synthetic data and dilated convolutional networks for unmanned aerial vehicles}},
url = {http://ieeexplore.ieee.org/document/8206190/},
year = {2017}
}
@article{Jung2018,
author = {Jung, Sunggoo and Cho, Sungwook and Lee, Dasol and Lee, Hanseob and Shim, David Hyunchul},
doi = {10.1002/rob.21743},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung et al. - 2018 - A direct visual servoing-based framework for the 2016 IROS Autonomous Drone Racing Challenge.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Aerial robotics,autonomous drone racing,collision avoidance,stereo vision,visual servoing},
month = {jan},
number = {1},
pages = {146--166},
publisher = {Wiley-Blackwell},
title = {{A direct visual servoing-based framework for the 2016 IROS Autonomous Drone Racing Challenge}},
url = {http://doi.wiley.com/10.1002/rob.21743},
volume = {35},
year = {2018}
}
@article{Shen2016,
abstract = {Deep convolutional neural networks continue to advance the state-of-the-art in many domains as they grow bigger and more complex. It has been observed that many of the parameters of a large network are redundant, allowing for the possibility of learning a smaller network that mimics the outputs of the large network through a process called Knowledge Distillation. We show, however, that standard Knowledge Distillation is not effective for learning small models for the task of pedestrian detection. To improve this process, we introduce a higher-dimensional hint layer to increase information flow. We also estimate the variance in the outputs of the large network and propose a loss function to incorporate this uncertainty. Finally, we attempt to boost the complexity of the small network without increasing its size by using as input hand-designed features that have been demonstrated to be effective for pedestrian detection. We succeed in training a model that contains {\$}400\backslashtimes{\$} fewer parameters than the large network while outperforming AlexNet on the Caltech Pedestrian Dataset.},
archivePrefix = {arXiv},
arxivId = {1612.00478},
author = {Shen, Jonathan and Vesdapunt, Noranart and Boddeti, Vishnu N. and Kitani, Kris M.},
eprint = {1612.00478},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - 2016 - In Teacher We Trust Learning Compressed Models for Pedestrian Detection.pdf:pdf},
month = {dec},
title = {{In Teacher We Trust: Learning Compressed Models for Pedestrian Detection}},
url = {http://arxiv.org/abs/1612.00478},
year = {2016}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf:pdf},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Romero2014,
abstract = {While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.},
archivePrefix = {arXiv},
arxivId = {1412.6550},
author = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
eprint = {1412.6550},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Romero et al. - 2014 - FitNets Hints for Thin Deep Nets.pdf:pdf},
month = {dec},
title = {{FitNets: Hints for Thin Deep Nets}},
url = {http://arxiv.org/abs/1412.6550},
year = {2014}
}
@article{Zhang2017a,
abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8{\%}) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves {\~{}}13x actual speedup over AlexNet while maintaining comparable accuracy.},
archivePrefix = {arXiv},
arxivId = {1707.01083},
author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
eprint = {1707.01083},
month = {jul},
title = {{ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}},
url = {http://arxiv.org/abs/1707.01083},
year = {2017}
}
@article{,
archivePrefix = {arXiv},
arxivId = {1511.07122},
eprint = {1511.07122},
title = {{No Title}}
}
@article{Cui2018,
abstract = {In order to improve the detection accuracy for objects at different scales, most of the recent works utilize the pyramidal feature hierarchy of the ConvNets from bottom to top. Nevertheless, the weak semantic information makes the bottom layers poor in detection, especially for small objects. Furthermore, most of the fine details are lost on the top layers. In this paper, we design a Multi-scale Deconvolutional Single Shot Detector for small objects (MDSSD for short). To obtain the feature maps with enriched representation power, we add the high-level layers with semantic information to the low-level layers via deconvolution Fusion Block. It is noteworthy that multiple high-level layers with different scales are upsampled simultaneously in our framework. We implement the skip connections to form more descriptive feature maps and predictions are made on these new fusion layers. Our proposed framework achieves 78.6{\%} mAP on PASCAL VOC2007 test and 26.8{\%} mAP on MS COCO test-dev2015 at 38.5 FPS with only 300*300 input.},
archivePrefix = {arXiv},
arxivId = {1805.07009},
author = {Cui, Lisha},
eprint = {1805.07009},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui - 2018 - MDSSD Multi-scale Deconvolutional Single Shot Detector for small objects.pdf:pdf},
month = {may},
title = {{MDSSD: Multi-scale Deconvolutional Single Shot Detector for small objects}},
url = {http://arxiv.org/abs/1805.07009},
year = {2018}
}
@article{Braun2018,
abstract = {Big data has had a great share in the success of deep learning in computer vision. Recent works suggest that there is significant further potential to increase object detection performance by utilizing even bigger datasets. In this paper, we introduce the EuroCity Persons dataset, which provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238200 person instances manually labeled in over 47300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211200). We optimize four state-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3) to serve as baselines for the new object detection benchmark. In experiments with previous datasets we analyze the generalization capabilities of these detectors when trained with the new dataset. We furthermore study the effect of the training set size, the dataset diversity (day- vs. night-time, geographical region), the dataset detail (i.e. availability of object orientation information) and the annotation quality on the detector performance. Finally, we analyze error sources and discuss the road ahead.},
archivePrefix = {arXiv},
arxivId = {1805.07193},
author = {Braun, Markus and Krebs, Sebastian and Flohr, Fabian and Gavrila, Dariu M.},
eprint = {1805.07193},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braun et al. - 2018 - The EuroCity Persons Dataset A Novel Benchmark for Object Detection.pdf:pdf},
month = {may},
title = {{The EuroCity Persons Dataset: A Novel Benchmark for Object Detection}},
url = {http://arxiv.org/abs/1805.07193},
year = {2018}
}
@article{Xu2017,
abstract = {This paper proposes a method for video smoke detection using synthetic smoke samples. The virtual data can automatically offer precise and rich annotated samples. However, the learning of smoke representations will be hurt by the appearance gap between real and synthetic smoke samples. The existed researches mainly work on the adaptation to samples extracted from original annotated samples. These methods take the object detection and domain adaptation as two independent parts. To train a strong detector with rich synthetic samples, we construct the adaptation to the detection layer of state-of-the-art single-model detectors (SSD and MS-CNN). The training procedure is an end-to-end stage. The classification, location and adaptation are combined in the learning. The performance of the proposed model surpasses the original baseline in our experiments. Meanwhile, our results show that the detectors based on the adversarial adaptation are superior to the detectors based on the discrepancy adaptation. Code will be made publicly available on http://smoke.ustc.edu.cn. Moreover, the domain adaptation for two-stage detector is described in Appendix A.},
archivePrefix = {arXiv},
arxivId = {1709.08142},
author = {Xu, Gao and Zhang, Yongming and Zhang, Qixing and Lin, Gaohua and Wang, Jinjun},
eprint = {1709.08142},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2017 - Domain Adaptation from Synthesis to Reality in Single-model Detector for Video Smoke Detection.pdf:pdf},
month = {sep},
title = {{Domain Adaptation from Synthesis to Reality in Single-model Detector for Video Smoke Detection}},
url = {http://arxiv.org/abs/1709.08142},
year = {2017}
}
@article{Graham2014,
abstract = {Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75{\%} of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.},
archivePrefix = {arXiv},
arxivId = {1412.6071},
author = {Graham, Benjamin},
eprint = {1412.6071},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graham - 2014 - Fractional Max-Pooling.pdf:pdf},
month = {dec},
title = {{Fractional Max-Pooling}},
url = {http://arxiv.org/abs/1412.6071},
year = {2014}
}
@article{Ren2017,
abstract = {We propose the Recurrent Soft Attention Model, which integrates the visual attention from the original image to a LSTM memory cell through a down-sample network. The model recurrently transmits visual attention to the memory cells for glimpse mask generation, which is a more natural way for attention integration and exploitation in general object detection and recognition problem. We test our model under the metric of the top-1 accuracy on the CIFAR-10 dataset. The experiment shows that our down-sample network and feedback mechanism plays an effective role among the whole network structure.},
archivePrefix = {arXiv},
arxivId = {1705.01921},
author = {Ren, Liliang},
eprint = {1705.01921},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren - 2017 - Recurrent Soft Attention Model for Common Object Recognition.pdf:pdf},
month = {may},
title = {{Recurrent Soft Attention Model for Common Object Recognition}},
url = {http://arxiv.org/abs/1705.01921},
year = {2017}
}
@article{Ablavatski2017,
abstract = {We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an improved attention-based architecture for multiple object recognition. The proposed model is a fully differentiable unit that can be optimized end-to-end by using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was employed as visual attention mechanism which allows to learn the geometric transformation of objects within images. With the combination of the Spatial Transformer and the powerful recurrent architecture, the proposed EDRAM can localize and recognize objects simultaneously. EDRAM has been evaluated on two publicly available datasets including MNIST Cluttered (with 70K cluttered digits) and SVHN (with up to 250k real world images of house numbers). Experiments show that it obtains superior performance as compared with the state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {1706.03581},
author = {Ablavatski, Artsiom and Lu, Shijian and Cai, Jianfei},
doi = {10.1109/WACV.2017.113},
eprint = {1706.03581},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ablavatski, Lu, Cai - 2017 - Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition.pdf:pdf},
month = {jun},
title = {{Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition}},
url = {http://arxiv.org/abs/1706.03581 http://dx.doi.org/10.1109/WACV.2017.113},
year = {2017}
}
@article{Hara2017,
abstract = {We propose augmenting deep neural networks with an attention mechanism for the visual object detection task. As perceiving a scene, humans have the capability of multiple fixation points, each attended to scene content at different locations and scales. However, such a mechanism is missing in the current state-of-the-art visual object detection methods. Inspired by the human vision system, we propose a novel deep network architecture that imitates this attention mechanism. As detecting objects in an image, the network adaptively places a sequence of glimpses of different shapes at different locations in the image. Evidences of the presence of an object and its location are extracted from these glimpses, which are then fused for estimating the object class and bounding box coordinates. Due to lacks of ground truth annotations of the visual attention mechanism, we train our network using a reinforcement learning algorithm with policy gradients. Experiment results on standard object detection benchmarks show that the proposed network consistently outperforms the baseline networks that does not model the attention mechanism.},
archivePrefix = {arXiv},
arxivId = {1702.01478},
author = {Hara, Kota and Liu, Ming-Yu and Tuzel, Oncel and Farahmand, Amir-massoud},
eprint = {1702.01478},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara et al. - 2017 - Attentional Network for Visual Object Detection.pdf:pdf},
month = {feb},
title = {{Attentional Network for Visual Object Detection}},
url = {http://arxiv.org/abs/1702.01478},
year = {2017}
}
@article{Ghosh2017,
abstract = {We present QuickNet, a fast and accurate network architecture that is both faster and significantly more accurate than other fast deep architectures like SqueezeNet. Furthermore, it uses less parameters than previous networks, making it more memory efficient. We do this by making two major modifications to the reference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable convolutions and 2) The use of parametric rectified linear units. We make the observation that parametric rectified linear units are computationally equivalent to leaky rectified linear units at test time and the observation that separable convolutions can be interpreted as a compressed Inception network (Chollet, 2016). Using these observations, we derive a network architecture, which we call QuickNet, that is both faster and more accurate than previous models. Our architecture provides at least four major advantages: (1) A smaller model size, which is more tenable on memory constrained systems; (2) A significantly faster network which is more tenable on computationally constrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10 Dataset which outperforms all but one result published so far, although we note that our works are orthogonal approaches and can be combined (4) Orthogonality to previous model compression approaches allowing for further speed gains to be realized.},
archivePrefix = {arXiv},
arxivId = {1701.02291},
author = {Ghosh, Tapabrata},
eprint = {1701.02291},
month = {jan},
title = {{QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures}},
url = {http://arxiv.org/abs/1701.02291},
year = {2017}
}
@article{Chung2018,
abstract = {To overcome the poor scalability of convolutional neural network, recurrent attention model(RAM) selectively choose what and where to look on the image. By directing recurrent attention model how to look the image, RAM can be even more successful in that the given clue narrow down the scope of the possible focus zone. In this perspective, this work proposes clued recurrent attention model (CRAM) which add clue or constraint on the RAM better problem solving. CRAM follows encoder-decoder framework, encoder utilizes recurrent attention model with spatial transformer network and decoder which varies depending on the task. To ensure the performance, CRAM tackles two computer vision task. One is the image classification task, with clue given as the binary image saliency which indicates the approximate location of object. The other is the inpainting task, with clue given as binary mask which indicates the occluded part. In both tasks, CRAM shows better performance than existing methods showing the successful extension of RAM.},
archivePrefix = {arXiv},
arxivId = {1804.10844},
author = {Chung, Minki and Cho, Sungzoon},
eprint = {1804.10844},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung, Cho - 2018 - CRAM Clued Recurrent Attention Model.pdf:pdf},
month = {apr},
title = {{CRAM: Clued Recurrent Attention Model}},
url = {http://arxiv.org/abs/1804.10844},
year = {2018}
}
@article{Shankar2018,
abstract = {We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.},
archivePrefix = {arXiv},
arxivId = {1804.10745},
author = {Shankar, Shiv and Piratla, Vihari and Chakrabarti, Soumen and Chaudhuri, Siddhartha and Jyothi, Preethi and Sarawagi, Sunita},
eprint = {1804.10745},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shankar et al. - 2018 - Generalizing Across Domains via Cross-Gradient Training.pdf:pdf},
month = {apr},
title = {{Generalizing Across Domains via Cross-Gradient Training}},
url = {http://arxiv.org/abs/1804.10745},
year = {2018}
}
@article{Liu2017,
abstract = {While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.},
archivePrefix = {arXiv},
arxivId = {1705.01314},
author = {Liu, Yen-Cheng and Yeh, Yu-Ying and Fu, Tzu-Chien and Wang, Sheng-De and Chiu, Wei-Chen and Wang, Yu-Chiang Frank},
eprint = {1705.01314},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Detach and Adapt Learning Cross-Domain Disentangled Deep Representation.pdf:pdf},
month = {may},
title = {{Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation}},
url = {http://arxiv.org/abs/1705.01314},
year = {2017}
}
@article{Alvar2018,
abstract = {Object tracking is the cornerstone of many visual analytics systems. While considerable progress has been made in this area in recent years, robust, efficient, and accurate tracking in real-world video remains a challenge. In this paper, we present a hybrid tracker that leverages motion information from the compressed video stream and a general-purpose semantic object detector acting on decoded frames to construct a fast and efficient tracking engine suitable for a number of visual analytics applications. The proposed approach is compared with several well-known recent trackers on the OTB tracking dataset. The results indicate advantages of the proposed method in terms of speed and/or accuracy. Another advantage of the proposed method over most existing trackers is its simplicity and deployment efficiency, which stems from the fact that it reuses and re-purposes the resources and information that may already exist in the system for other reasons.},
archivePrefix = {arXiv},
arxivId = {1805.00107},
author = {Alvar, Saeed Ranjbar and Baji{\'{c}}, Ivan V.},
eprint = {1805.00107},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvar, Baji{\'{c}} - 2018 - MV-YOLO Motion Vector-aided Tracking by Semantic Object Detection.pdf:pdf},
month = {apr},
title = {{MV-YOLO: Motion Vector-aided Tracking by Semantic Object Detection}},
url = {http://arxiv.org/abs/1805.00107},
year = {2018}
}
@article{Howard2017,
abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
archivePrefix = {arXiv},
arxivId = {1704.04861},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
eprint = {1704.04861},
month = {apr},
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
url = {http://arxiv.org/abs/1704.04861},
year = {2017}
}

@article{Kingma2014AdamAM,
	title={Adam: A Method for Stochastic Optimization},
	author={Diederik P. Kingma and Jimmy Ba},
	journal={CoRR},
	year={2014},
	volume={abs/1412.6980}
}

@article{Redmona,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP 50 in 51 ms on a Titan X, com-pared to 57.5 AP 50 in 198 ms by RetinaNet, similar perfor-mance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
author = {Redmon, Joseph and Farhadi, Ali},
title = {{YOLOv3: An Incremental Improvement}},
url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf}
}
@article{Dodge2016,
abstract = {Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high quality image datasets, yet in practical applications the input images can not be assumed to be of high quality. Recently, deep neural networks have obtained state-of-the-art performance on many machine vision tasks. In this paper we provide an evaluation of 4 state-of-the-art deep neural network models for image classification under quality distortions. We consider five types of quality distortions: blur, noise, contrast, JPEG, and JPEG2000 compression. We show that the existing networks are susceptible to these quality distortions, particularly to blur and noise. These results enable future work in developing deep neural networks that are more invariant to quality distortions.},
archivePrefix = {arXiv},
arxivId = {1604.04004},
author = {Dodge, Samuel and Karam, Lina},
eprint = {1604.04004},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dodge, Karam - 2016 - Understanding How Image Quality Affects Deep Neural Networks(2).pdf:pdf},
month = {apr},
title = {{Understanding How Image Quality Affects Deep Neural Networks}},
url = {http://arxiv.org/abs/1604.04004},
year = {2016}
}

@inproceedings{Huang2016,
	title={Densely connected convolutional networks.},
	author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
	booktitle={CVPR},
	volume={1},
	number={2},
	pages={3},
	year={2017}
}
@article{Kumar2018,
abstract = {In the framework of convolutional neural networks that lie at the heart of deep learning, downsampling is often performed with a max-pooling operation that only retains the element with maximum activation, while completely discarding the information contained in other elements in a pooling region. To address this issue, a novel pooling scheme, Ordinal Pooling Network (OPN), is introduced in this work. OPN rearranges all the elements of a pooling region in a sequence and assigns different weights to these elements based upon their orders in the sequence, where the weights are learned via the gradient-based optimisation. The results of our small-scale experiments on image classification task demonstrate that this scheme leads to a consistent improvement in the accuracy over max-pooling operation. This improvement is expected to increase in deeper networks, where several layers of pooling become necessary.},
archivePrefix = {arXiv},
arxivId = {1804.02702},
author = {Kumar, Ashwani},
eprint = {1804.02702},
month = {apr},
title = {{Ordinal Pooling Networks: For Preserving Information over Shrinking Feature Maps}},
url = {http://arxiv.org/abs/1804.02702},
year = {2018}
}
@article{Bai2017,
abstract = {Large scale image dataset and deep convolutional neural network (DCNN) are two primary driving forces for the rapid progress made in generic object recognition tasks in recent years. While lots of network architectures have been continuously designed to pursue lower error rates, few efforts are devoted to enlarge existing datasets due to high labeling cost and unfair comparison issues. In this paper, we aim to achieve lower error rate by augmenting existing datasets in an automatic manner. Our method leverages both Web and DCNN, where Web provides massive images with rich contextual information, and DCNN replaces human to automatically label images under guidance of Web contextual information. Experiments show our method can automatically scale up existing datasets significantly from billions web pages with high accuracy, and significantly improve the performance on object recognition tasks by using the automatically augmented datasets, which demonstrates that more supervisory information has been automatically gathered from the Web. Both the dataset and models trained on the dataset are made publicly available.},
archivePrefix = {arXiv},
arxivId = {1708.08201},
author = {Bai, Yalong and Yang, Kuiyuan and Mei, Tao and Ma, Wei-Ying and Zhao, Tiejun},
eprint = {1708.08201},
month = {aug},
title = {{Automatic Dataset Augmentation}},
url = {http://arxiv.org/abs/1708.08201},
year = {2017}
}
@article{Zhu2018,
abstract = {Despite the recent success of video object detection on Desktop GPUs, its architecture is still far too heavy for mobiles. It is also unclear whether the key principles of sparse feature propagation and multi-frame feature aggregation apply at very limited computational resources. In this paper, we present a light weight network architecture for video object detection on mobiles. Light weight image object detector is applied on sparse key frames. A very small network, Light Flow, is designed for establishing correspondence across frames. A flow-guided GRU module is designed to effectively aggregate features on key frames. For non-key frames, sparse feature propagation is performed. The whole network can be trained end-to-end. The proposed system achieves 60.2{\%} mAP score at speed of 25.6 fps on mobiles (e.g., HuaWei Mate 8).},
archivePrefix = {arXiv},
arxivId = {1804.05830},
author = {Zhu, Xizhou and Dai, Jifeng and Zhu, Xingchi and Wei, Yichen and Yuan, Lu},
eprint = {1804.05830},
month = {apr},
title = {{Towards High Performance Video Object Detection for Mobiles}},
url = {http://arxiv.org/abs/1804.05830},
year = {2018}
}
@article{Chen2018,
abstract = {High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, e.g. those that require detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, i.e. reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6{\%} at 20 fps, or 79.0{\%} at 62 fps as a performance/speed tradeoff.},
archivePrefix = {arXiv},
arxivId = {1804.05472},
author = {Chen, Kai and Wang, Jiaqi and Yang, Shuo and Zhang, Xingcheng and Xiong, Yuanjun and Loy, Chen Change and Lin, Dahua},
eprint = {1804.05472},
month = {apr},
title = {{Optimizing Video Object Detection via a Scale-Time Lattice}},
url = {http://arxiv.org/abs/1804.05472},
year = {2018}
}
@article{Younes2018,
abstract = {Visual Odometry (VO) can be categorized as being either direct or feature based. When the system is calibrated photometrically, and images are captured at high rates, direct methods have shown to outperform feature-based ones in terms of accuracy and processing time; they are also more robust to failure in feature-deprived environments. On the downside, Direct methods rely on heuristic motion models to seed the estimation of camera motion between frames; in the event that these models are violated (e.g., erratic motion), Direct methods easily fail. This paper proposes a novel system entitled FDMO (Feature assisted Direct Monocular Odometry), which complements the advantages of both direct and featured based techniques. FDMO bootstraps indirect feature tracking upon the sub-pixel accurate localized direct keyframes only when failure modes (e.g., large baselines) of direct tracking occur. Control returns back to direct odometry when these conditions are no longer violated. Efficiencies are introduced to help FDMO perform in real time. FDMO shows significant drift (alignment, rotation {\&} scale) reduction when compared to DSO {\&} ORB SLAM when evaluated using the TumMono and EuroC datasets.},
archivePrefix = {arXiv},
arxivId = {1804.05422},
author = {Younes, Georges and Asmar, Daniel and Zelek, John},
eprint = {1804.05422},
month = {apr},
title = {{FDMO: Feature Assisted Direct Monocular Odometry}},
url = {http://arxiv.org/abs/1804.05422},
year = {2018}
}
@article{Liu2018,
abstract = {Deep neural networks have made remarkable progresses on various computer vision tasks. Recent works have shown that depth, width and shortcut connections of networks are all vital to their performances. In this paper, we introduce a method to sparsify DenseNet which can reduce connections of a L-layer DenseNet from O(L{\^{}}2) to O(L), and thus we can simultaneously increase depth, width and connections of neural networks in a more parameter-efficient and computation-efficient way. Moreover, an attention module is introduced to further boost our network's performance. We denote our network as SparseNet. We evaluate SparseNet on datasets of CIFAR(including CIFAR10 and CIFAR100) and SVHN. Experiments show that SparseNet can obtain improvements over the state-of-the-art on CIFAR10 and SVHN. Furthermore, while achieving comparable performances as DenseNet on these datasets, SparseNet is x2.6 smaller and x3.7 faster than the original DenseNet.},
archivePrefix = {arXiv},
arxivId = {1804.05340},
author = {Liu, Wenqi and Zeng, Kun},
eprint = {1804.05340},
month = {apr},
title = {{SparseNet: A Sparse DenseNet for Image Classification}},
url = {http://arxiv.org/abs/1804.05340},
year = {2018}
}
@article{Chen2018a,
abstract = {A fundamental problem with few-shot learning is the scarcity of data in training. A natural solution to alleviate this scarcity is to augment the existing images for each training class. However, directly augmenting samples in image space may not necessarily, nor sufficiently, explore the intra-class variation. To this end, we propose to directly synthesize instance features by leveraging the semantics of each class. Essentially, a novel auto-encoder network dual TriNet, is proposed for feature augmentation. The encoder TriNet projects multi-layer visual features of deep CNNs into the semantic space. In this space, data augmentation is induced, and the augmented instance representation is projected back into the image feature spaces by the decoder TriNet. Two data argumentation strategies in the semantic space are explored; notably these seemingly simple augmentations in semantic space result in complex augmented feature distributions in the image feature space, resulting in substantially better performance. The code and models of our paper will be published on: https://github.com/tankche1/Semantic-Feature-Augmentation-in-Few-shot-Learning.},
archivePrefix = {arXiv},
arxivId = {1804.05298},
author = {Chen, Zitian and Fu, Yanwei and Zhang, Yinda and Jiang, Yu-Gang and Xue, Xiangyang and Sigal, Leonid},
eprint = {1804.05298},
month = {apr},
title = {{Semantic Feature Augmentation in Few-shot Learning}},
url = {http://arxiv.org/abs/1804.05298},
year = {2018}
}
@article{Wu2016,
abstract = {Recently, virtual reality, augmented reality, robotics, autonomous driving et al attract much attention of both academic and industrial community, in which image based camera localization is a key task. However, there has not been a complete review on image-based camera localization. It is urgent to map this topic to help people enter the field quickly. In this paper, an overview of image based camera localization is presented. A new and complete kind of classifications for image based camera localization is provided and the related techniques are introduced. Trends for the future development are also discussed. It will be useful to not only researchers but also engineers and other people interested.},
archivePrefix = {arXiv},
arxivId = {1610.03660},
author = {Wu, Yihong and Tang, Fulin and Li, Heping},
eprint = {1610.03660},
month = {oct},
title = {{Image Based Camera Localization: an Overview}},
url = {http://arxiv.org/abs/1610.03660},
year = {2016}
}
@article{Carlson2018,
abstract = {Recent work has focused on generating synthetic imagery and augmenting real imagery to increase the size and variability of training data for learning visual tasks in urban scenes. This includes increasing the occurrence of occlusions or varying environmental and weather effects. However, few have addressed modeling the variation in the sensor domain. Unfortunately, varying sensor effects can degrade performance and generalizability of results for visual tasks trained on human annotated datasets. This paper proposes an efficient, automated physically-based augmentation pipeline to vary sensor effects -- specifically, chromatic aberration, blur, exposure, noise, and color cast -- across both real and synthetic imagery. In particular, this paper illustrates that augmenting training datasets with the proposed pipeline improves the robustness and generalizability of object detection on a variety of benchmark vehicle datasets.},
archivePrefix = {arXiv},
arxivId = {1803.07721},
author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
eprint = {1803.07721},
month = {mar},
title = {{Modeling Camera Effects to Improve Deep Vision for Real and Synthetic Data}},
url = {http://arxiv.org/abs/1803.07721},
year = {2018}
}
@article{Tekin2017,
abstract = {We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task (Kehl et al., ICCV'17) that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by the YOLO network design that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our approach substantially outperforms other recent CNN-based approaches when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of the existing methods, but at 10 fps or less, they are much slower than our method.},
archivePrefix = {arXiv},
arxivId = {1711.08848},
author = {Tekin, Bugra and Sinha, Sudipta N. and Fua, Pascal},
eprint = {1711.08848},
month = {nov},
title = {{Real-Time Seamless Single Shot 6D Object Pose Prediction}},
url = {http://arxiv.org/abs/1711.08848},
year = {2017}
}
@article{Peng2017,
abstract = {Domain adaptation is an important tool to transfer knowledge about a task (e.g. classification) learned in a source domain to a second, or target domain. Current approaches assume that task-relevant target-domain data is available during training. We demonstrate how to perform domain adaptation when no such task-relevant target-domain data is available. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses privileged information from task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest but also close to the target-domain representation. Therefore, the source-domain task of interest solution (e.g. a classifier for classification tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform domain adaptation in classification tasks without access to task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating task-relevant target-domain representations with task-relevant source-domain data. To the best of our knowledge, ZDDA is the first domain adaptation and sensor fusion method which requires no task-relevant target-domain data. The underlying principle is not particular to computer vision data, but should be readily extensible to other domains.},
archivePrefix = {arXiv},
arxivId = {1707.01922},
author = {Peng, Kuan-Chuan and Wu, Ziyan and Ernst, Jan},
eprint = {1707.01922},
month = {jul},
title = {{Zero-Shot Deep Domain Adaptation}},
url = {http://arxiv.org/abs/1707.01922},
year = {2017}
}
@article{Bhat2018,
abstract = {In the field of generic object tracking numerous attempts have been made to exploit deep features. Despite all expectations, deep trackers are yet to reach an outstanding level of performance compared to methods solely based on handcrafted features. In this paper, we investigate this key issue and propose an approach to unlock the true potential of deep features for tracking. We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness. We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking. Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy. Extensive experiments are performed on four challenging datasets. On VOT2017, our approach significantly outperforms the top performing tracker from the challenge with a relative gain of 17{\%} in EAO.},
archivePrefix = {arXiv},
arxivId = {1804.06833},
author = {Bhat, Goutam and Johnander, Joakim and Danelljan, Martin and Khan, Fahad Shahbaz and Felsberg, Michael},
eprint = {1804.06833},
month = {apr},
title = {{Unveiling the Power of Deep Tracking}},
url = {http://arxiv.org/abs/1804.06833},
year = {2018}
}
@article{Pinheiro2017,
abstract = {The objective of unsupervised domain adaptation is to leverage features from a labeled source domain and learn a classifier for an unlabeled target domain, with a similar but different data distribution. Most deep learning approaches to domain adaptation consist of two steps: (i) learn features that preserve a low risk on labeled samples (source domain) and (ii) make the features from both domains to be as indistinguishable as possible, so that a classifier trained on the source can also be applied on the target domain. In general, the classifiers in step (i) consist of fully-connected layers applied directly on the indistinguishable features learned in (ii). In this paper, we propose a different way to do the classification, using similarity learning. The proposed method learns a pairwise similarity function in which classification can be performed by computing similarity between prototype representations of each category. The domain-invariant features and the categorical prototype representations are learned jointly and in an end-to-end fashion. At inference time, images from the target domain are compared to the prototypes and the label associated with the one that best matches the image is outputed. The approach is simple, scalable and effective. We show that our model achieves state-of-the-art performance in different unsupervised domain adaptation scenarios.},
archivePrefix = {arXiv},
arxivId = {1711.08995},
author = {Pinheiro, Pedro O.},
eprint = {1711.08995},
month = {nov},
title = {{Unsupervised Domain Adaptation with Similarity Learning}},
url = {http://arxiv.org/abs/1711.08995},
year = {2017}
}
@article{Tan2018,
abstract = {Mobile network that millions of people use every day is one of the most complex systems in real world. Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater challenges than in prior works. Actually, learning to solve complex problems in real world to benefit everyone and make the world better has long been ultimate goal of AI. However, application of deep reinforcement learning (DRL) to complex problems in real world still remains unsolved, due to imperfect information, data scarcity and complex rules in real world, potential negative impact to real world, etc. To bridge this reality gap, we propose a sim-to-real framework to direct transfer learning from simulation to real world without any training in real world. First, we distill temporal-spatial relationships between cells and mobile users to scalable 3D image-like tensor to best characterize partially observed mobile network. Second, inspired by AlphaGo, we introduce a novel self-play mechanism to empower DRL agents to gradually improve intelligence by competing for best record on multiple tasks, just like athletes compete for world record in decathlon. Third, a decentralized DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen mobile networks in another simulator as well as 6 field trials on 4 commercial mobile networks in real world, we demonstrate the capability of this sim-to-real framework to direct transfer the learning not only from one simulator to another simulator, but also from simulation to real world. This is the first time that a DRL agent successfully transfers its learning directly from simulation to very complex real world problems with imperfect information, complex rules, huge state/action space, and multi-agent interactions.},
archivePrefix = {arXiv},
arxivId = {1802.06416},
author = {Tan, Yongxi and Yang, Jin and Chen, Xin and Song, Qitao and Chen, Yunjun and Ye, Zhangxiang and Su, Zhenqiang},
eprint = {1802.06416},
month = {feb},
title = {{Sim-to-Real Optimization of Complex Real World Mobile Network with Imperfect Information via Deep Reinforcement Learning from Self-play}},
url = {http://arxiv.org/abs/1802.06416},
year = {2018}
}
@article{Hegde2018,
abstract = {Convolutional Neural Networks (CNNs) have begun to permeate all corners of electronic society (from voice recognition to scene generation) due to their high accuracy and machine efficiency per operation. At their core, CNN computations are made up of multi-dimensional dot products between weight and input vectors. This paper studies how weight repetition ---when the same weight occurs multiple times in or across weight vectors--- can be exploited to save energy and improve performance during CNN inference. This generalizes a popular line of work to improve efficiency from CNN weight sparsity, as reducing computation due to repeated zero weights is a special case of reducing computation due to repeated weights. To exploit weight repetition, this paper proposes a new CNN accelerator called the Unique Weight CNN Accelerator (UCNN). UCNN uses weight repetition to reuse CNN sub-computations (e.g., dot products) and to reduce CNN model size when stored in off-chip DRAM ---both of which save energy. UCNN further improves performance by exploiting sparsity in weights. We evaluate UCNN with an accelerator-level cycle and energy model and with an RTL implementation of the UCNN processing element. On three contemporary CNNs, UCNN improves throughput-normalized energy consumption by 1.2x - 4x, relative to a similarly provisioned baseline accelerator that uses Eyeriss-style sparsity optimizations. At the same time, the UCNN processing element adds only 17-24{\%} area overhead relative to the same baseline.},
archivePrefix = {arXiv},
arxivId = {1804.06508},
author = {Hegde, Kartik and Yu, Jiyong and Agrawal, Rohit and Yan, Mengjia and Pellauer, Michael and Fletcher, Christopher W.},
eprint = {1804.06508},
month = {apr},
title = {{UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition}},
url = {http://arxiv.org/abs/1804.06508},
year = {2018}
}
@article{Tremblay2018,
abstract = {We present a new dataset, called Falling Things (FAT), for advancing the state-of-the-art in object detection and 3D pose estimation in the context of robotics. By synthetically combining object models and backgrounds of complex composition and high graphical quality, we are able to generate photorealistic images with accurate 3D pose annotations for all objects in all images. Our dataset contains 60k annotated photos of 21 household objects taken from the YCB dataset. For each image, we provide the 3D poses, per-pixel class segmentation, and 2D/3D bounding box coordinates for all objects. To facilitate testing different input modalities, we provide mono and stereo RGB images, along with registered dense depth images. We describe in detail the generation process and statistical analysis of the data.},
archivePrefix = {arXiv},
arxivId = {1804.06534},
author = {Tremblay, Jonathan and To, Thang and Birchfield, Stan},
eprint = {1804.06534},
month = {apr},
title = {{Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation}},
url = {http://arxiv.org/abs/1804.06534},
year = {2018}
}
@article{Tremblay2018a,
archivePrefix = {arXiv},
arxivId = {1804.06516},
author = {Tremblay, Jonathan and Prakash, Aayush and Acuna, David and Brophy, Mark and Jampani, Varun and Anil, Cem and To, Thang and Cameracci, Eric and Boochoon, Shaad and Birchfield, Stan},
eprint = {1804.06516},
file = {:home/phil/Downloads/1804.06516.pdf:pdf},
month = {apr},
title = {{Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization}},
url = {https://arxiv.org/abs/1804.06516},
year = {2018}
}
@article{Li2018,
archivePrefix = {arXiv},
arxivId = {1804.06215},
author = {Li, Zeming and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Deng, Yangdong and Sun, Jian},
eprint = {1804.06215},
month = {apr},
title = {{DetNet: A Backbone network for Object Detection}},
url = {https://arxiv.org/abs/1804.06215},
year = {2018}
}
@article{Lukezic2018,
archivePrefix = {arXiv},
arxivId = {1804.07056},
author = {Luke{\v{z}}i{\v{c}}, Alan and Zajc, Luka {\v{C}}ehovin and Voj{\'{i}}ř, Tom{\'{a}}{\v{s}} and Matas, Jiř{\'{i}} and Kristan, Matej},
eprint = {1804.07056},
month = {apr},
title = {{Now you see me: evaluating performance in long-term visual tracking}},
url = {https://arxiv.org/abs/1804.07056},
year = {2018}
}
@article{Wang2018,
archivePrefix = {arXiv},
arxivId = {1804.06882},
author = {Wang, Robert J. and Li, Xiang and Ao, Shuang and Ling, Charles X.},
eprint = {1804.06882},
month = {apr},
title = {{Pelee: A Real-Time Object Detection System on Mobile Devices}},
url = {https://arxiv.org/abs/1804.06882},
year = {2018}
}
@article{Meilland,
abstract = {Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations ex-ist simultaneously, no models have been proposed to han-dle them together. Furthermore, neither deformation has been considered previously in the context of monocular full-image 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this pa-per a complete dense 3D registration model will be derived to account for both motion blur and rolling shutter deforma-tions simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstrated for complex scenarios where both blur and shutter deformations are dominant.},
author = {Meilland, Maxime and Drummond, Tom and Comport, Andrew I},
doi = {10.1109/ICCV.2013.252},
title = {{A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration}},
url = {http://openaccess.thecvf.com/content{\_}iccv{\_}2013/papers/Meilland{\_}A{\_}Unified{\_}Rolling{\_}2013{\_}ICCV{\_}paper.pdf}
}
@article{Le,
abstract = {—We draw a formal connection between using syn-thetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural net-work trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.},
author = {Le, Tuan Anh and Baydin, Atılım G{\"{u}}ne and Zinkov, Robert and Wood, Frank},
title = {{Using Synthetic Data to Train Neural Networks is Model-Based Reasoning}},
url = {http://www.robots.ox.ac.uk/{~}fwood/assets/pdf/le2016synthetic.pdf}
}
@article{Rozantsev,
abstract = {We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited line of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances. A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available.},
author = {Rozantsev, Artem and Lepetit, Vincent and Fua, Pascal},
keywords = {object detection,synthetic data,synthetic image rendering},
title = {{On Rendering Synthetic Images for Training an Object Detector}},
url = {https://arxiv.org/pdf/1411.7911.pdf}
}
@article{Moysset2016,
abstract = {The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.},
archivePrefix = {arXiv},
arxivId = {1611.05664},
author = {Moysset, Bastien and Kermorvant, Christoper and Wolf, Christian},
eprint = {1611.05664},
month = {nov},
title = {{Learning to detect and localize many objects from few examples}},
url = {http://arxiv.org/abs/1611.05664},
year = {2016}
}
@article{Najibi,
abstract = {We introduce G-CNN, an object detection technique based on CNNs which works without proposal algorithms. G-CNN starts with a multi-scale grid of fixed bounding boxes. We train a regressor to move and scale elements of the grid towards objects iteratively. G-CNN models the problem of object detection as finding a path from a fixed grid to boxes tightly surrounding the objects. G-CNN with around 180 boxes in a multi-scale grid performs compara-bly to Fast R-CNN which uses around 2K bounding boxes generated with a proposal technique. This strategy makes detection faster by removing the object proposal stage as well as reducing the number of boxes to be processed.},
author = {Najibi, Mahyar and Rastegari, Mohammad and Davis, Larry S},
title = {{G-CNN: an Iterative Grid Based Object Detector}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Najibi{\_}G-CNN{\_}An{\_}Iterative{\_}CVPR{\_}2016{\_}paper.pdf}
}
@article{Kong,
abstract = {Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recal-l, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Fea-tures well incorporate deep but highly semantic, interme-diate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating pro-posals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detec-tion accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (in-cluding all steps) on a GPU, thus having the potential for real-time processing.},
author = {Kong, Tao and Yao, Anbang and Chen, Yurong and Sun, Fuchun},
title = {{HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Kong{\_}HyperNet{\_}Towards{\_}Accurate{\_}CVPR{\_}2016{\_}paper.pdf}
}
@article{Rauber2017,
abstract = {Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox . The most up-to-date documentation can be found at http://foolbox.readthedocs.io .},
archivePrefix = {arXiv},
arxivId = {1707.04131},
author = {Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},
eprint = {1707.04131},
month = {jul},
title = {{Foolbox: A Python toolbox to benchmark the robustness of machine learning models}},
url = {http://arxiv.org/abs/1707.04131},
year = {2017}
}
@article{Park2018,
abstract = {This paper improves state-of-the-art visual object trackers that use online adaptation. Our core contribution is an offline meta-learning-based method to adjust the initial deep networks used in online adaptation-based tracking. The meta learning is driven by the goal of deep networks that can quickly be adapted to robustly model a particular target in future frames. Ideally the resulting models focus on features that are useful for future frames, and avoid overfitting to background clutter, small parts of the target, or noise. By enforcing a small number of update iterations during meta-learning, the resulting networks train significantly faster. We demonstrate this approach on top of the high performance tracking approaches: tracking-by-detection based MDNet and the correlation based CREST. Experimental results on standard benchmarks, OTB2015 and VOT2016, show that our meta-learned versions of both trackers improve speed, accuracy, and robustness.},
archivePrefix = {arXiv},
arxivId = {1801.03049},
author = {Park, Eunbyung and Berg, Alexander C.},
eprint = {1801.03049},
month = {jan},
title = {{Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers}},
url = {http://arxiv.org/abs/1801.03049},
year = {2018}
}
@article{Groh2018,
abstract = {The goal of this work is to enable deep neural networks to learn representations for irregular 3D structures -- just like in common approaches for 2D images. Unfortunately, current network primitives such as convolution layers are specifically designed to exploit the natural data representation of images -- a fixed and regular grid structure. This represents a limitation when transferring these techniques to more unstructured data like 3D point clouds or higher dimensional data. In this work, we propose a surprisingly natural generalization flex-convolution of the conventional convolution layer and provide a highly efficient implementation. Compared to very specific neural network architectures for point cloud processing, our more generic approach yields competitive results on the rather small standard benchmark sets using fewer parameters and lower memory consumption. Our design even allows for raw neural networks prediction on several magnitudes larger point clouds, providing superior results compared to previous hand-tuned and well-engineered approaches on the 2D-3D-S dataset.},
archivePrefix = {arXiv},
arxivId = {1803.07289},
author = {Groh, Fabian and Wieschollek, Patrick and Lensch, Hendrik P. A.},
eprint = {1803.07289},
month = {mar},
title = {{Flex-Convolution (Deep Learning Beyond Grid-Worlds)}},
url = {http://arxiv.org/abs/1803.07289},
year = {2018}
}
@article{Yang2018,
abstract = {Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block. As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, the capacity of our model is not determined by the network size as with other trackers -- the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.},
archivePrefix = {arXiv},
arxivId = {1803.07268},
author = {Yang, Tianyu and Chan, Antoni B.},
eprint = {1803.07268},
month = {mar},
title = {{Learning Dynamic Memory Networks for Object Tracking}},
url = {http://arxiv.org/abs/1803.07268},
year = {2018}
}
@article{Fathy2018,
abstract = {Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform hand-crafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.},
archivePrefix = {arXiv},
arxivId = {1803.07231},
author = {Fathy, Mohammed E. and Tran, Quoc-Huy and Zia, M. Zeeshan and Vernaza, Paul and Chandraker, Manmohan},
eprint = {1803.07231},
month = {mar},
title = {{Hierarchical Metric Learning and Matching for 2D and 3D Geometric Correspondences}},
url = {http://arxiv.org/abs/1803.07231},
year = {2018}
}
@article{Zhu2018a,
abstract = {As we move towards large-scale object detection, it is unrealistic to expect annotated training data for all object classes at sufficient scale, and so methods capable of unseen object detection are required. We propose a novel zero-shot method based on training an end-to-end model that fuses semantic attribute prediction with visual features to propose object bounding boxes for seen and unseen classes. While we utilize semantic features during training, our method is agnostic to semantic information for unseen classes at test-time. Our method retains the efficiency and effectiveness of YOLO for objects seen during training, while improving its performance for novel and unseen objects. The ability of state-of-art detection methods to learn discriminative object features to reject background proposals also limits their performance for unseen objects. We posit that, to detect unseen objects, we must incorporate semantic information into the visual domain so that the learned visual features reflect this information and leads to improved recall rates for unseen objects. We test our method on PASCAL VOC and MS COCO dataset and observed significant improvements on the average precision of unseen classes.},
archivePrefix = {arXiv},
arxivId = {1803.07113},
author = {Zhu, Pengkai and Wang, Hanxiao and Bolukbasi, Tolga and Saligrama, Venkatesh},
eprint = {1803.07113},
month = {mar},
title = {{Zero-Shot Detection}},
url = {http://arxiv.org/abs/1803.07113},
year = {2018}
}
@article{Jarrett,
abstract = {In many recent object recognition systems, feature ex-traction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extrac-tion in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three ques-tions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learn-ing the filter banks in an unsupervised or supervised man-ner improve the performance over random filters or hard-wired filters? 3. Is there any advantage to using an ar-chitecture with two stages of feature extraction, rather than one? We show that using non-linearities that include recti-fication and local contrast normalization is the single most important ingredient for good accuracy on object recogni-tion benchmarks. We show that two stages of feature ex-traction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the sys-tem achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by super-vised refinement produces good accuracy on Caltech-101 ({\textgreater} 65{\%}), and the lowest known error rate on the undis-torted, unprocessed MNIST dataset (0.53{\%}).},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc Aurelio and Lecun, Yann},
title = {{What is the Best Multi-Stage Architecture for Object Recognition?}},
url = {http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf}
}
@article{Rada,
abstract = {To be robust to illumination changes when detecting objects in images, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is very cumbersome, or sometimes even impossible, for some applications such as 3D pose estimation of specific objects, which is the application we focus on in this paper. We therefore propose a novel illumination normalization method that lets us learn to detect objects and estimate their 3D pose under challenging illumination conditions from very few training samples. Our key insight is that normalization parameters should adapt to the input image. In particular, we realized this via a Convolutional Neural Network trained to predict the parameters of a generalization of the Difference-of-Gaussians method. We show that our method significantly outperforms standard normalization methods and demonstrate it on two challenging 3D detection and pose estimation problems.},
author = {Rad, Mahdi and Roth, Peter M and Lepetit, Vincent},
keywords = {Cheese},
title = {{ALCN: Meta-Learning for Contrast Normalization Applied to Robust 3D Pose Estimation}},
url = {https://arxiv.org/pdf/1708.09633.pdf}
}
@article{Liao2015,
abstract = {Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
archivePrefix = {arXiv},
arxivId = {1508.00330},
author = {Liao, Zhibin and Carneiro, Gustavo},
eprint = {1508.00330},
month = {aug},
title = {{On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units}},
url = {http://arxiv.org/abs/1508.00330},
year = {2015}
}
@article{Shu2018,
abstract = {Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin {\&} Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1802.08735},
author = {Shu, Rui and Bui, Hung H. and Narui, Hirokazu and Ermon, Stefano},
eprint = {1802.08735},
month = {feb},
title = {{A DIRT-T Approach to Unsupervised Domain Adaptation}},
url = {http://arxiv.org/abs/1802.08735},
year = {2018}
}
@article{Chen2018b,
abstract = {Temporal object detection has attracted significant attention, but most popular detection methods can not leverage the rich temporal information in video or robotic vision. Although many different algorithms have been developed for video detection task, real-time online approaches are frequently deficient. In this paper, based on attention mechanism and convolutional long short-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD) for robotic vision. Distinct from previous methods, we take aim at temporally integrating pyramidal feature hierarchy using ConvLSTM, and design a novel structure including a high-level temporal unit as well as a low-level one (HL-TU) for multi-scale feature maps. Moreover, we develop a creative temporal analysis unit, namely, attention-aware ConvLSTM (AC-LSTM), in which a temporal attention module is specially tailored for background suppression and scale suppression while ConvLSTM temporally integrates attention-aware features. An association loss is designed for temporal coherence. Finally, our method is evaluated on ImageNet VID dataset. Extensive comparisons on the detection capability confirm or validate the superiority of the proposed approach. Consequently, the developed TSSD is fairly faster and achieves an overall competitive performance in terms of mean average precision. As a temporal, real-time, and online detector, TSSD is applicable to robot's intelligent perception.},
archivePrefix = {arXiv},
arxivId = {1803.00197},
author = {Chen, Xingyu and Wu, Zhengxing and Yu, Junzhi},
eprint = {1803.00197},
month = {feb},
title = {{TSSD: Temporal Single-Shot Object Detection Based on Attention-Aware LSTM}},
url = {http://arxiv.org/abs/1803.00197},
year = {2018}
}
@article{Dong2017,
abstract = {In the same vein of discriminative one-shot learning, Siamese networks allow recognizing an object from a single exemplar with the same class label. However, they do not take advantage of the underlying structure of the data and the relationship among the multitude of samples as they only rely on pairs of instances for training. In this paper, we propose a new quadruplet deep network to examine the potential connections among the training instances, aiming to achieve a more powerful representation. We design four shared networks that receive multi-tuple of instances as inputs and are connected by a novel loss function consisting of pair-loss and triplet-loss. According to the similarity metric, we select the most similar and the most dissimilar instances as the positive and negative inputs of triplet loss from each multi-tuple. We show that this scheme improves the training performance. Furthermore, we introduce a new weight layer to automatically select suitable combination weights, which will avoid the conflict between triplet and pair loss leading to worse performance. We evaluate our quadruplet framework by model-free tracking-by-detection of objects from a single initial exemplar in several Visual Object Tracking benchmarks. Our extensive experimental analysis demonstrates that our tracker achieves superior performance with a real-time processing speed of 78 frames-per-second (fps).},
archivePrefix = {arXiv},
arxivId = {1705.07222},
author = {Dong, Xingping and Shen, Jianbing and Liu, Yu and Wang, Wenguan and Porikli, Fatih},
eprint = {1705.07222},
month = {may},
title = {{Quadruplet Network with One-Shot Learning for Fast Visual Object Tracking}},
url = {http://arxiv.org/abs/1705.07222},
year = {2017}
}
@article{Varadarajan2018,
abstract = {We propose a weakly supervised method using two algorithms to predict object bounding boxes given only an image classification dataset. First algorithm is a simple Fully Convolutional Network (FCN) trained to classify object instances. We use the property of FCN to return a mask for images larger than training images to get a primary output segmentation mask during test time by passing an image pyramid to it. We enhance the FCN output mask into final output bounding boxes by a Convolutional Encoder-Decoder (ConvAE) viz. the second algorithm. ConvAE is trained to localize objects on an artificially generated dataset of output segmentation masks. We demonstrate the effectiveness of this method in localizing objects in grocery shelves where annotating data for object detection is hard due to variety of objects. This method can be extended to any problem domain where collecting images of objects is easy and annotating their coordinates is hard.},
archivePrefix = {arXiv},
arxivId = {1803.06813},
author = {Varadarajan, Srikrishna and Srivastava, Muktabh Mayank},
eprint = {1803.06813},
month = {mar},
title = {{Weakly Supervised Object Localization on grocery shelves using simple FCN and Synthetic Dataset}},
url = {http://arxiv.org/abs/1803.06813},
year = {2018}
}
@article{Wei2018,
abstract = {A significant challenge in object detection is accurate identification of an object's position in image space, whereas one algorithm with one set of parameters is usually not enough, and the fusion of multiple algorithms and/or parameters can lead to more robust results. Herein, a new computational intelligence fusion approach based on the dynamic analysis of agreement among object detection outputs is proposed. Furthermore, we propose an online versus just in training image augmentation strategy. Experiments comparing the results both with and without fusion are presented. We demonstrate that the augmented and fused combination results are the best, with respect to higher accuracy rates and reduction of outlier influences. The approach is demonstrated in the context of cone, pedestrian and box detection for Advanced Driver Assistance Systems (ADAS) applications.},
archivePrefix = {arXiv},
arxivId = {1803.06554},
author = {Wei, Pan and Ball, John E. and Anderson, Derek T.},
eprint = {1803.06554},
month = {mar},
title = {{Fusion of an Ensemble of Augmented Image Detectors for Robust Object Detection}},
url = {http://arxiv.org/abs/1803.06554},
year = {2018}
}
@article{Gupta2018,
abstract = {We present here, a novel network architecture called MergeNet for discovering small obstacles for on-road scenes in the context of autonomous driving. The basis of the architecture rests on the central consideration of training with less amount of data since the physical setup and the annotation process for small obstacles is hard to scale. For making effective use of the limited data, we propose a multi-stage training procedure involving weight-sharing, separate learning of low and high level features from the RGBD input and a refining stage which learns to fuse the obtained complementary features. The model is trained and evaluated on the Lost and Found dataset and is able to achieve state-of-art results with just 135 images in comparison to the 1000 images used by the previous benchmark. Additionally, we also compare our results with recent methods trained on 6000 images and show that our method achieves comparable performance with only 1000 training samples.},
archivePrefix = {arXiv},
arxivId = {1803.06508},
author = {Gupta, Krishnam and Javed, Syed Ashar and Gandhi, Vineet and Krishna, K. Madhava},
eprint = {1803.06508},
month = {mar},
title = {{MergeNet: A Deep Net Architecture for Small Obstacle Discovery}},
url = {http://arxiv.org/abs/1803.06508},
year = {2018}
}


@inproceedings{Chen2018c,
	title={Domain adaptive faster r-cnn for object detection in the wild},
	author={Chen, Yuhua and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={3339--3348},
	year={2018}
}

@article{Vass,
abstract = {Lens distortion is often an issue in post production houses when combining footage taken with different cameras or integrating computer graphics into live action plates. While this error of the imaging process has been studied thoroughly for many years in the field of computer vision, almost none of the existing tools have all the key features users need. Some algorithms work well for any kind of distortion but are hard to calibrate, while others are fully automatic but fail for fisheye or anamorphic lenses. In this paper the different approaches of removing lens distortion are summarized, and a semi-automatic system is introduced that fits the need of post production facilities.},
author = {Vass, Gergely and Perlaki, Tam{\'{a}}s},
title = {{Applying and removing lens distortion in post production}},
url = {http://www.vassg.hu/pdf/vass{\_}gg{\_}2003{\_}lo.pdf}
}
@article{Li2018a,
abstract = {Recent work has tackled the problem of autonomous navigation by imitating a teacher and learning an end-to-end policy, which directly predicts controls from raw images. However, these approaches tend to be sensitive to mistakes by the teacher and do not scale well to other environments or vehicles. To this end, we propose a modular network architecture that decouples perception from control, and is trained using Observational Imitation Learning (OIL), a novel imitation learning variant that supports online training and automatic selection of optimal behavior from observing multiple teachers. We apply our proposed methodology to the challenging problem of unmanned aerial vehicle (UAV) racing. We develop a simulator that enables the generation of large amounts of synthetic training data (both UAV captured images and its controls) and also allows for online learning and evaluation. We train a perception network to predict waypoints from raw image data and a control network to predict UAV controls from these waypoints using OIL. Our modular network is able to autonomously fly a UAV through challenging race tracks at high speeds. Extensive experiments demonstrate that our trained network outperforms its teachers, end-to-end baselines, and even human pilots in simulation. The supplementary video can be viewed at https://youtu.be/PeTXSoriflc},
archivePrefix = {arXiv},
arxivId = {1803.01129},
author = {Li, Guohao and Mueller, Matthias and Casser, Vincent and Smith, Neil and Michels, Dominik L and Ghanem, Bernard},
eprint = {1803.01129},
month = {mar},
title = {{Teaching UAVs to Race With Observational Imitation Learning}},
url = {https://arxiv.org/pdf/1803.01129.pdf http://arxiv.org/abs/1803.01129},
year = {2018}
}
@article{Ning,
abstract = {In this paper, we develop a new approach of spatially supervised recurrent convo-lutional neural networks for visual object tracking. Our recurrent convolutional network exploits the history of locations as well as the distinctive visual features learned by the deep neural networks. Inspired by recent bounding box regres-sion methods for object detection, we study the regression capability of Long Short-Term Memory (LSTM) in the temporal domain, and propose to concate-nate high-level visual features produced by convolutional networks with region information. In contrast to existing deep learning based trackers that use binary classification for region candidates, we use regression for direct prediction of the tracking locations both at the convolutional layer and at the recurrent unit. Our extensive experimental results and performance comparison with state-of-the-art tracking methods on challenging benchmark video tracking datasets shows that our tracker is more accurate and robust while maintaining low computational cost. For most test video sequences, our method achieves the best tracking performance, often outperforms the second best by a large margin.},
author = {Ning, Guanghan and Zhang, Zhi and Huang, Chen and He, Zhihai and Ren, Xiaobo and Wang, Haohong},
title = {{Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking}},
url = {https://arxiv.org/pdf/1607.05781.pdf}
}
@incollection{Lee2018,
author = {Lee, Jinho and Iwana, Brian Kenji and Ide, Shouta and Hayashi, Hideaki and Uchida, Seiichi},
doi = {10.1007/978-3-319-75786-5_10},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2018 - Globally Optimal Object Tracking with Complementary Use of Single Shot Multibox Detector and Fully Convolutional Net.pdf:pdf},
pages = {110--122},
title = {{Globally Optimal Object Tracking with Complementary Use of Single Shot Multibox Detector and Fully Convolutional Network}},
url = {http://link.springer.com/10.1007/978-3-319-75786-5{\_}10},
year = {2018}
}
@inproceedings{Kunii2017,
author = {Kunii, Yasuharu and Kovacs, Gabor and Hoshi, Naoaki},
booktitle = {2017 IEEE 26th International Symposium on Industrial Electronics (ISIE)},
doi = {10.1109/ISIE.2017.8001512},
isbn = {978-1-5090-1412-5},
month = {jun},
pages = {1747--1752},
publisher = {IEEE},
title = {{Mobile robot navigation in natural environments using robust object tracking}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=8001512 http://ieeexplore.ieee.org/document/8001512/},
year = {2017}
}
@article{Ning2016,
abstract = {In this paper, we develop a new approach of spatially supervised recurrent convolutional neural networks for visual object tracking. Our recurrent convolutional network exploits the history of locations as well as the distinctive visual features learned by the deep neural networks. Inspired by recent bounding box regression methods for object detection, we study the regression capability of Long Short-Term Memory (LSTM) in the temporal domain, and propose to concatenate high-level visual features produced by convolutional networks with region information. In contrast to existing deep learning based trackers that use binary classification for region candidates, we use regression for direct prediction of the tracking locations both at the convolutional layer and at the recurrent unit. Our extensive experimental results and performance comparison with state-of-the-art tracking methods on challenging benchmark video tracking datasets shows that our tracker is more accurate and robust while maintaining low computational cost. For most test video sequences, our method achieves the best tracking performance, often outperforms the second best by a large margin.},
archivePrefix = {arXiv},
arxivId = {1607.05781},
author = {Ning, Guanghan and Zhang, Zhi and Huang, Chen and He, Zhihai and Ren, Xiaobo and Wang, Haohong},
eprint = {1607.05781},
month = {jul},
title = {{Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking}},
url = {http://arxiv.org/abs/1607.05781},
year = {2016}
}
@article{Domingos,
abstract = {Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is of-ten feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer science and other fields. However, developing successful machine learning applications requires a substantial amount of " black art " that is hard to find in textbooks. This article summarizes twelve key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions.},
author = {Domingos, Pedro},
title = {{A Few Useful Things to Know about Machine Learning}},
url = {https://homes.cs.washington.edu/{~}pedrod/papers/cacm12.pdf}
}

@inproceedings{Girshick2013,
	title={Rich feature hierarchies for accurate object detection and semantic segmentation},
	author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={580--587},
	year={2014}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
month = {dec},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{DeCroon,
abstract = {We propose a novel gaze-control model for detecting objects in images. The model, named ACT-DETECT, uses the information from local image samples in order to shift its gaze towards object locations. The model consti-tutes two main contributions. The first contribution is that the model's setup makes it computationally highly efficient in comparison with existing window-sliding methods for object detection, while retaining an acceptable detection performance. ACT-DETECT is evaluated on a face-detection task using a publicly available image set. In terms of detection performance, ACT-DETECT slightly outperforms the window-sliding methods that have been applied to the face-detection task. In terms of computational efficiency, ACT-DETECT clearly outperforms the window-sliding methods: it requires in the order of hundreds fewer samples for detection. The second contribution of the model lies in its more extensive use of local samples than previous models: instead of merely using them for verifying object presence at the gaze location, the model uses them to determine a direction and distance to the object of interest. The simultaneous adaptation of both the model's visual features and its gaze-control strategy leads to the discovery of features and strategies for exploiting the local context of objects. For example, the model uses the spatial relations between the bodies of the persons in the images and their faces. The resulting gaze control is a temporal process, in which the object's context is exploited at different scales and at different image locations relative to the object.},
author = {{De Croon}, G C H E and Postma, @bullet E O and {Van Den Herik}, @bullet H J},
doi = {10.1007/s12559-010-9093-9},
keywords = {Active vision {\'{A}},Computationally efficient object detection {\'{A}},Evolutionary algorithms,Gaze control {\'{A}}},
title = {{Adaptive Gaze Control for Object Detection}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs12559-010-9093-9.pdf}
}
@article{Papadopoulos,
abstract = {Training object class detectors typically requires a large set of images in which objects are annotated by bounding-boxes. However, manually drawing bounding-boxes is very time consuming. We propose a new scheme for training object detectors which only requires annotators to verify bounding-boxes produced automatically by the learning al-gorithm. Our scheme iterates between re-training the de-tector, re-localizing objects in the training images, and hu-man verification. We use the verification signal both to improve re-training and to reduce the search space for re-localisation, which makes these steps different to what is normally done in a weakly supervised setting. Exten-sive experiments on PASCAL VOC 2007 show that (1) us-ing human verification to update detectors and reduce the search space leads to the rapid production of high-quality bounding-box annotations; (2) our scheme delivers detec-tors performing almost as good as those trained in a fully supervised setting, without ever drawing any bounding-box; (3) as the verification task is very quick, our scheme sub-stantially reduces total annotation time by a factor 6×-9×.},
author = {Papadopoulos, Dim P and Uijlings, Jasper R R and Keller, Frank and Ferrari, Vittorio},
title = {{We don't need no bounding-boxes: Training object class detectors using only human verification}},
url = {http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/papadopoulos16cvpr.pdf}
}
@article{Zhang2017,
abstract = {We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of object detection features within a typical deep detector, by a semantic segmentation branch and a location-agnostic module. The segmentation branch is supervised by weak segmentation ground-truth, i.e., no extra annotation is required. In conjunction with that, we employ a location-agnostic module which learns relationship between channels and object classes in a self-supervised manner. Comprehensive experimental results on both PASCAL VOC and MS COCO detection datasets demonstrate the effectiveness of the proposed method. In particular, with a VGG16 based DES, we achieve an mAP of 81.6 on VOC2007 test and an mmAP of 32.8 on COCO test-dev with an inference speed of 36.7 milliseconds per image on a Titan X Pascal GPU. With a lower resolution version, we achieve an mAP of 79.5 on VOC2007 with an inference speed of 14.7 milliseconds per image.},
archivePrefix = {arXiv},
arxivId = {1712.00433},
author = {Zhang, Zhishuai and Qiao, Siyuan and Xie, Cihang and Shen, Wei and Wang, Bo and Yuille, Alan L.},
eprint = {1712.00433},
month = {dec},
title = {{Single-Shot Object Detection with Enriched Semantics}},
url = {http://arxiv.org/abs/1712.00433},
year = {2017}
}
@article{Loquercio,
author = {Loquercio, Antonio and Maqueda, Ana I and Del-Blanco, Carlos R and Scaramuzza, Davide},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loquercio et al. - Unknown - DroNet Learning to Fly by Driving.pdf:pdf},
title = {{DroNet: Learning to Fly by Driving}},
url = {http://rpg.ifi.uzh.ch/docs/RAL18{\_}Loquercio.pdf}
}
@article{Kong2016,
abstract = {Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.},
archivePrefix = {arXiv},
arxivId = {1604.00600},
author = {Kong, Tao and Yao, Anbang and Chen, Yurong and Sun, Fuchun},
eprint = {1604.00600},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong et al. - 2016 - HyperNet Towards Accurate Region Proposal Generation and Joint Object Detection.pdf:pdf},
month = {apr},
title = {{HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection}},
url = {http://arxiv.org/abs/1604.00600},
year = {2016}
}
@article{Cai2016,
abstract = {A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.07155v1},
author = {Cai, Zhaowei and Fan, Quanfu and Feris, Rogerio S and Vasconcelos, Nuno},
eprint = {arXiv:1607.07155v1},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai et al. - 2016 - A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection.pdf:pdf},
keywords = {()},
title = {{A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection}},
url = {https://arxiv.org/pdf/1607.07155.pdf},
year = {2016}
}
@article{Winter,
abstract = {3D reconstruction and pose estimation have been huge areas of re-search in recent years. Success in these two areas have allowed enormous strides in augmented reality, 3D scanning, and interac-tive gaming. However, utilizing machine learning in this realm has been a difficulty since creating relevant datasets is extremely time consuming. Additionally, 3D annotated video datasets do not exist yet. By rendering 3D objects, we create a dataset of image se-quences to emulate video frame data. Using a VGG net [Simonyan and Zisserman 2014] and a RNN, we introduce a model that can predict a object's 3D bounding box based on synthetic video-like image sequences. Our model can be extended to predict 3D feature points to create point cloud data for 3D reconstruction.},
author = {Winter, Hanna K},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winter - Unknown - 3D Point Estimation Using A Recursive Neural Network.pdf:pdf},
title = {{3D Point Estimation Using A Recursive Neural Network}},
url = {http://cs229.stanford.edu/proj2016/report/Winter{\_}3DPositionEstimationUsingARecursiveNeuralNetwork{\_}report.pdf}
}
@article{TekinEPFL,
abstract = {We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [10] that only predicts an approx-imate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a re-sult, it is much faster – 50 fps on a Titan X (Pascal) GPU – and more suitable for real-time processing. The key com-ponent of our method is a new CNN architecture inspired by [27, 28] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our ap-proach substantially outperforms other recent CNN-based approaches [10, 25] when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.},
author = {{Tekin EPFL}, Bugra and Sinha, Sudipta N and {Fua EPFL}, Pascal},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tekin EPFL, Sinha, Fua EPFL - Unknown - Real-Time Seamless Single Shot 6D Object Pose Prediction.pdf:pdf},
title = {{Real-Time Seamless Single Shot 6D Object Pose Prediction}},
url = {https://arxiv.org/pdf/1711.08848.pdf}
}
@article{Mousavian,
abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current tech-niques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then com-bines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The sec-ond output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation im-posed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Al-though conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmenta-tion and flat ground priors [4] and sub-category detec-tion [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Ko{\v{s}}eck{\'{a}}, Jana},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mousavian et al. - Unknown - 3D Bounding Box Estimation Using Deep Learning and Geometry.pdf:pdf},
title = {{3D Bounding Box Estimation Using Deep Learning and Geometry}},
url = {https://arxiv.org/pdf/1612.00496.pdf}
}
@article{Poirson,
abstract = {For applications in navigation and robotics, estimating the 3D pose of objects is as important as detection. Many approaches to pose estimation rely on detecting or track-ing parts or keypoints [11, 21]. In this paper we build on a recent state-of-the-art convolutional network for sliding-window detection [10] to provide detection and rough pose estimation in a single shot, without intermediate stages of detecting parts or initial bounding boxes. While not the first system to treat pose estimation as a categorization problem, this is the first attempt to combine detection and pose esti-mation at the same level using a deep learning approach. The key to the architecture is a deep convolutional network where scores for the presence of an object category, the off-set for its location, and the approximate pose are all es-timated on a regular grid of locations in the image. The resulting system is as accurate as recent work on pose esti-mation (42.4{\%} 8 View mAVP on Pascal 3D+ [21]) and sig-nificantly faster (46 frames per second (FPS) on a TITAN X GPU). This approach to detection and rough pose estima-tion is fast and accurate enough to be widely applied as a pre-processing step for tasks including high-accuracy pose estimation, object tracking and localization, and vSLAM.},
author = {Poirson, Patrick and Ammirato, Phil and Fu, Cheng-Yang and Liu, Wei and Ko{\v{s}}eck{\'{a}}, Jana and Berg, Alexander C},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poirson et al. - Unknown - Fast Single Shot Detection and Pose Estimation.pdf:pdf},
title = {{Fast Single Shot Detection and Pose Estimation}},
url = {https://arxiv.org/pdf/1609.05590.pdf}
}
@article{Kehl,
abstract = {We present a novel method for detecting 3D model in-stances and estimating their 6D poses from RGB data in a single shot. To this end, we extend the popular SSD paradigm to cover the full 6D pose space and train on syn-thetic model data only. Our approach competes or sur-passes current state-of-the-art methods that leverage RGB-D data on multiple challenging datasets. Furthermore, our method produces these results at around 10Hz, which is many times faster than the related methods. For the sake of reproducibility, we make our trained networks and detec-tion code publicly available.},
author = {Kehl, Wadim and Manhardt, Fabian and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kehl et al. - Unknown - SSD-6D Making RGB-Based 3D Detection and 6D Pose Estimation Great Again.pdf:pdf},
title = {{SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again}},
url = {https://arxiv.org/pdf/1711.10006.pdf}
}
@article{Fiaz,
abstract = {—Visual object tracking is an important computer vision problem with numerous real-world applications includ-ing human-computer interaction, autonomous vehicles, robotics, motion-based recognition, video indexing, surveillance and se-curity. In this paper, we aim to extensively review the latest trends and advances in the tracking algorithms and evaluate the robustness of trackers in the presence of noise. The first part of this work comprises a comprehensive survey of recently proposed tracking algorithms. We broadly categorize trackers into correlation filter based trackers and the others as non-correlation filter trackers. Each category is further classified into various types of trackers based on the architecture of the tracking mechanism. In the second part of this work, we experimentally evaluate tracking algorithms for robustness in the presence of additive white Gaussian noise. Multiple levels of additive noise are added to the Object Tracking Benchmark (OTB) 2015, and the precision and success rates of the tracking algorithms are eval-uated. Some algorithms suffered more performance degradation than others, which brings to light a previously unexplored aspect of the tracking algorithms. The relative rank of the algorithms based on their performance on benchmark datasets may change in the presence of noise. Our study concludes that no single tracker is able to achieve the same efficiency in the presence of noise as under noise-free conditions; thus, there is a need to include a parameter for robustness to noise when evaluating newly proposed tracking algorithms.},
author = {Fiaz, Mustansar and Mahmood, Arif and {Ki Jung}, Soon},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiaz, Mahmood, Ki Jung - Unknown - Tracking Noisy Targets A Review of Recent Object Tracking Approaches.pdf:pdf},
keywords = {Deep learning,Index Terms—Background modeling,Inpaint-ing,foreground detection},
title = {{Tracking Noisy Targets: A Review of Recent Object Tracking Approaches}},
url = {https://arxiv.org/pdf/1802.03098.pdf}
}
@article{Li,
abstract = {—Image-based localization, or camera relocalization, is a fundamental problem in computer vision and robotics, and it refers to estimating camera pose from an image. Recent state-of-the-art approaches use learning based methods, such as Random Forests (RFs) and Convolutional Neural Networks (CNNs), to regress for each pixel in the image its corresponding position in the scene's world coordinate frame, and solve the final pose via a RANSAC-based optimization scheme using the predicted correspondences. In this paper, instead of in a patch-based manner, we propose to perform the scene coordinate regression in a full-frame manner to make the computation efficient at test time and, more importantly, to add more global context to the regression process to improve the robustness. To do so, we adopt a fully convolutional encoder-decoder neural network architecture which accepts a whole image as input and produces scene coordinate predictions for all pixels in the image. However, using more global context is prone to overfitting. To alleviate this issue, we propose to use data augmentation to generate more data for training. In addition to the data augmentation in 2D image space, we also augment the data in 3D space. We evaluate our approach on the publicly available 7-Scenes dataset, and experiments show that it has better scene coordinate predictions and achieves state-of-the-art results in localization with improved robustness on the hardest frames (e.g., frames with repeated structures).},
author = {Li, Xiaotian and Ylioinas, Juha and Kannala, Juho},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Ylioinas, Kannala - Unknown - Full-Frame Scene Coordinate Regression for Image-Based Localization.pdf:pdf},
title = {{Full-Frame Scene Coordinate Regression for Image-Based Localization}},
url = {https://arxiv.org/pdf/1802.03237.pdf}
}
@article{Kimura,
abstract = {In this paper, we propose imitation networks, a sim-ple but effective method for training neural net-works with a limited amount of training data. Our approach inherits the idea of knowledge distillation that transfers knowledge from a deep or wide ref-erence model to a shallow or narrow target model. The proposed method employs this idea to mimic predictions of reference estimators that are much more robust against overfitting than the network we want to train. Different from almost all the previ-ous work for knowledge distillation that requires a large amount of labeled training data, the proposed method requires only a small amount of training data. Instead, we introduce pseudo training exam-ples that are optimized as a part of model param-eters. Experimental results for several benchmark datasets demonstrate that the proposed method out-performed all the other baselines, such as naive training of the target model and standard knowl-edge distillation.},
author = {Kimura, Akisato and Ghahramani, Zoubin and Takeuchi, Koh and Iwata, Tomoharu and Ueda, Naonori},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kimura et al. - Unknown - Imitation networks Few-shot learning of neural networks from scratch.pdf:pdf},
title = {{Imitation networks: Few-shot learning of neural networks from scratch}},
url = {https://arxiv.org/pdf/1802.03039.pdf}
}
@article{Ashraf,
abstract = {The ability to automatically detect other vehicles on the road is vital to the safety of partially-autonomous and fully-autonomous vehicles. Most of the high-accuracy techniques for this task are based on R-CNN or one of its faster variants. In the research community, much emphasis has been applied to using 3D vision or complex R-CNN variants to achieve higher accuracy. However, are there more straightforward modifications that could deliver higher accuracy? Yes. We show that increasing input image resolution (i.e. upsampling) offers up to 12 percentage-points higher accuracy compared to an off-the-shelf baseline. We also find situations where earlier/shallower layers of CNN provide higher accuracy than later/deeper layers. We further show that shallow models and upsampled images yield competitive accuracy. Our findings contrast with the current trend towards deeper and larger models to achieve high accuracy in domain specific detection tasks.},
author = {Ashraf, Khalid and Wu, Bichen and Iandola, Forrest N and Moskewicz, Mattthew W and Keutzer, Kurt},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashraf et al. - Unknown - Shallow Networks for High-Accuracy Road Object-Detection.pdf:pdf},
title = {{Shallow Networks for High-Accuracy Road Object-Detection}},
url = {https://arxiv.org/pdf/1606.01561.pdf}
}

@inproceedings{Huang,
	title={Speed/accuracy trade-offs for modern convolutional object detectors},
	author={Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and others},
	booktitle={IEEE CVPR},
	volume={4},
	year={2017}
}

@article{Xiang,
abstract = {SSD [18] is one of the state-of-the-art object detection algorithms, and it combines high detection accuracy with real-time speed. However, it is widely recognized that SSD is less accurate in detecting small objects compared to large objects, because it ignores the context from out-side the proposal boxes. In this paper, we present CSSD– a shorthand for context-aware single-shot multibox object detector. CSSD is built on top of SSD, with additional lay-ers modeling multi-scale contexts. We describe two vari-ants of CSSD, which differ in their context layers, using di-lated convolution layers (DiCSSD) and deconvolution lay-ers (DeCSSD) respectively. The experimental results show that the multi-scale context modeling significantly improves the detection accuracy. In addition, we study the relation-ship between effective receptive fields (ERFs) and the the-oretical receptive fields (TRFs), particularly on a VGGNet. The empirical results further strengthen our conclusion that SSD coupled with context layers achieves better detection results especially for small objects (+3.2{\%}AP @0.5 on MS-COCO compared to the newest SSD [19]), while maintain-ing comparable runtime performance.},
author = {Xiang, Wei and Zhang, Dong-Qing and Athitsos, Vassilis and Yu, Heather},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiang et al. - Unknown - Context-aware Single-Shot Detector.pdf:pdf},
title = {{Context-aware Single-Shot Detector}},
url = {https://pdfs.semanticscholar.org/a299/fd58b86c7d92ac617395b2ada496bc097236.pdf}
}
@article{Linb,
abstract = {This paper presents a fast vehicle detector which can be deployed on NVIDIA DrivePX2 under real-time constraints. The network predicts bounding boxes with different aspect ratio and scale priors from the specifically-designed prediction module given concatenated multi-scale feature map. A new data augmentation strategy is proposed to systematically generate a lot of vehicle training images whose appearance is randomly truncated so our detector could detect occluded vehicles better. Besides, we propose a non-region-based online hard example mining framework which performs fine-tuning by picking (1) hard examples and (2) detection results with insufficient IOU. Compared to other classical object detectors, this work achieves very competitive result in terms of average precision (AP) and computational speed. For the newly-defined vehicle class (car+bus) on VOC2007 test, our detector achieves 85.32 AP and runs at 48 FPS and 30 FPS on NVIDIA Titan X {\&} GP106 (DrivePX2), respectively.},
author = {Lin, Che-tsung and Santoso, Patrisia Sherryl and Chen, Shu-ping and Lin, Hung-jin and Lai, Shang-hong},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Fast Vehicle Detector for Autonomous Driving.pdf:pdf},
keywords = {2,2Intelligent Mobility Division,Che-Tsung Lin1,Hsinchu,Hung-Jin Lin1,Industrial Technology Research Institute,Mechanical and Systems Research Laboratories,National Tsing Hua University,Patrisia Sherryl Santoso2,Shang-Hong Lai1 1Department of Computer Science,Shu-Ping Chen1,Taiwan},
title = {{Fast Vehicle Detector for Autonomous Driving}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w3/Lin{\_}Fast{\_}Vehicle{\_}Detector{\_}ICCV{\_}2017{\_}paper.pdf}
}
@inproceedings{TripathiSanDiego,
abstract = {Deep convolutional Neural Networks (CNN) are the state-of-the-art performers for object detection task. It is well known that object detection requires more computation and memory than image classification. Thus the consolidation of a CNN-based object detection for an embedded system is more challenging. In this work, we propose LCDet, a fully-convolutional neural network for generic object detection that aims to work in embedded systems. We design and develop an end-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit quantization on the learned weights. We use face detection as a use case. Our TF-Slim based network can predict different faces of different shapes and sizes in a single forward pass. Our experimental results show that the proposed method achieves comparative accuracy comparing with state-of-the-art CNN-based face detection methods, while reducing the model size by 3x and memory-BW by {\~{}}4x comparing with one of the best real-time CNN-based object detector such as YOLO. TF 8-bit quantized model provides additional 4x memory reduction while keeping the accuracy as good as the floating point model. The proposed model thus becomes amenable for embedded implementations.},
archivePrefix = {arXiv},
arxivId = {1705.05922},
author = {Tripathi, Subarna and Dane, Gokce and Kang, Byeongkeun and Bhaskaran, Vasudev and Nguyen, Truong},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.56},
eprint = {1705.05922},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tripathi et al. - 2017 - LCDet Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
pages = {411--420},
title = {{LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems}},
url = {https://vision.cornell.edu/se3/wp-content/uploads/2017/07/LCDet{\_}CVPRW.pdf},
volume = {2017-July},
year = {2017}
}
@article{Jung,
abstract = {We present an robust visual detection method for indoor autonomous drone racing (ADR). Our unmanned micro aerial vehicle (MAV), that is built with low-cost, off-the-shelf hardware, detect the racing gates with a monocular camera using deep-learning method. The biggest chal-lenging tasks for this ADR is to detect quickly and reliably race gates while avoiding collisions. In this paper, we introduce convolution neural network (CNN) called single shot detector (SSD) to robustly detect the race gate in various indoor light condition and environment. All vision processing, except flight control, runs in real time on the NVIDIA embedded GPU (Tegra X1) and the learning process is implemented on the desktop GPU (NVIDIA GTX980) hardware platform. We provide details about the hardware system and software algorithms used to implement the proposed solution. Based on the learning results, we show that our proposed solution can provide quick and reliable results in the indoor environment through experiments that pass through the race gate. I. Nomenclature N = number of matched default boxes l = predicted box g = ground truth box L con f = confidence loss L loc = localization loss $\alpha$ = weight term c = multiple classes confidences s min = lowest layer scale s max = highest layer scale m = number of feature maps II. Introduction D racing is gaining popularity as one of the new sports. The improved performance of avionics, FPV devices and quadrotor-type MAV makes it possible [1]. Furthermore, autonomous unmanned race competitions were tried in line with the development of indoor navigation technology and the flow of autonomous robot [2]. Vision-based object detection is an important component in such MAV applications. As the visual device getting smaller and the development of GPU computing algorithms, robot applications using complex vision processing tasks are now available in the MAV platforms. In the drone race, the drone must pass through the race gates placed along the course. Thus, fast and accurate gate detection is an important factor for a successful race. In the drone-racing arena a number of gates are placed with the same shape and color along the course. Therefore, it is difficult to detect the gate with classical image recognition techniques due to the gate duplication problem. Classical method are mainly tuned by people according to feature information (i.e., shape or color) thus, it is quite sensitive to the circumstance. In particular, in the case of two or more gates are overlapped each other with a depth and slight position difference, there is a high possibility that the two gates recognized as one large gate. Therefore, other vision approaches are need to solve this classical image recognition problem.},
author = {Jung, Sunggoo and Lee, Hanseob and Shim, David Hyunchul},
doi = {10.2514/6.2018-2138},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung, Lee, Shim - Unknown - Real Time Embedded System Framework for Autonomous Drone Racing using Deep Learning Techniques.pdf:pdf},
title = {{Real Time Embedded System Framework for Autonomous Drone Racing using Deep Learning Techniques}}
}
@article{Lin,
abstract = {0 0.2 0.4 0.6 0.8 1 probability of ground truth class 0 1 2 3 4 5 loss = 0 = 0.5 = 1 = 2 = 5 well-classiied examples well-classiied examples CE(pt) = − log(pt) FL(pt) = −(1 − pt) $\gamma$ log(pt) Figure 1. We propose a novel loss we term the Focal Loss that adds a factor (1 − pt) $\gamma$ to the standard cross entropy criterion. Setting $\gamma$ {\textgreater} 0 reduces the relative loss for well-classified examples (pt {\textgreater} .5), putting more focus on hard, misclassified examples. As our experiments will demonstrate, the proposed focal loss enables training highly accurate dense object detectors in the presence of vast numbers of easy background examples. Abstract The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object lo-cations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the ex-treme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelm-ing the detector during training. To evaluate the effective-ness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of pre-vious one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at:},
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Focal Loss for Dense Object Detection.pdf:pdf},
title = {{Focal Loss for Dense Object Detection}},
url = {https://github.com/facebookresearch/Detectron.}
}
@article{Inoue,
abstract = {— Robotic learning in simulation environments pro-vides a faster, more scalable, and safer training methodology than learning directly with physical robots. Also, synthesizing images in a simulation environment for collecting large-scale image data is easy, whereas capturing camera images in the real world is time consuming and expensive. However, learning from only synthetic images may not achieve the desired performance in real environments due to the gap between synthetic and real images. We thus propose a method that transfers learned capability of detecting object position from a simulation environment to the real world. Our method enables us to use only a very limited dataset of real images while leveraging a large dataset of synthetic images using multiple variational autoencoders. It detects object positions 6 to 7 times more precisely than the baseline of directly learning from the dataset of the real images. Object position estimation under varying environmental conditions forms one of the underlying requirement for standard robotic manipulation tasks. We show that the proposed method performs robustly in different lighting conditions or with other distractor objects present for this requirement. Using this detected object position, we transfer pick-and-place or reaching tasks learned in a simulation environment to an actual physical robot without re-training.},
author = {Inoue, Tadanobu and Chaudhury, Subhajit and {De Magistris}, Giovanni and Dasgupta, Sakyasingha},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue et al. - Unknown - Transfer learning from synthetic to real images using variational autoencoders for robotic applications.pdf:pdf},
title = {{Transfer learning from synthetic to real images using variational autoencoders for robotic applications}},
url = {https://arxiv.org/pdf/1709.06762.pdf}
}

@inproceedings{He,
	title={Convolutional neural networks at constrained time cost},
	author={He, Kaiming and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={5353--5360},
	year={2015}
}

@article{Laskar,
abstract = {We propose a new deep learning based approach for camera relocalization. Our approach localizes a given query image by using a convolutional neural network (CNN) for first retrieving similar database images and then predicting the relative pose between the query and the database images, whose poses are known. The camera lo-cation for the query image is obtained via triangulation from two relative translation estimates using a RANSAC based approach. Each relative pose estimate provides a hy-pothesis for the camera orientation and they are fused in a second RANSAC scheme. The neural network is trained for relative pose estimation in an end-to-end manner using training image pairs. In contrast to previous work, our ap-proach does not require scene-specific training of the net-work, which improves scalability, and it can also be ap-plied to scenes which are not available during the training of the network. As another main contribution, we release a challenging indoor localisation dataset covering 5 differ-ent scenes registered to a common coordinate frame. We evaluate our approach using both our own dataset and the standard 7 Scenes benchmark. The results show that the proposed approach generalizes well to previously unseen scenes and compares favourably to other recent CNN-based methods .},
author = {Laskar, Zakaria and Melekhov, Iaroslav and Kalia, Surya and Kannala, Juho},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laskar et al. - Unknown - Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network.pdf:pdf},
title = {{Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w17/Laskar{\_}Camera{\_}Relocalization{\_}by{\_}ICCV{\_}2017{\_}paper.pdf}
}
@article{Melekhov,
abstract = {This paper presents a convolutional neural network based approach for estimating the relative pose between two cameras. The proposed network takes RGB images from both cameras as input and directly produces the relative rotation and translation as output. The system is trained in an end-to-end manner utilising transfer learning from a large scale classification dataset. The introduced approach is com-pared with widely used local feature based methods (SURF, ORB) and the results indicate a clear improvement over the baseline. In addition, a variant of the proposed architecture containing a spatial pyramid pooling (SPP) layer is evaluated and shown to further improve the performance.},
author = {Melekhov, Iaroslav and Ylioinas, Juha and Kannala, Juho and Rahtu, Esa},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Melekhov et al. - Unknown - Relative Camera Pose Estimation Using Convolutional Neural Networks.pdf:pdf},
keywords = {deep neural networks,relative camera pose estimation,spa-tial pyramid pooling},
title = {{Relative Camera Pose Estimation Using Convolutional Neural Networks}},
url = {https://users.aalto.fi/{~}kannalj1/publications/acivs2017.pdf}
}

@inproceedings{YoungwanLee,
	title={Wide-Residual-Inception networks for real-time object detection},
	author={Lee, Youngwan and Kim, Huieun and Park, Eunsoo and Cui, Xuenan and Kim, Hakil},
	booktitle={Intelligent Vehicles Symposium (IV), 2017 IEEE},
	pages={758--764},
	year={2017},
	organization={IEEE}
}
@inproceedings{ChengchengNing2017,
author = {{Chengcheng Ning} and {Huajun Zhou} and {Yan Song} and {Jinhui Tang}},
booktitle = {2017 IEEE International Conference on Multimedia {\&} Expo Workshops (ICMEW)},
doi = {10.1109/ICMEW.2017.8026312},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chengcheng Ning et al. - 2017 - Inception Single Shot MultiBox Detector for object detection.pdf:pdf},
isbn = {978-1-5386-0560-8},
month = {jul},
pages = {549--554},
publisher = {IEEE},
title = {{Inception Single Shot MultiBox Detector for object detection}},
url = {http://ieeexplore.ieee.org/document/8026312/},
year = {2017}
}
@article{Cao2016,
abstract = {—Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. Additionally, we propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several datasets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.01706v1},
author = {Cao, Yuanzhouhan and Shen, Chunhua and Shen, Heng Tao},
eprint = {arXiv:1610.01706v1},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Shen, Shen - 2016 - Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation.pdf:pdf},
journal = {APPEARING IN IEEE TRANSACTIONS ON IMAGE PROCESSING},
keywords = {Index Terms—Object detection,deep convolutional networks,depth estimation,semantic segmentation},
number = {1},
title = {{Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation}},
url = {https://arxiv.org/pdf/1610.01706.pdf},
year = {2016}
}
@article{Crivellaro1109,
abstract = {—We present an algorithm for estimating the pose of a rigid object in real-time under challenging conditions. Our method effectively handles poorly textured objects in cluttered, changing environments, even when their appearance is corrupted by large occlusions, and it relies on grayscale images to handle metallic environments on which depth cameras would fail. As a result, our method is suitable for practical Augmented Reality applications including industrial environments. At the core of our approach is a novel representation for the 3D pose of object parts: We predict the 3D pose of each part in the form of the 2D projections of a few control points. The advantages of this representation is three-fold: We can predict the 3D pose of the object even when only one part is visible; when several parts are visible, we can easily combine them to compute a better pose of the object; the 3D pose we obtain is usually very accurate, even when only few parts are visible. We show how to use this representation in a robust 3D tracking framework. In addition to extensive comparisons with the state-of-the-art, we demonstrate our method on a practical Augmented Reality application for maintenance assistance in the ATLAS particle detector at CERN.},
author = {Crivellaro, Alberto and Rad, Mahdi and Verdie, Yannick and {Moo Yi}, Kwang and Fua, Pascal and Lepetit, Vincent},
doi = {10.1109/TPAMI.2017.2708711},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crivellaro et al. - 1109 - IEEE Transactions on Pattern Analysis and Machine Intelligence Robust 3D Object Tracking from Monocular Image.pdf:pdf},
keywords = {3D Tracking !,Index Terms—3D Detection},
title = {{IEEE Transactions on Pattern Analysis and Machine Intelligence Robust 3D Object Tracking from Monocular Images using Stable Parts}},
url = {http://www.ieee.org/publications{\_}standards/publications/rights/index.html},
year = {1109}
}
@article{Ren,
abstract = {—Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them " Networks on Convolutional feature maps " (NoCs). We discover that aside from deep feature maps, a deep and convolutional per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly lead to good detection accuracy without using such a per-region classifier. We show by experiments that despite the effective ResNets and Faster R-CNN systems, the design of NoCs is an essential element for the 1st-place winning entries in ImageNet and MS COCO challenges 2015.},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Object Detection Networks on Convolutional Feature Maps.pdf:pdf},
title = {{Object Detection Networks on Convolutional Feature Maps}},
url = {https://arxiv.org/pdf/1504.06066.pdf}
}
@article{Chen,
abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and pre-dicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view fe-ature fusion. The proposal network generates 3D candi-date boxes efficiently from the bird's eye view representa-tion of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25{\%} and 30{\%} AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our appro-ach obtains 14.9{\%} higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - Multi-View 3D Object Detection Network for Autonomous Driving.pdf:pdf},
title = {{Multi-View 3D Object Detection Network for Autonomous Driving}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Chen{\_}Multi-View{\_}3D{\_}Object{\_}CVPR{\_}2017{\_}paper.pdf}
}
@article{Lina,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid rep-resentations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to con-struct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extrac-tor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model en-tries including those from the COCO 2016 challenge win-ners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
author = {Lin, Tsung-Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Feature Pyramid Networks for Object Detection.pdf:pdf},
title = {{Feature Pyramid Networks for Object Detection}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Lin{\_}Feature{\_}Pyramid{\_}Networks{\_}CVPR{\_}2017{\_}paper.pdf}
}
@article{OndUska,
abstract = {This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or sys-tem identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an un-supervised manner, only based on raw, occluded sen-sor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset de-signed to mimic the task of tracking objects in 2D laser data – as commonly encountered in robotics applica-tions – and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.},
author = {{Ond U{\v{s}}ka}, Peter and Posner, Ingmar},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ond U{\v{s}}ka, Posner - Unknown - Deep Tracking Seeing Beyond Seeing Using Recurrent Neural Networks.pdf:pdf},
title = {{Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1602.00991.pdf}
}
@article{Milan,
abstract = {We present a novel approach to online multi-target tracking based on recurrent neural networks (RNNs). Tracking mul-tiple objects in real-world scenes involves many challenges, including a) an a-priori unknown and time-varying number of targets, b) a continuous state estimation of all present targets, and c) a discrete combinatorial problem of data association. Most previous methods involve complex models that require tedious tuning of parameters. Here, we propose for the first time, an end-to-end learning approach for online multi-target tracking. Existing deep learning methods are not designed for the above challenges and cannot be trivially applied to the task. Our solution addresses all of the above points in a prin-cipled way. Experiments on both synthetic and real data show promising results obtained at ≈300 Hz on a standard CPU, and pave the way towards future research in this direction.},
author = {Milan, Anton and {Hamid Rezatofighi}, S and Dick, Anthony and Reid, Ian and Schindler, Konrad},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milan et al. - Unknown - Online Multi-Target Tracking Using Recurrent Neural Networks.pdf:pdf},
title = {{Online Multi-Target Tracking Using Recurrent Neural Networks}},
url = {http://www.milanton.de/files/aaai2017/aaai2017-anton-rnntracking.pdf}
}
@article{Fang,
abstract = {As deep neural networks revolutionize many fundamental computer vision prob-lems, there have not been many works using neural networks to track objects. In this project, we design and implement a tracking pipeline using convolutional neural networks and recurrent neural networks. Our model can handle detection and tracking jointly using appearance and motion features. We use MOT data challenge as a highly occluded single object tracking dataset. We demonstrate good qualitative and quantitative results of our model and discuss how to extend the pipeline to multi-object tracking.},
author = {Fang, Kuan},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang - Unknown - Track-RNN Joint Detection and Tracking Using Recurrent Neural Networks.pdf:pdf},
title = {{Track-RNN: Joint Detection and Tracking Using Recurrent Neural Networks}},
url = {https://web.stanford.edu/class/cs231a/prev{\_}projects{\_}2016/final{\_}report (7).pdf}
}
@article{Ye,
abstract = {Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource-limited scenarios. A widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the compu-tations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a compu-tational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to repro-duce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.},
author = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ye et al. - Unknown - RETHINKING THE SMALLER-NORM-LESS- INFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS.pdf:pdf},
title = {{RETHINKING THE SMALLER-NORM-LESS- INFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS}},
url = {https://arxiv.org/pdf/1802.00124.pdf}
}
@article{Shimobaba2018,
abstract = {—Digital holography enables us to reconstruct objects in three-dimensional space from holograms captured by an imaging device. For the reconstruction, we need to know the depth position of the recoded object in advance. In this study, we propose depth prediction using convolutional neural network (CNN)-based regression. In the previous researches, the depth of an object was estimated through reconstructed images at different depth positions from a hologram using a certain metric that indicates the most focused depth position; however, such a depth search is time-consuming. The CNN of the proposed method can directly predict the depth position with millimeter precision from holograms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.00664v1},
author = {Shimobaba, Tomoyoshi and Kakue, Takashi and Ito, Tomoyoshi},
eprint = {arXiv:1802.00664v1},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shimobaba, Kakue, Ito - 2018 - Convolutional neural network-based regression for depth prediction in digital holography.pdf:pdf},
keywords = {Index Terms—digital holography,convolutional neural net-work,depth prediction,multiple regression},
title = {{Convolutional neural network-based regression for depth prediction in digital holography}},
url = {https://arxiv.org/pdf/1802.00664.pdf},
year = {2018}
}
@article{Zhang,
abstract = {This paper reviews recent studies in emerging di-rections of understanding neural-network represen-tations and learning neural networks with inter-pretable/disentangled middle-layer representations. Although deep neural networks have exhibited su-perior performance in various tasks, the inter-pretability is always an Achilles' heel of deep neu-ral networks. At present, deep neural networks obtain a high discrimination power at the cost of low interpretability of their black-box representa-tions. We believe that the high model interpretabil-ity may help people to break several bottlenecks of deep learning, e.g. learning from very few an-notations, learning via human-computer communi-cations at the semantic level, and semantically de-bugging network representations. In this paper, we focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN repre-sentations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Fi-nally, we discuss prospective trends of explainable artificial intelligence.},
author = {Zhang, Quanshi and Zhu, Song-Chun},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhu - Unknown - Visual Interpretability for Deep Learning a Survey.pdf:pdf},
title = {{Visual Interpretability for Deep Learning: a Survey}},
url = {https://arxiv.org/pdf/1802.00614.pdf}
}
@article{,
doi = {10.1080/10095020.2017.1420509},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Geo-spatial Information Science.pdf:pdf},
issn = {1009-5020},
title = {{Geo-spatial Information Science}},
url = {http://www.tandfonline.com/action/journalInformation?journalCode=tgsi20}
}
@article{Falanga,
abstract = {— We address one of the main challenges towards autonomous quadrotor flight in complex environments, which is flight through narrow gaps. While previous works relied on off-board localization systems or on accurate prior knowledge of the gap position and orientation in the world reference frame, we rely solely on onboard sensing and computing and estimate the full state by fusing gap detection from a single onboard camera with an IMU. This problem is challenging for two reasons: (i) the quadrotor pose uncertainty with respect to the gap increases quadratically with the distance from the gap; (ii) the quadrotor has to actively control its orientation towards the gap to enable state estimation (i.e., active vision). We solve this problem by generating a trajectory that considers geometric, dynamic, and perception constraints: during the approach maneuver, the quadrotor always faces the gap to allow state estimation, while respecting the vehicle dynamics; during the traverse through the gap, the distance of the quadrotor to the edges of the gap is maximized. Furthermore, we replan the trajectory during its execution to cope with the varying uncertainty of the state estimate. We successfully evaluate and demonstrate the proposed approach in many real experiments, achieving a success rate of 80{\%} and gap orientations up to 45 • . To the best of our knowledge, this is the first work that addresses and achieves autonomous, aggressive flight through narrow gaps using only onboard sensing and computing and without prior knowledge of the pose of the gap.},
author = {Falanga, Davide and Mueggler, Elias and Faessler, Matthias and Scaramuzza, Davide},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Falanga et al. - Unknown - Aggressive Quadrotor Flight through Narrow Gaps with Onboard Sensing and Computing using Active Vision.pdf:pdf},
title = {{Aggressive Quadrotor Flight through Narrow Gaps with Onboard Sensing and Computing using Active Vision}},
url = {http://rpg.ifi.uzh.ch/aggressive{\_}flight.html}
}
@article{DeBrabandere,
abstract = {Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a rep-resentation of the image that can easily be clustered into instances with a simple post-processing step. The loss func-tion encourages the network to map each pixel to a point in feature space so that pixels belonging to the same in-stance lie close together while different instances are sepa-rated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and dis-tinct from recent efforts in instance segmentation. In con-trast to previous works, our method does not rely on ob-ject proposals or recurrent mechanisms. A key contribu-tion of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on-par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popu-lar detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmenta-tion benchmarks.},
author = {{De Brabandere}, Bert and Neven, Davy},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Brabandere, Neven - Unknown - Semantic Instance Segmentation with a Discriminative Loss Function.pdf:pdf},
title = {{Semantic Instance Segmentation with a Discriminative Loss Function}},
url = {https://arxiv.org/pdf/1708.02551.pdf}
}
@article{Chena,
abstract = {In this work, we tackle the problem of instance segmen-tation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction predic-tion. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining seman-tic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different se-mantic classes including background, while the direction prediction, estimating each pixel's direction towards its cor-responding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incor-porating recent successful methods from both segmentation and detection (e.g., atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance seg-mentation benchmark and shows comparable performance with other state-of-art models.},
author = {Chen, Liang-Chieh and Hermans, Alexander and Papandreou, George and Schroff, Florian and Wang, Peng and Adam, Hartwig},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - MaskLab Instance Segmentation by Refining Object Detection with Semantic and Direction Features.pdf:pdf},
title = {{MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features}},
url = {https://arxiv.org/pdf/1712.04837.pdf}
}
@article{Long,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build " fully convolutional " networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [5] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative im-provement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - Unknown - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf}
}
@article{Abolafia2018,
abstract = {We consider the task of program synthesis in the presence of a reward function over the output of programs, where the goal is to find programs with maximal rewards. We employ an iterative optimization scheme, where we train an RNN on a dataset of K best programs from a priority queue of the generated programs so far. Then, we synthesize new programs and add them to the priority queue by sampling from the RNN. We benchmark our algorithm, called priority queue train-ing (or PQT), against genetic algorithm and reinforcement learning baselines on a simple but expressive Turing complete programming language called BF. Our ex-perimental results show that our simple PQT algorithm significantly outperforms the baselines. By adding a program length penalty to the reward function, we are able to synthesize short, human readable programs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.03526v1},
author = {Abolafia, Daniel A and Norouzi, Mohammad and Le, Quoc V and Brain, Google},
eprint = {arXiv:1801.03526v1},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abolafia et al. - 2018 - NEURAL PROGRAM SYNTHESIS WITH PRIORITY QUEUE TRAINING.pdf:pdf},
title = {{NEURAL PROGRAM SYNTHESIS WITH PRIORITY QUEUE TRAINING}},
url = {https://arxiv.org/pdf/1801.03526.pdf},
year = {2018}
}
@article{Zhang,
abstract = {Which image is more similar to the reference image? Reference Patch 1 Patch 0 Reference Patch 1 Patch 0 Reference Patch 1 Patch 0 L2/PSNR, SSIM, FSIM Random Networks Unsupervised Networks Self-Supervised Networks Supervised Networks Humans Figure 1: Which patch (left or right) is " closer " to the middle patch in these examples? In each case, the tradi-tional metrics (L2/PSNR, SSIM, FSIM) disagree with human judgments. But deep networks, even across architectures (Squeezenet [18], AlexNet [25], VGG [48]) and supervision type (supervised [44], self-supervised [12, 37, 40, 59], and even unsupervised [24]), provide an emergent embedding which agrees surprisingly well with humans. We further cal-ibrate existing deep embeddings on a large-scale database of perceptual judgments; models and data can be found at https://www.github.com/richzhang/PerceptualSimilarity. Abstract While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the under-lying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called " percep-tual losses " ? What elements are critical for their success? To answer these questions, we introduce a new Full Refer-ence Image Quality Assessment (FR-IQA) dataset of per-ceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG fea-tures, but holds across different deep architectures and lev-els of supervision (supervised, self-supervised, or even un-supervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual represen-tations.},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver and Research, Adobe},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:pdf},
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
url = {https://arxiv.org/pdf/1801.03924.pdf}
}
@article{Li,
abstract = {Deploying deep neural networks on mobile devices is a chal-lenging task. Current model compression methods such as matrix decomposition effectively reduce the deployed model size, but still cannot satisfy real-time processing requirement. This paper first discovers that the major obstacle is the ex-cessive execution time of non-tensor layers such as pooling and normalization without tensor-like trainable parameters. This motivates us to design a novel acceleration framework: DeepRebirth through " slimming " existing consecutive and parallel non-tensor and tensor layers. The layer slimming is executed at different substructures: (a) streamline slimming by merging the consecutive non-tensor and tensor layer ver-tically; (b) branch slimming by merging non-tensor and ten-sor branches horizontally. The proposed optimization oper-ations significantly accelerate the model execution and also greatly reduce the run-time memory cost since the slimmed model architecture contains less hidden layers. To maximally avoid accuracy loss, the parameters in new generated layers are learned with layer-wise fine-tuning based on both theoret-ical analysis and empirical verification. As observed in the ex-periment, DeepRebirth achieves more than 3x speed-up and 2.5x run-time memory saving on GoogLeNet with only 0.4{\%} drop of top-5 accuracy on ImageNet. Furthermore, by com-bining with other model compression techniques, DeepRe-birth offers an average of 65ms inference time on the CPU of Samsung Galaxy S6 with 86.5{\%} top-5 accuracy, 14{\%} faster than SqueezeNet which only has a top-5 accuracy of 80.5{\%}.},
author = {Li, Dawei and Wang, Xiaolong and Kong, Deguang},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Wang, Kong - Unknown - DeepRebirth Accelerating Deep Neural Network Execution on Mobile Devices.pdf:pdf},
title = {{DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices}},
url = {https://arxiv.org/pdf/1708.04728.pdf}
}
@article{Fong,
abstract = {In an effort to understand the meaning of the intermedi-ate representations captured by deep networks, recent pa-pers have tried to associate specific semantic concepts to individual neural network filter responses, where interest-ing correlations are often found, largely by focusing on ex-tremal filter responses. In this paper, we show that this ap-proach can favor easy-to-interpret cases that are not neces-sarily representative of the average behavior of a represen-tation. A more realistic but harder-to-study hypothesis is that se-mantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec frame-work, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a con-cept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better char-acterize the meaning of a representation and its relationship to other concepts.},
author = {Fong, Ruth and Vedaldi, Andrea},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Vedaldi - Unknown - Net2Vec Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks.pdf:pdf},
title = {{Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks}},
url = {https://arxiv.org/pdf/1801.03454.pdf}
}
@article{Bentzen2018,
abstract = {We present a formalization and computational implemen-tation of the second formulation of Kant's categorical im-perative. This ethical principle requires an agent to never treat someone merely as a means but always also as an end. Here we interpret this principle in terms of how persons are causally affected by actions. We introduce Kantian causal agency models in which moral patients, actions, goals, and causal influence are represented, and we show how to formal-ize several readings of Kant's categorical imperative that cor-respond to Kant's concept of strict and wide duties towards oneself and others. Stricter versions handle cases where an action directly causally affects oneself or others, whereas the wide version maximizes the number of persons being treated as an end. We discuss limitations of our formalization by pointing to one of Kant's cases that the machinery cannot handle in a satisfying way.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.03160v1},
author = {Bentzen, Martin Mose and Lindner, Felix},
eprint = {arXiv:1801.03160v1},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bentzen, Lindner - 2018 - A Formalization of Kant's Second Formulation of the Categorical Imperative.pdf:pdf},
title = {{A Formalization of Kant's Second Formulation of the Categorical Imperative}},
url = {https://arxiv.org/pdf/1801.03160.pdf},
year = {2018}
}
@article{Tang,
abstract = {—Deep CNN-based object detection systems have achieved remarkable success on several large-scale object detection benchmarks. However, training such detectors requires a large number of labeled bounding boxes, which are more difficult to obtain than image-level annotations. Previous work addresses this issue by transforming image-level classifiers into object detectors. This is done by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations. We improve this previous work by incorporating knowledge about object similarities from visual and semantic domains during the transfer process. The intuition behind our proposed method is that visually and semantically similar categories should exhibit more common transferable properties than dissimilar categories, e.g. a better detector would result by transforming the differences between a dog classifier and a dog detector onto the cat class, than would by transforming from the violin class. Experimental results on the challenging ILSVRC2013 detection dataset demonstrate that each of our proposed object similarity based knowledge transfer methods outperforms the baseline methods. We found strong evidence that visual similarity and semantic relatedness are complementary for the task, and when combined notably improve detection, achieving state-of-the-art detection performance in a semi-supervised setting.},
author = {Tang, Yuxing and Wang, Josiah and Wang, Xiaofang and Gao, Boyang and Delland, Emmanuel and Gaizauskas, Robert and Chen, Liming},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - Unknown - Large Scale Semi-supervised Object Detection Using Visual and Semantic Knowledge Transfer.pdf:pdf},
keywords = {Index Terms—Object detection,convolutional neural networks,semantic similarity,semi-supervised learning,transfer learning,visual similarity},
title = {{Large Scale Semi-supervised Object Detection Using Visual and Semantic Knowledge Transfer}},
url = {https://arxiv.org/pdf/1801.03145.pdf}
}
@article{Jha,
abstract = {In this paper, we describe the mechanical design, system overview, integration and control techniques associated with SKALA, a unique large-sized robot for carrying a person with physical disabilities, up and down staircases. As a reg-ular wheelchair is unable to perform such a maneuver, the system functions as a non-conventional wheelchair with sev-eral intelligent features. We describe the unique mechanical design and the design choices associated with it. We show-case the embedded control architecture that allows for sev-eral different modes of teleoperation, all of which have been described in detail. We further investigate the architecture associated with the autonomous operation of the system.},
author = {Jha, Siddharth and Chaudhary, Himanshu and Kharagpur, Iit and Satardey, Swapnil and Kumar, Piyush and Roy, Ankush and Deshmukh, Aditya},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jha et al. - Unknown - Design, Analysis {\&}amp Prototyping of a Semi-Automated Staircase-Climbing Rehabilitation Robot.pdf:pdf},
keywords = {Caterpillar Drive,Embedded systems,Keywords Mechatronics,Rehabilitation Robotics,Sen-sors and actuators,Stair-Climbing Wheelchair},
title = {{Design, Analysis {\&} Prototyping of a Semi-Automated Staircase-Climbing Rehabilitation Robot}},
url = {https://arxiv.org/pdf/1801.03425.pdf}
}
@article{Li2017,
abstract = {Recently, learning equivariant representations has attracted considerable research attention. Dieleman et al. introduce four operations which can be inserted to CNN to learn deep representations equivariant to rotation. However, feature maps should be copied and rotated four times in each layer in their approach, which causes much running time and memory overhead. In order to address this problem, we propose Deep Rotation Equivariant Network(DREN) consisting of cycle layers, isotonic layers and decycle layers.Our proposed layers apply rotation transformation on filters rather than feature maps, achieving a speed up of more than 2 times with even less memory overhead. We evaluate DRENs on Rotated MNIST and CIFAR-10 datasets and demonstrate that it can improve the performance of state-of-the-art architectures. Our codes are released on GitHub.},
archivePrefix = {arXiv},
arxivId = {1705.08623},
author = {Li, Junying and Yang, Zichen and Liu, Haifeng and Cai, Deng},
eprint = {1705.08623},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Deep Rotation Equivariant Network.pdf:pdf},
month = {may},
title = {{Deep Rotation Equivariant Network}},
url = {http://arxiv.org/abs/1705.08623},
year = {2017}
}

@article{Rena,
	title={Faster R-CNN: towards real-time object detection with region proposal networks},
	author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
	number={6},
	pages={1137--1149},
	year={2017},
	publisher={IEEE}
}
@article{Mohta2017,
abstract = {One of the most challenging tasks for a flying robot is to autonomously navigate between target locations quickly and reliably while avoiding obstacles in its path, and with little to no a-priori knowledge of the operating environment. This challenge is addressed in the present paper. We describe the system design and software architecture of our proposed solution, and showcase how all the distinct components can be integrated to enable smooth robot operation. We provide critical insight on hardware and software component selection and development, and present results from extensive experimental testing in real-world warehouse environments. Experimental testing reveals that our proposed solution can deliver fast and robust aerial robot autonomous navigation in cluttered, GPS-denied environments.},
archivePrefix = {arXiv},
arxivId = {1712.02052},
author = {Mohta, Kartik and Watterson, Michael and Mulgaonkar, Yash and Liu, Sikang and Qu, Chao and Makineni, Anurag and Saulnier, Kelsey and Sun, Ke and Zhu, Alex and Delmerico, Jeffrey and Karydis, Konstantinos and Atanasov, Nikolay and Loianno, Giuseppe and Scaramuzza, Davide and Daniilidis, Kostas and Taylor, Camillo Jose and Kumar, Vijay},
eprint = {1712.02052},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohta et al. - 2017 - Fast, Autonomous Flight in GPS-Denied and Cluttered Environments.pdf:pdf},
month = {dec},
title = {{Fast, Autonomous Flight in GPS-Denied and Cluttered Environments}},
url = {http://arxiv.org/abs/1712.02052},
year = {2017}
}
@article{Elhoseiny,
abstract = {In the Object Recognition task, there exists a di-chotomy between the categorization of objects and estimating object pose, where the former ne-cessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different cate-gories of objects. With the rise of deep archi-tectures, the prime focus has been on object cat-egory recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose estimation using these approaches has received relatively less attention. In this work, we study how Convolutional Neural Net-works (CNN) architectures can be adapted to the task of simultaneous object recognition and pose estimation. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representa-tions. We extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art.},
author = {Elhoseiny, Mohamed and El-Gaaly, Tarek and Bakry, Amr and Elgammal, Ahmed},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elhoseiny et al. - Unknown - A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimatio.pdf:pdf},
title = {{A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimation}},
url = {http://proceedings.mlr.press/v48/elhoseiny16.pdf}
}
@article{Brachmann,
abstract = {In recent years, the task of estimating the 6D pose of object instances and complete scenes, i.e. camera localiza-tion, from a single input image has received considerable attention. Consumer RGB-D cameras have made this fea-sible, even for difficult, texture-less objects and scenes. In this work, we show that a single RGB image is sufficient to achieve visually convincing results. Our key concept is to model and exploit the uncertainty of the system at all stages of the processing pipeline. The uncertainty comes in the form of continuous distributions over 3D object coor-dinates and discrete distributions over object labels. We give three technical contributions. Firstly, we develop a regularized, auto-context regression framework which iter-atively reduces uncertainty in object coordinate and object label predictions. Secondly, we introduce an efficient way to marginalize object coordinate distributions over depth. This is necessary to deal with missing depth information. Thirdly, we utilize the distributions over object labels to de-tect multiple objects simultaneously with a fixed budget of RANSAC hypotheses. We tested our system for object pose estimation and camera localization on commonly used data sets. We see a major improvement over competing systems.},
author = {Brachmann, Eric and Michel, Frank and Krull, Alexander and Yang, Michael Ying and Gumhold, Stefan and Rother, Carsten},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brachmann et al. - Unknown - Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image.pdf:pdf},
title = {{Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Brachmann{\_}Uncertainty-Driven{\_}6D{\_}Pose{\_}CVPR{\_}2016{\_}paper.pdf}
}
@article{Savarese,
abstract = {We propose a novel and robust model to represent and learn generic 3D object categories. We aim to solve the problem of true 3D object categorization for handling arbi-trary rotations and scale changes. Our approach is to cap-ture a compact model of an object category by linking to-gether diagnostic parts of the objects from different viewing points. We emphasize on the fact that our " parts " are large and discriminative regions of the objects that are composed of many local invariant features. Instead of recovering a full 3D geometry, we connect these parts through their mu-tual homographic transformation. The resulting model is a compact summarization of both the appearance and geom-etry information of the object class. We propose a frame-work in which learning is done via minimal supervision compared to previous works. Our results on categorization show superior performances to state-of-the-art algorithms such as [23]. Furthermore, we have compiled a new 3D ob-ject dataset that consists of 10 different object categories. We have tested our algorithm on this dataset and have ob-tained highly promising results.},
author = {Savarese, Silvio and Fei-Fei, Li},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Savarese, Fei-Fei - Unknown - 3D generic object categorization, localization and pose estimation.pdf:pdf},
title = {{3D generic object categorization, localization and pose estimation}},
url = {http://vision.stanford.edu/documents/SavareseFei-Fei{\_}ICCV2007.pdf}
}
@article{Peng,
abstract = {Crowdsourced 3D CAD models are becoming easily ac-cessible online, and can potentially generate an infinite number of training images for almost any object category. We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real train-ing data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD-rendered images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of in-variance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previ-ous methods on the PASCAL VOC2007 dataset when learn-ing in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.},
author = {Peng, Xingchao and Sun, Baochen and Ali, Karim and Saenko, Kate},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - Unknown - Learning Deep Object Detectors from 3D Models.pdf:pdf},
title = {{Learning Deep Object Detectors from 3D Models}},
url = {http://www.karimali.org/publications/PSAS{\_}ICCV15.pdf}
}
@article{Mueller,
abstract = {We present an approach for real-time, robust and accu-rate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing meth-ods typically fail for hand-object interactions in cluttered scenes imaged from egocentric viewpoints—common for virtual or augmented reality applications. Our approach uses two subsequently applied Convolutional Neural Net-works (CNNs) to localize the hand and regress 3D joint locations. Hand localization is achieved by using a CNN to estimate the 2D position of the hand center in the input, even in the presence of clutter and occlusions. The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time. For added accuracy, robustness and temporal stability, we refine the pose estimates using a kine-matic pose tracking energy. To train the CNNs, we intro-duce a new photorealistic dataset that uses a merged reality approach to capture and synthesize large amounts of anno-tated data of natural hand interaction in cluttered scenes. Through quantitative and qualitative evaluation, we show that our method is robust to self-occlusion and occlusions by objects, particularly in moving egocentric perspectives.},
author = {Mueller, Franziska and Sridhar, Srinath and Mehta, Dushyant and Casas, Dan and Sotnychenko, Oleksandr and Theobalt, Christian},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mueller et al. - Unknown - Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor.pdf:pdf},
title = {{Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor}},
url = {https://arxiv.org/pdf/1704.02201.pdf}
}
@article{Lepetit2005,
abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Aug-mented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated inter-faces can use. Computer Vision offers solutions that are cheap, practical and non-invasive. This survey reviews the different techniques and approaches that have been developed by industry and research. First, important math-ematical tools are introduced: Camera representation, robust estima-tion and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery con-cludes with the different possible choices that should be made when},
author = {Lepetit, Vincent and Fua, Pascal},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects A Survey(2).pdf:pdf},
journal = {Computer Graphics and Vision},
number = {1},
pages = {1--89},
title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.8962{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {2005}
}
@article{Rad,
abstract = {We introduce a novel method for 3D object detection and pose estimation from color images only. We first use seg-mentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a " holistic " approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes, as proposed in [3] for the pose of objects' parts. This, however, is not sufficient for han-dling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this prob-lem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional ad-ditional step that refines the predicted poses as in [17] for hand pose estimation. We improve the state-of-the-art on the LINEMOD dataset from 73.7{\%} [2] to 89.3{\%} of correctly registered RGB frames. We are also the first to report re-sults on the Occlusion dataset [1] using color images only. We obtain 54{\%} of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, com-pared to the 67{\%} of the state-of-the-art [10] on the same sequences which uses both color and depth. The full ap-proach is also scalable, as a single network can be trained for multiple objects simultaneously.},
author = {Rad, Mahdi and Lepetit, Vincent},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rad, Lepetit - Unknown - BB8 A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects.pdf:pdf},
title = {{BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth}},
url = {https://arxiv.org/pdf/1703.10896.pdf}
}
@article{Hodan,
abstract = {We introduce T-LESS, a new public dataset for estimat-ing the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit sym-metries and mutual similarities in shape and/or size. Com-pared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are pro-vided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training im-ages depict individual objects against a black background. Test images originate from twenty test scenes having vary-ing complexity, which increases from simple scenes with several isolated objects to very challenging ones with mul-tiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a sys-tematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with sig-nificant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.},
author = {Hodaň, Tom{\'{a}}{\v{s}} and Haluza, Pavel and Obdr{\v{z}}{\'{a}}lek, St{\v{e}}p{\'{a}}n and Matas, Jiř{\'{i}} and Lourakis, Manolis and Zabulis, Xenophon},
doi = {10.1109/WACV.2017.103},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodaň et al. - Unknown - T-LESS An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects.pdf:pdf},
title = {{T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects}},
url = {https://pdfs.semanticscholar.org/e2a8/4869c68e73f76c2d326cb2b97c9795562f0c.pdf}
}
@article{Doumanoglou,
abstract = {In this paper we tackle the problem of estimating the 3D pose of object instances, using convolutional neural networks. State of the art methods usually solve the challenging problem of regression in angle space indirectly, focusing on learning discriminative features that are later fed into a separate architecture for 3D pose estimation. In contrast, we propose an end-to-end learning framework for directly regressing object poses by exploiting Siamese Networks. For a given image pair, we enforce a similarity measure between the representation of the sample images in the feature and pose space respectively, that is shown to boost regression performance. Furthermore, we argue that our pose-guided feature learning using our Siamese Regression Network generates more discriminative features that outperform the state of the art. Last, our feature learning formulation provides the ability of learning features that can perform under severe occlusions, demonstrating high performance on our novel hand-object dataset.},
author = {Doumanoglou, Andreas and Balntas, Vassileios and Kouskouridas, Rigas and Kim, Tae-Kyun},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doumanoglou et al. - Unknown - Siamese Regression Networks with Efficient mid-level Feature Extraction for 3D Object Pose Estimation.pdf:pdf},
title = {{Siamese Regression Networks with Efficient mid-level Feature Extraction for 3D Object Pose Estimation}},
url = {https://arxiv.org/pdf/1607.02257.pdf}
}
@article{Turan2017,
abstract = {We present a robust deep learning based 6 degrees-of-freedom (DoF) localization system for endoscopic capsule robots. Our system mainly focuses on localization of endoscopic capsule robots inside the GI tract using only visual information captured by a mono camera integrated to the robot. The proposed system is a 23-layer deep convolutional neural network (CNN) that is capable to esti-mate the pose of the robot in real time using a standard CPU. The dataset for the evaluation of the system was recorded inside a surgical human stomach model with realistic surface texture, softness, and surface liquid properties so that the pre-trained CNN architecture can be transferred confidently into a real endoscopic scenario. An average error of 7.1{\%} and 3.4{\%} for translation and rotation has been obtained, respectively. The results accomplished from the ex-periments demonstrate that a CNN pre-trained with raw 2D endoscopic images performs accurately inside the GI tract and is robust to various challenges posed by reflection distortions, lens imperfections, vignetting, noise, motion blur, low resolution, and lack of unique landmarks to track.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.05435v1},
author = {Turan, Mehmet and Almalioglu, Yasin and Konukoglu, Ender and Sitti, Metin},
eprint = {arXiv:1705.05435v1},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Turan et al. - 2017 - A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots.pdf:pdf},
keywords = {CNN,Capsule endoscope robot,deep learning,localization},
title = {{A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots}},
url = {https://arxiv.org/pdf/1705.05435.pdf},
year = {2017}
}
@article{Lepetit2005,
abstract = {Monocular Model-Based 3D Tracking of Rigid Objects: A Survey},
author = {Lepetit, Vincent and Fua, Pascal},
doi = {10.1561/0600000001},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects A Survey.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
keywords = {3D reconstruction and image-based modeling,Computer Graphics},
number = {1},
pages = {1--89},
publisher = {Now Publishers, Inc.},
title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
url = {http://www.nowpublishers.com/article/Details/CGV-001},
volume = {1},
year = {2005}
}
@article{Chen,
abstract = {—In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed " DeepLab " system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.pdf:pdf},
keywords = {Atrous Convolution,Conditional Random Fields,Index Terms—Convolutional Neural Networks,Semantic Segmentation},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1606.00915.pdf}
}
@article{Badrinarayanan,
abstract = {—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Badrinarayanan, Kendall, Cipolla - Unknown - SegNet A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.pdf:pdf},
keywords = {Decoder,Encoder,Index Terms—Deep Convolutional Neural Networks,Indoor Scenes,Pooling,Road Scenes,Semantic Pixel-Wise Segmentation,Upsampling},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {http://mi.eng.cam.ac.uk/projects/segnet/.}
}
@article{Garon,
abstract = {Fig. 1. From an input RGBD sequence (top), our method leverages a deep neural network to automatically track the 6-DOF pose of an object even under significant clutter and occlusion (bottom). We demonstrate, through extensive experiments on a novel dataset of real objects with known ground truth pose, that our approach outperforms the state of the art both in terms of accuracy and robustness to occlusions. Abstract—We present a temporal 6-DOF tracking method which leverages deep learning to achieve state-of-the-art performance on challenging datasets of real world capture. Our method is both more accurate and more robust to occlusions than the existing best performing approaches while maintaining real-time performance. To assess its efficacy, we evaluate our approach on several challenging RGBD sequences of real objects in a variety of conditions. Notably, we systematically evaluate robustness to occlusions through a series of sequences where the object to be tracked is increasingly occluded. Finally, our approach is purely data-driven and does not require any hand-designed features: robust tracking is automatically learned from data.},
author = {Garon, Mathieu and Lalonde, Jean-Fran{\c{c}}ois},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garon, Lalonde - Unknown - Deep 6-DOF Tracking.pdf:pdf},
keywords = {Augmented Reality,Deep Learning,Index Terms—Tracking},
title = {{Deep 6-DOF Tracking}},
url = {https://arxiv.org/pdf/1703.09771.pdf}
}
@article{Kehl,
abstract = {We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.},
author = {Kehl, Wadim and Milletari, Fausto and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kehl et al. - Unknown - Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation.pdf:pdf},
title = {{Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation}},
url = {http://campar.in.tum.de/pub/kehl2016eccv/kehl2016eccv.pdf}
}
@article{Rastegari,
abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32× mem-ory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily bi-nary operations. This results in 58× faster convolutional operations and 32× memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9{\%} less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16{\%} in top-1 accuracy.},
author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rastegari et al. - Unknown - XNOR-Net ImageNet Classification Using Binary Convolutional Neural Networks.pdf:pdf},
keywords = {Binary Convolution,Binary Deep Learning,Binary Neural Networks,Convolutional Neural Network,Deep Learning},
title = {{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}},
url = {https://pjreddie.com/media/files/papers/xnor.pdf}
}
@article{Redmon,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Szegedy, Christian and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew and Simon, Marcel and Rodner, Erik and Denzler, Joachim and Redmon, Joseph and Farhadi, Ali and Ioffe, Sergey and Szegedy, Christian and Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-yang and Berg, Alexander C and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew and Shlens, Jonathon and Wojna, Zbigniew and Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt and He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian and Chen, Tianqi and Guestrin, Carlos},
doi = {10.1142/9789812771728_0012},
eprint = {1612.08242},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - YOLO9000 Better, Faster, Stronger.pdf:pdf},
isbn = {1879-2057 (Electronic)$\backslash$n0001-4575 (Linking)},
issn = {0146-4833},
journal = {Data Mining with Decision Trees},
keywords = {convolutional neural network,deep learning,denoising auto-encoder,image denoising,large-scale machine learning,real-time object detection},
number = {3},
pages = {352350},
pmid = {23021419},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {https://arxiv.org/abs/1612.08242},
volume = {7},
year = {2016}
}
@article{Felzenszwalb,
abstract = {This paper describes a discriminatively trained, multi-scale, deformable part model for object detection. Our sys-tem achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person de-tection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hid-den CRF, leads to a non-convex training problem. How-ever, a latent SVM is semi-convex and the training prob-lem becomes convex once latent information is specified for the positive examples. We believe that our training meth-ods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.},
author = {Felzenszwalb, Pedro and Mcallester, David and Ramanan, Deva},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Felzenszwalb, Mcallester, Ramanan - Unknown - A Discriminatively Trained, Multiscale, Deformable Part Model.pdf:pdf},
title = {{A Discriminatively Trained, Multiscale, Deformable Part Model}},
url = {http://people.cs.uchicago.edu/{~}pff/papers/latent.pdf}
}
@article{Uijlings,
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmen-tation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possi-ble object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 {\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced num-ber of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:},
author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
doi = {10.1007/s11263-013-0620-5},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - Unknown - Selective Search for Object Recognition.pdf:pdf},
journal = {Int J Comput Vis},
title = {{Selective Search for Object Recognition}},
url = {http://disi.}
}
@article{,
abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. We describe a version of capsules in which each capsule has a logis-tic unit to represent the presence of an entity and a 4x4 pose matrix which could learn to represent the relationship between that entity and the viewer. A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by viewpoint-invariant transformation matri-ces that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated using the EM algorithm such that the output of each capsule is routed to a cap-sule in the layer above that receives a cluster of similar votes. The whole system is trained discriminatively by unrolling 3 iterations of EM between each pair of adjacent layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45{\%} compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attack than our baseline convolutional neural network.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
eprint = {1710.09829},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Matrix Capsules With Em Routing.pdf:pdf},
title = {{Matrix Capsules With Em Routing}},
url = {https://openreview.net/pdf?id=HJWLfGWRb}
}
@article{Sermanet2014,
abstract = {We present an integrated framework for using Convolutional Networks for classi-fication, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound-aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul-taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6229v4},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and Lecun, Yann},
eprint = {arXiv:1312.6229v4},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet et al. - 2014 - OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
keywords = {()},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {https://arxiv.org/pdf/1312.6229.pdf},
year = {2014}
}
@article{Liu,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines pre-dictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into sys-tems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 in-put, SSD achieves 74.3{\%} mAP 1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a compa-rable state-of-the-art Faster R-CNN model. Compared to other single stage meth-ods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - Unknown - SSD Single Shot MultiBox Detector.pdf:pdf},
keywords = {Convolutional Neural Network,Real-time Object Detection},
title = {{SSD: Single Shot MultiBox Detector}},
url = {https://www.cs.unc.edu/{~}wliu/papers/ssd.pdf}
}
@article{Wu,
abstract = {Object detection is a crucial task for autonomous driv-ing. In addition to requiring high accuracy to ensure safety, object detection for autonomous driving also requires real-time inference speed to guarantee prompt vehicle control, as well as small model size and energy efficiency to enable embedded system deployment. In this work, we propose SqueezeDet, a fully convolu-tional neural network for object detection that aims to si-multaneously satisfy all of the above constraints. In our network we use convolutional layers not only to extract fea-ture maps, but also as the output layer to compute bound-ing boxes and class probabilities. The detection pipeline of our model only contains a single forward pass of a neu-ral network, thus it is extremely fast. Our model is fully-convolutional, which leads to small model size and bet-ter energy efficiency. Finally, our experiments show that our model is very accurate, achieving state-of-the-art ac-curacy on the KITTI [9] benchmark. The source code of SqueezeDet is open-source released 1 .},
author = {Wu, Bichen and Iandola, Forrest and Jin, Peter H and Keutzer, Kurt},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - Unknown - SqueezeDet Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autono.pdf:pdf},
title = {{SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving}},
url = {https://arxiv.org/pdf/1612.01051.pdf}
}
@inproceedings{Kendall,
abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
archivePrefix = {arXiv},
arxivId = {1505.07427},
author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.336},
eprint = {1505.07427},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall, Grimes, Cipolla - 2015 - PoseNet A convolutional network for real-time 6-dof camera relocalization.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
keywords = {Deep,Monocular,Relocalization},
mendeley-tags = {Deep,Monocular,Relocalization},
month = {may},
pages = {2938--2946},
title = {{PoseNet: A convolutional network for real-time 6-dof camera relocalization}},
url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Kendall{\_}PoseNet{\_}A{\_}Convolutional{\_}ICCV{\_}2015{\_}paper.pdf http://arxiv.org/abs/1505.07427},
volume = {2015 Inter},
year = {2015}
}
@article{Zitnick,
abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We pro-pose a novel method for generating object bounding box proposals us-ing edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring propos-als. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
author = {Zitnick, C Lawrence and Doll{\'{a}}r, Piotr},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zitnick, Doll{\'{a}}r - Unknown - Edge Boxes Locating Object Proposals from Edges.pdf:pdf},
keywords = {edge detection,object detection,object proposals},
title = {{Edge Boxes: Locating Object Proposals from Edges}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.453.5208{\&}rep=rep1{\&}type=pdf}
}
@article{Uijlings,
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmen-tation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possi-ble object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 {\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced num-ber of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:},
author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
doi = {10.1007/s11263-013-0620-5},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - Unknown - Selective Search for Object Recognition.pdf:pdf},
journal = {Int J Comput Vis},
title = {{Selective Search for Object Recognition}},
url = {http://disi.}
}
@article{Dai,
abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
archivePrefix = {arXiv},
arxivId = {1703.06211},
author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
doi = {10.1051/0004-6361/201527329},
eprint = {1703.06211},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2017 - Deformable Convolutional Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Deformable Convolutional Networks}},
url = {http://arxiv.org/abs/1703.06211},
year = {2017}
}
@article{Ouyang,
abstract = {In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric con-straint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averag-ing. The proposed approach improves the mean averaged precision obtained by RCNN [14], which was the state-of-the-art, from 31{\%} to 50.3{\%} on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1{\%}. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline.},
author = {Ouyang, Wanli and Wang, Xiaogang and Zeng, Xingyu and Qiu, Shi and Luo, Ping and Tian, Yonglong and Li, Hongsheng and Yang, Shuo and Wang, Zhe and Loy, Chen-Change and Tang, Xiaoou},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ouyang et al. - Unknown - DeepID-Net Deformable Deep Convolutional Neural Networks for Object Detection.pdf:pdf},
title = {{DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Ouyang{\_}DeepID-Net{\_}Deformable{\_}Deep{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Girshick,
abstract = {Deformable part models (DPMs) and convolutional neu-ral networks (CNNs) are two widely used tools for vi-sual recognition. They are typically viewed as distinct ap-proaches: DPMs are graphical models (Markov random fields), while CNNs are " black-box " non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a synthesis of the two ideas. Our con-struction involves unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. From this perspective, it is natural to replace the standard im-age features used in DPMs with a learned feature extractor. We call the resulting model a DeepPyramid DPM and ex-perimentally validate it on PASCAL VOC object detection. We find that DeepPyramid DPMs significantly outperform DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while run-ning significantly faster.},
author = {Girshick, Ross and Iandola, Forrest and Darrell, Trevor and Malik, Jitendra},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - Unknown - Deformable Part Models are Convolutional Neural Networks.pdf:pdf},
title = {{Deformable Part Models are Convolutional Neural Networks}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Girshick{\_}Deformable{\_}Part{\_}Models{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Liang,
abstract = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many prop-erties with the visual system of the brain. A prominent dif-ference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abun-dant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the in-put is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the ac-tivities of its neighboring units. This property enhances the ability of the model to integrate the context informa-tion, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning pro-cess. The model is tested on four benchmark object recog-nition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increas-ing the number of parameters leads to even better perfor-mance. These results demonstrate the advantage of the re-current structure over purely feed-forward structure for ob-ject recognition.},
author = {Liang, Ming and Hu, Xiaolin},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Hu - Unknown - Recurrent Convolutional Neural Network for Object Recognition.pdf:pdf},
title = {{Recurrent Convolutional Neural Network for Object Recognition}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Liang{\_}Recurrent{\_}Convolutional{\_}Neural{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Forsyth,
author = {Forsyth, David},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Forsyth - Unknown - Object Detection with Discriminatively Trained Part-Based Models.pdf:pdf},
title = {{Object Detection with Discriminatively Trained Part-Based Models}},
url = {https://pdfs.semanticscholar.org/ad5a/b85d8f8302e04ec40e139d364574083aa951.pdf}
}
@article{Erhan,
abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
archivePrefix = {arXiv},
arxivId = {1312.2249},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
eprint = {1312.2249},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan et al. - 2013 - Scalable Object Detection using Deep Neural Networks.pdf:pdf},
month = {dec},
title = {{Scalable Object Detection using Deep Neural Networks}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2014/papers/Erhan{\_}Scalable{\_}Object{\_}Detection{\_}2014{\_}CVPR{\_}paper.pdf http://arxiv.org/abs/1312.2249},
year = {2013}
}
@article{Caicedo,
abstract = {We present an active detection model for localizing ob-jects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identi-fying the correct location of a target object. This agent learns to deform a bounding box using simple transforma-tion actions, with the goal of determining the most spe-cific location of target objects following top-down reason-ing. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object af-ter analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
author = {Caicedo, Juan C and Lazebnik, Svetlana},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caicedo, Lazebnik - Unknown - Active Object Localization with Deep Reinforcement Learning.pdf:pdf},
title = {{Active Object Localization with Deep Reinforcement Learning}},
url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Caicedo{\_}Active{\_}Object{\_}Localization{\_}ICCV{\_}2015{\_}paper.pdf}
}
@article{Redmon,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
annote = {- mostly localization error
- feature extraction, classification, localization is one pipeline
- single class detecters can be highly optimized
- uses google net convolutional layer
- image is split in SxS grid
- each grid proposes: B Bounding boxes, and C classes
- a bounding box gets "responsible" for a certain object if the center of it falls into the box, this is calculated by the highest intersection over union in that grid cell
- dropout, extensive data augmentation},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
annote = {- first "deep" network
- limited by cpu/gpu constraints
- applied image augmentation to increase sample size
- variable learning rate},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Viola2004,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distin-guished by three key contributions. The first is the introduction of a new image representation called the Integral Image which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small num-ber of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a cascade which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detec-tion the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P ; and Jones},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2004 - Rapid Object Detection Using a Boosted Cascade of Simple Features.pdf:pdf},
title = {{Rapid Object Detection Using a Boosted Cascade of Simple Features}},
url = {http://www.merl.com},
year = {2004}
}
@article{Lienhart,
abstract = {Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10{\%} lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5{\%}. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3{\%} on the CMU face set [7].},
author = {Lienhart, Rainer and Maydt, Jochen},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lienhart, Maydt - Unknown - n Extended Set of Haar-like Features for Rapid Object Detection.pdf:pdf},
title = {{n Extended Set of Haar-like Features for Rapid Object Detection}},
url = {http://www.videoanalysis.org/Prof.{\_}Dr.{\_}Rainer{\_}Lienhart/Publications{\_}files/ICIP2002.pdf}
}
@article{Ren,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Pro-posal Network (RPN) that shares full-image convolutional features with the de-tection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and ob-jectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster{\_}rcnn.},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}
@article{LeCun,
abstract = {M achine-learning technology powers many aspects of modern society: from web searches to content filtering on social net-works to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users' interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, con-structing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a fea-ture extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representa-tion, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence commu-nity for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applica-ble to many domains of science, business and government. In addition to beating records in image recognition},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - Unknown - Deep learning.pdf:pdf},
title = {{Deep learning}},
url = {http://pages.cs.wisc.edu/{~}dyer/cs540/handouts/deep-learning-nature2015.pdf}
}
@article{Naseer,
abstract = {— Precise localization of robots is imperative for their safe and autonomous navigation in both indoor and outdoor environments. In outdoor scenarios, the environment typically undergoes significant perceptual changes and requires robust methods for accurate localization. Monocular camera-based approaches provide an inexpensive solution to such challenging problems compared to 3D LiDAR-based methods. Recently, approaches have leveraged deep convolutional neu-ral networks (CNNs) to perform place recognition and they turn out to outperform traditional handcrafted features under challenging perceptual conditions. In this paper, we propose an approach for directly regressing a 6-DoF camera pose using CNNs and a single monocular RGB image. We leverage the idea of transfer learning for training our network as this technique has shown to perform better when the number of training samples are not very high. Furthermore, we propose novel data augmentation in 3D space for additional pose coverage which leads to more accurate localization. In contrast to the traditional visual metric localization approaches, our resulting map size is constant with respect to the database. During localization, our approach has a constant time complexity of O(1) and is independent of the database size and runs in real-time at∼80 Hz using a single GPU. We show the localization accuracy of our approach on publicly available datasets and that it outperforms CNN-based state-of-the-art methods.},
author = {Naseer, Tayyab and Burgard, Wolfram},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naseer, Burgard - Unknown - Deep Regression for Monocular Camera-based 6-DoF Global Localization in Outdoor Environments.pdf:pdf},
title = {{Deep Regression for Monocular Camera-based 6-DoF Global Localization in Outdoor Environments}},
url = {http://ais.informatik.uni-freiburg.de/publications/papers/naseer17iros.pdf}
}
@article{Kendall,
abstract = {PoseNet: Convolutional neural network monocular camera relocalization. Relocalization results for an input image (top), the predicted camera pose of a visual reconstruction (middle), shown again overlaid in red on the original image (bottom). Our system relocalizes to within approximately 2m and 6 • for large outdoor scenes spanning 50, 000m 2 . For an online demonstration, please see our project webpage: mi.eng.cam.ac.uk/projects/relocalisation/ Abstract We present a robust and real-time monocular six de-gree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF cam-era pose from a single RGB image in an end-to-end man-ner with no need of additional engineering or graph op-timisation. The algorithm can operate indoors and out-doors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 • accuracy for large scale outdoor scenes and 0.5m and 10 • accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demon-strating that convnets can be used to solve complicated out of image plane regression problems. This was made possi-ble by leveraging transfer learning from large scale classi-fication data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT reg-istration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.},
author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall, Grimes, Cipolla - Unknown - PoseNet A Convolutional Network for Real-Time 6-DOF Camera Relocalization.pdf:pdf},
title = {{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization}},
url = {https://arxiv.org/pdf/1505.07427.pdf}
}
@misc{Lepetit,
author = {Lepetit, Vincent},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lepetit - Unknown - Using Deep Learning for ! Localization from Images.pdf:pdf},
title = {{Using Deep Learning for  ! Localization from Images}},
url = {https://www.labri.fr/perso/vlepetit/teaching/visum17/03{\_}localization.pdf}
}
@article{Rubio,
abstract = {— We propose a robust and efficient method to estimate the pose of a camera with respect to complex 3D textured models of the environment that can potentially contain more than 100, 000 points. To tackle this problem we follow a top down approach where we combine high-level deep network classifiers with low level geometric approaches to come up with a solution that is fast, robust and accurate. Given an input image, we initially use a pre-trained deep network to compute a rough estimation of the camera pose. This initial estimate constrains the number of 3D model points that can be seen from the camera viewpoint. We then establish 3D-to-2D corres-pondences between these potentially visible points of the model and the 2D detected image features. Accurate pose estimation is finally obtained from the 2D-to-3D correspondences using a novel PnP algorithm that rejects outliers without the need to use a RANSAC strategy, and which is between 10 and 100 times faster than other methods that use it. Two real experiments dealing with very large and complex 3D models demonstrate the effectiveness of the approach.},
author = {Rubio, A and Villamizar, M and Ferraz, L and Penate-Sanchez, A and Ramisa, A and Simo-Serra, E and Sanfeliu, A and Moreno-Noguer, F},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubio et al. - Unknown - Efficient Monocular Pose Estimation for Complex 3D Models.pdf:pdf},
title = {{Efficient Monocular Pose Estimation for Complex 3D Models}},
url = {http://www.iri.upc.edu/files/scidoc/1638-Efficient-monocular-pose-estimation-for-complex-3D-models.pdf}
}
@article{Kniaz,
abstract = {Accurate estimation of camera external orientation with respect to a known object is one of the central problems in photogrammetry and computer vision. In recent years this problem is gaining an increasing attention in the field of UAV autonomous flight. Such application requires a real-time performance and robustness of the external orientation estimation algorithm. The accuracy of the solution is strongly dependent on the number of reference points visible on the given image. The problem only has an analytical solution if 3 or more reference points are visible. However, in limited visibility conditions it is often needed to perform external orientation with only 2 visible reference points. In such case the solution could be found if the gravity vector direction in the camera coordinate system is known. A number of algorithms for external orientation estimation for the case of 2 known reference points and a gravity vector were developed to date. Most of these algorithms provide analytical solution in the form of polynomial equation that is subject to large errors in the case of complex reference points configurations. This paper is focused on the development of a new computationally effective and robust algorithm for external orientation based on positions of 2 known reference points and a gravity vector. The algorithm implementation for guidance of a Parrot AR.Drone 2.0 micro-UAV is discussed. The experimental evaluation of the algorithm proved its computational efficiency and robustness against errors in reference points positions and complex configurations.},
author = {Kniaz, V V},
doi = {10.5194/isprsarchives-XLI-B5-63-2016},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kniaz - Unknown - ROBUST VISION-BASED POSE ESTIMATION ALGORITHM FOR AN UAV WITH KNOWN GRAVITY VECTOR.pdf:pdf},
keywords = {UAV,external orientation estimation,machine vision,motion capture system},
title = {{ROBUST VISION-BASED POSE ESTIMATION ALGORITHM FOR AN UAV WITH KNOWN GRAVITY VECTOR}},
url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B5/63/2016/isprs-archives-XLI-B5-63-2016.pdf}
}
@article{Erol2007,
abstract = {Direct use of the hand as an input device is an attractive method for providing natural human-computer interaction (HCI). Currently, the only technology that satisfies the advanced requirements of hand-based input for HCI is glove-based sensing. This technology, however, has several drawbacks including that it hinders the ease and naturalness with which the user can interact with the computer-controlled environment, and it requires long calibration and setup procedures. Computer vision (CV) has the potential to provide more natural, non-contact solutions. As a result, there have been considerable research efforts to use the hand as an input device for HCI. In particular, two types of research directions have emerged. One is based on gesture classification and aims to extract high-level abstract information corresponding to motion patterns or postures of the hand. The second is based on pose estimation systems and aims to capture the real 3D motion of the hand. This paper presents a literature review on the latter research direction, which is a very challenging problem in the context of HCI. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1412.0065},
author = {Erol, Ali and Bebis, George and Nicolescu, Mircea and Boyle, Richard D and Twombly, Xander},
doi = {10.1016/j.cviu.2006.10.012},
eprint = {1412.0065},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf:pdf},
isbn = {1077-3142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Gesture recognition,Gesture-based HCI,Hand pose estimation},
number = {1-2},
pages = {52--73},
pmid = {17533767},
title = {{Vision-based hand pose estimation: A review}},
url = {https://ac.els-cdn.com/S1077314206002281/1-s2.0-S1077314206002281-main.pdf?{\_}tid=60899afe-c307-11e7-b84d-00000aacb35e{\&}acdnat=1509982391{\_}fb739a4053e355313c7abb81c58a13a1},
volume = {108},
year = {2007}
}
@article{,
doi = {10.1016/J.CVIU.2006.10.012},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
month = {oct},
number = {1-2},
pages = {52--73},
publisher = {Academic Press},
title = {{Vision-based hand pose estimation: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314206002281},
volume = {108},
year = {2007}
}
@article{Lepetit2005,
abstract = {Monocular Model-Based 3D Tracking of Rigid Objects: A Survey},
author = {Lepetit, Vincent and Fua, Pascal},
doi = {10.1561/0600000001},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects A Survey.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
keywords = {3D reconstruction and image-based modeling,Computer Graphics},
number = {1},
pages = {1--89},
publisher = {Now Publishers, Inc.},
title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
url = {http://www.nowpublishers.com/article/Details/CGV-001},
volume = {1},
year = {2005}
}
@article{Murphy-Chutorian,
abstract = {— The capacity to estimate the head pose of another person is a common human ability that presents a unique chal-lenge for computer vision systems. Compared to face detection and recognition, which have been the primary foci of face-related vision research, identity-invariant head pose estimation has fewer rigorously evaluated systems or generic solutions. In this paper, we discuss the inherent difficulties in head pose estimation and present an organized survey describing the evolution of the field. Our discussion focuses on the advantages and disadvantages of each approach and spans 90 of the most innovative and characteristic papers that have been published on this topic. We compare these systems by focusing on their ability to estimate coarse and fine head pose, highlighting approaches that are well suited for unconstrained environments.},
author = {Murphy-Chutorian, Erik and Trivedi, Mohan Manubhai},
doi = {10.1109/TPAMI.2008.106},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy-Chutorian, Trivedi - Unknown - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Head Pose Estimation in Computer Vi.pdf:pdf},
keywords = {Face Analysis,Facial Land Marks,Gesture Analysis,Human Computer In-terfaces,Index Terms— Head Pose Estimation},
title = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Head Pose Estimation in Computer Vision: A Survey}},
url = {https://pdfs.semanticscholar.org/cf87/e167446bd22e9422445dfed4f74a3f0579f9.pdf}
}
@article{Madsen2001,
author = {Madsen, O and Ayromlou, M and Beltran, C},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Madsen, Ayromlou, Beltran - 2001 - Model and Vision Based Pose Estimation for Mobile Robots.pdf:pdf},
journal = {IASTED International Conference on Robotics and Applications},
keywords = {mobile robots,model based vision,pose estimation},
pages = {1--8},
title = {{Model and Vision Based Pose Estimation for Mobile Robots}},
url = {http://83.212.134.96/robotics/wp-content/uploads/2011/12/Model-and-Vision-Based-Pose-Estimation-for-Mobile-Robots.pdf},
year = {2001}
}
@article{Rudol2006,
author = {Rudol, Piotr and Wzorek, Mariusz and Conte, Gianpaolo and Doherty, Patrick},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudol et al. - 2006 - Vision-based Pose Estimation for Autonomous Indoor Navigation of Micro Unmanned Aircraft Systems.pdf:pdf},
isbn = {9781424450404},
journal = {Image (Rochester, N.Y.)},
pages = {2030--2030},
title = {{Vision-based Pose Estimation for Autonomous Indoor Navigation of Micro Unmanned Aircraft Systems}},
volume = {12},
year = {2006}
}
@article{Bonin-font2008,
abstract = {Mobile robot vision-based navigation has been the source of countless research contributions, from the domains of both vision and control. Vision is becoming more and more common in applications such as localization, automatic map construction, autonomous navigation, path following, inspection, monitoring or risky situation detection. This survey presents those pieces of work, from the nineties until nowadays, which constitute a wide progress in visual navigation techniques for land, aerial and autonomous underwater vehicles. The paper deals with two major approaches: map-based navigation and mapless navigation. Map-based navigation has been in turn subdivided in metric map-based navigation and topological map-based navigation. Our outline to mapless navigation includes reactive techniques based on qualitative characteristics extraction, appearance-based localization, optical flow, features tracking, plane ground detection/tracking, etc... The recent concept of visual sonar has also been revised.},
author = {Bonin-font, Francisco and Ortiz, Alberto and Oliver, Gabriel and Alberto, Francisco Bonin-font and Gabriel, Ortiz},
doi = {10.1007/s10846-008-9235-4},
file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonin-font et al. - 2008 - Visual Navigation for Mobile Robots A Survey.pdf:pdf},
isbn = {0921-0296},
issn = {0921-0296},
journal = {Journal of Intelligent and Robotic Systems},
keywords = {Mobile robots,Visual navigation,and feder funding,lista{\_}filtrada,mobile robots,supported by dpi 2005-09001-c03-02,this work is partially,toread,visual navigation},
pages = {263--296},
title = {{Visual Navigation for Mobile Robots: A Survey}},
url = {http://dx.doi.org/10.1007/s10846-008-9235-4{\%}5Cnhttp://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42549109228{\&}partnerID=40{\&}rel=R8.0.0},
volume = {53},
year = {2008}
}
