	\chapter{Modelling the Detection of \ac{EWFO}}
	\label{sec:object_detection}
	
	Object Detection can be described by two individual goals: the description of what kind of object is seen (Classification), as well as where it is seen (Localization). Hence, an Object Detection pipeline transforms the raw image to a set of one or more areas and corresponding class labels. Images are high dimensional signals that can contain redundant and task irrelevant information. As the performance of most machine learning models decreases when the feature space becomes too large (curse of dimensionality), Computer Vision pipelines usually apply a feature extraction stage, before the actual prediction is done. An overview is displayed in \Cref{fig:obj_pipeline}.
	
	\begin{figure}[hbtp]
		
		\centering
		\includegraphics[width=\linewidth]{fig/ObjectDetection}
		\caption{Object Detection Pipeline.where $B_n$ describes an area, $C_1$ a class label, $I$ the image and $f$ the object detection function.}
		\label{fig:obj_pipeline}
	\end{figure}
	\begin{enumerate}
	\item The feature extraction stage extracts task relevant information from the image and infers an internal, more abstract representation that is usually of lower dimension.
	
	\item The classification/localization stage produces the final output based on this representation. 
	
	\end{enumerate}

	An efficient feature extraction pipeline is thereby crucial for the success of an Object Detection pipeline. If the inferred representation is clearly separable, a simple classification stage can distinguish between classes. On the other hand even a flexible classifier struggles with a highly overlapping feature space. Hence, Feature Engineering, the design of feature extractors is a highly investigated field in Computer Vision. Methods range from supervised and unsupervised Machine Learning techniques to conducting domain experts and trying to include their expert knowledge into the pipeline. In practical Computer Vision problems this often results in the cumbersome design of feature extractors for the application. Moreover, it requires domain knowledge and it is questionable whether the features designed for one task are easily transferable to other tasks. Finally, it results in a pipeline where each step needs to be optimized individually.
	
	In recent years Deep Learning based models achieved big advances in the task of Object Detection. In contrast to their traditional counter parts, these models combine Feature Extraction and Classification/Localization stage in one model. The whole pipeline is then optimized given the task and the raw image. This omits the often cumbersome work of designing feature extractors and object models. Furthermore, it has been shown that Deep Learning based features generalize well between different Computer Vision tasks \cite{Razavian}. Finally, the modular architecture of Deep Learning models allows to trade-off computational costs and model performance.
	
	However, their superior performance comes with several drawbacks. First of all, large amounts of annotated examples are required in order to train the vast amount of parameters present in Deep Learning models. Furthermore, the computational costs during training and inference are high. Only faster and tailored processing units like \acp{GPU} enable the practical application of Deep Learning models. Finally, the presentation learned by the model is not transparent. Hence, the process that leads to the decision of a Computer Vision system can usually not be understand by a human. Also, there is no guarantee that the learned representation is not highly redundant. This is particularly problematic for devices with time constraints and limited computational resources like \acp{MAV}.
	
	This work investigates the detection of \acp{EWFO} on \ac{MAV}. \acp{EWFO} consist of simple shapes but are largely occupied by background. Hence, other objects of interest and/or distractors can appear in this area. Furthermore, sensor and lens properties as well as motion noise can have large impact on the appearance of \acp{EWFO} in the image. This makes the design of an appropriate feature extractor a non-trivial task. Deep Learning would allow to learn a feature extractor and object model given data and the task.
	
	This work address the design of a Deep Learning model for the task of \ac{EWFO}-Detection.
	
	The relevant research question of this chapter is stated as follows:\\
	\textbf{How can a detection model represent \acp{EWFO}?}
	
	\begin{enumerate}
		\item[\textbf{RQ2.1}] Can state-of-the art models represent an \ac{EWFO}?
		\item[\textbf{RQ2.2}] What is the representation a state-of-the-art model learns for the Detection of \acp{EWFO}?
		\item[\textbf{RQ2.3}] Can the insights be used to create a more suitable model for the Detection \ac{EWFO}?
	\end{enumerate}

	The first question will be answered by analysing the performance of state of the art model on the detection of \acp{EWFO}. RQ2.2 will be answered by conduction a sensitivity analysis on the trained model and visualizing the internal representation. RQ2.3 will be answered by refactoring the model architecture and examining whether the performance can be improved or weights can be removed.

	The rest of the chapter is organized as follows: \Cref{sec:object_detection:related} discusses relevant related work. \Cref{sec:object_detection:approach} describes the methodology of this work. \Cref{sec:object_detection:hypothesis} formulates several hypotheses to be investigated. \Cref{sec:object_detection:experiments} outlines the experiments conducted to evaluate the formulated hypotheses. \Cref{sec:object_detection:results} describes the obtained results. \Cref{sec:object_detection:discussion} discusses the results. \Cref{sec:object_detection:conclusion} answers the research question and formulates a conclusion.



\section{Approach}
\label{sec:object_detection:approach}
	$$
	L({p_i},{t_i}) = \frac{1}{N_{cls}} \sum L_{cls}(p_i,p_i*) + \lambda \frac{1}{N_{reg}} p_i* L_{reg}(t_i,t_i*)
	$$
	
	Where $i$ is an anchor, $p_i$ the predicted probability of the anchor being an object and $t_i$ a vector containing 4 bounding box offsets to the default anchor size. The ground truth label $t_i*$ contains the true coordinates, while $p_i*$ is 1 if the anchor overlaps a ground truth bounding box by some threshold. \todo{Which exact loss functions are used}


Comparing state of the art results shows the superiority of \acp{CNN}-based methods in basically every vision task \todo{elaborate}. Hence, the first hypothesis formulated is that a state of the art object detector should be able to learn the detection task of wire frame objects. 

As the single class case is considered the loss functions of state of the art detectors is simplified to the following:

\todo{text}

Hence, the only difference between X,Y,Z is ...
Therefore the second hypothesis to be evaluated is that there is not a very large difference between the mentioned methods.


The reason to be assumed responsible for the superiority of \acp{CNN}-based methods is the fact that the can learn powerful object representations directly on the task. \todoref{visualizing cnns} show how the model combines simple shapes like edges, corners and blobs to more complex shapes like noses and eyes. For the task of wire-frame object detection there is no such intuitive combination of such higher order shapes. Therefore the second hypothesis formulated is that the deeper levels of a \acp{CNN} are not necessary for the detector.

\section{Hypothesis}

\label{sec:object_detection:hypothesis}
Several hypothesis are formulated and will be examined experimentally:
\begin{enumerate}
	\item[$\mathcal{H}_1$] \textbf{A \acp{CNN} should be able to learn the object detection task.}
	\item[$\mathcal{H}_2$] \textbf{Considering the single class case state of the art methods will learn the same representation}
	\item[$\mathcal{H}_3$] \textbf{For wireframe objects the deeper layers don't learn anything as the object consist of relatively simple shapes.}
	
\end{enumerate}
\newpage
\section{Experiments}

First we show that many weights in an object detector are superflous when detecting single wire frame objects:
1. We train an object detector on a single but complex object and compare the filters to multiple objects
2. We train an object detector on a single wireframe object and compare the filters to the other feature detectors
3. We use the gained insights to prune the network 
4. The pruned network should perform poorly when used on the complex object
4. We analyse the new network in terms of sensitivity towards: occlusion, colour, distance, angle

\section{Results}

\section{Conclusion}