\chapter{Transfer Learning}
\label{sec:training}

Supervised machine learning relies on the assumption that a hypothesis $h$ can be learned from a limited set of samples $X$ with their corresponding set of labels $Y$. The goal is to learn $h$ from a source set $D_s = \{X_{s},Y_{s}\}$ such that it can be applied on a target set of unknown examples $D_t = \{X_{t}\}$ to obtain the corresponding labels:

$$
h(X_t)\rightarrow Y_t
$$ 

Whether $h$ is performing well, can evaluated by splitting the labelled set of $D_s$ in a training set $D_{train}$ and test set $D_{test}$. $h$ is learned using the full information of $D_{train}$ and applied on the $D_{test}$. By evaluating a performance metric $m_A$ on $D_{train}$ information about the representation strength of $h$ can be inferred. The performance metric $m$ on $D_{test}$ gives an estimate of the performance of $h$ on $D_t$. Comparing $m_A$ and $m$ allows to evaluate whether model and dataset are suitable for the task.
			
This basic supervised machine learning setting relies on the assumption that $D_s$ follows the same i.i.d distribution as $D_t$. However, this can not be assumed within this thesis. As most of the data is artificially created it will share properties with the real data but only to the extent that they can be modelled during creation. For example a graphic engine can not fully capture visual properties of all materials and light sources. Furthermore, are the environmental conditions for applications of the method such as at the IROS 2018 autonomous drone race unknown and can be significantly different to the ones used at training time. When the source domain $S$ only shares a subset of properties with the target domain $T$ this is also referred to as a domain shift scenario. 

Learning models in such an environment is concerned by the field of Transfer Learning. The goal is to find a concept $h$ that performs best in the target domain $T$. Similar to $m_A$ and $m$ the performance $m_S$ in $S$ and the performance $m_T$ in $T$ can be used to evaluate whether a suitable model/dataset has been chosen/is available.

The task of learning from synthetic data can be summarized in:

$$
\text{arg}\max\limits_{h,S} m_t
$$

The goal is to create source domain $D_s$ and find a concept $h$ that maximizes its performance in the target domain $m_t$. This formulation yields two levels at which the domain shift problem can be addressed: (1) Generating data $S$ or (2) finding a hypothesis $h$ that maximizes the performance.

In this thesis two types of domain shifts are considered: (1) A semi-supervised domain shift between synthetic data and real images. This means $D_s$ is available in large quantity, namely it is generated by a graphical engine. For $D_t$ a considerably smaller amount of labels is available \todo{describe real datasets}. 

(2) An unsupervised domain shift when environment conditions at test time can be significantly different to the ones at training time. In this case information about $D_t$ is only available as graphical models of the object of interest and several images of the domain. \todo{double check is this really an unsupervised domain shift since we dont even know X}

The conducted research is limited to \ac{CNN}-based object detection as they are investigated in this thesis. The chapter focuses on domain adaptation while the exact model is described in \autoref{sec:object_detection}.

The relevant question to be investigated in this chapter is the following:

\begin{center}
	\textbf{How can data be generated to train a model for wire frame object detection on \acp{MAV's}?}
	\textbf{How can such a model be made robust against domain shifts?}
\end{center}

The first question will be answered by choosing a state of the art method for object detection and training it with varying properties in $D_s$. By evaluating $m_s$ on synthetic data and $m_t$ on real images the influence of these properties will be measured.  

The second question will be answered by simulating a domain with synthetic data. That is the model will be trained in a room with significant different environment properties $D_s$ from the room it is tested in $D_t$.

The remaining parts of this chapter are structured as follows: \autoref{sec:training:related} discusses relevant related work. Based on the gained insights \autoref{sec:training:hypothesis} formulates several hypotheses to be investigated. \autoref{sec:training:experiments} outlines the experiments conducted to evaluate the formulated hypotheses. \autoref{sec:training:results} describes the obtained results. \autoref{sec:training:conclusion} discusses the results and answers the research question.

\section{Related Work}
\label{sec:training:related}

Domain shifts have been intensively studied in Machine Learning. 

A recent work \cite{Chen2018c} concerns the domain shift for object detection with deep neural networks. Based on the $\mathcal{H}$-Divergence \cite{Ben-David2010} domain classifiers are included in the network on the higher order feature activations. By incorporating the classifier in the training process and using a gradient reverse layer \todoref{gradient reverse} the feature activations are forced to be more similar. This enables to perform feature alignment in and end-to-end training process.

\cite{Xu2017} incorporate the domain adaptation by adversarial training. In a first step the network is trained using samples for the target and source domain. The obtained feature extractor is used to train a domain classifier. Finally the feature extractor is updated by inverting the domain classifier loss function and thus aligning the feature extractors.

\cite{Inoue} use variational auto encoders to create synthetic images.

\cite{Rozantsev} estimate parameters from real images to render synthetic images.

\cite{Peng2017} includes task-irrelevant samples and a source classifier to make the final network robust (?). Called zero-shot domain adaptation as no samples of the target domain are required.

\cite{Liu2018a}

\cite{Peng} use 3D CAD-models to augment the data during training.

Incorporating Camera Effects:
\cite{Carlson2018}
\begin{itemize}
	\item heaps of references in how camera properties influence object detectors
	\item introduces image augmentation pipeline based on physical model: chromatic aberration, blur, exposure, noise and color shift
	\item input parameters are modelled by hand, then randomly selected within "realistic" range
	\item no lense distortion
	\item method seem to benefit for small objects and when oversaturation applies due to camera effects
\end{itemize}


\cite{Vass}

Image Augmentation
\cite{Bai2017},

\ac{DR} was initially introduced by \cite{Tobin2017} for object localization. In contrast to domain adaptation approaches the method does not try to model the domain shift. Instead large variability in the source domain shall enable the model to learn a robust representation that is domain agnostic. \cite{Tremblay2018a} extends the approach to object detection.

The data is generated by a graphical engine. The following properties of the scene and the object are varied \cite{Tremblay2018a}:

\begin{enumerate}
	\item number, type and texture of objects of interest
	\item number, types, colours, scales of irrelevant objects (distractors)
	\item background image
	\item camera pose
	\item light sources
	\item visibility of ground plane
\end{enumerate}

An advantage of \ac{DR} is its ease of implementation and use, no assumptions about the target distribution have to be made neither are samples or labels of the target domain required. However, the method can fail to capture important patterns if the randomization is too strong. For example the movement pattern of a \ac{MAV} is ignored when placing the camera randomly.
 
\section{Approach}
\label{sec:training:hypothesis}

\subsection{Generalization versus Specialization}

While a lot of approaches discussed in \autoref{sec:training:related} aim to learn a general representation that works across domains, other methods try to incorporate more target domain knowledge to improve performance. These two ways can be seen as generalization and specialization.  \todo{mention bias variance trade off here?}


The first hypothesis formulated is that there is a trade-off between generalization and specialization. For example placing the \ac{OoI} at random locations in an image with various backgrounds forces a model to learn a more general representation. This holds especially for wire-frame objects where a large part of the surrounding bounding box is occupied by background. On the other hand, placing the \ac{OoI} align with a scene, only in positions that are physically possible reduces the variance greatly and should simplify the learning problem. Additionally it allows to exploit context cues such as that certain objects can not fly and therefore usually don't appear on sky background. Hence, it is expected that the training performance $m_{a}$ of a certain model $h$ improves with increasing specialization. Moreover, simplifying the learning problem should allow a simpler model to learn the same task as a more complex model. That is with increasing specialization it should be possible to reduce the number of parameters $w$ while $m_{a}$ stays constant. Finally, if increasing specialization of a training set improves $m_{a}$ it follows that the specialization also improves test error $m$ as long as the specialization does not omit patterns that are present in the target domain. For example, if the model is applied in the real world it can safely be assumed that objects obey physical laws.

\subsection{Domain Properties}

In this thesis object detection for wire frame objects on \acp{MAV} is investigated. As in this scenario computational resources are limited, simple models are the preferred solution. Following the first hypothesis it should be possible to use a model with less parameters and thus faster computation time, if more knowledge of the target domain is included. Therefore different domain properties are defined and it is studied how their incorporation in the training process affects model performance.   

\subsubsection{Background}


	Background refers to how the object is embedded in the environment. For example if synthetic objects are placed on random backgrounds this will not necessarily represent the background in the real world.
	
	The property can be influenced when creating the object scene: (1) The objects can be placed on randomly selected images, (2) The background can be chosen according to some properties, e.g. their similarity to the target domain. (3) A full scene can be rendered when creating data.
	
\subsubsection{Object Placement.} 
	
	Object placement refers to the object's location in the scene. For example one would not expect an apple placed at the ceiling of a room.
	
	This property can be controlled when placing the images in background: (1) The objects can be placed at a random location on the background. (2) The objects can be placed similarly as in the target domain.
	
\subsubsection{Illumination.} 
	
	Light conditions can influence the object appearance and its background.
	
	This property can be controlled by the graphic engine: (1) A sun-like light source can be used that spreads equally across the scene. (2) More particular light sources can be used e.g. for simulation artificial indoor light.
	
	
\subsubsection{Camera Placement.} 
	
	Camera placement refers to the view point of the camera. For example on an \ac{MAV} the view point is limited within a range where it is still possible to fly. Randomly placing the camera does not capture such patterns.
	
	This property can be controlled when synthesizing data: (1) The camera can be placed randomly in the scene, as long as the object is still visible. (2) The camera can be placed based on the expectations in the target domain. (3) The camera can be placed following a physical model of a drone.
	
\subsubsection{Camera Properties.} Camera properties refers to the intrinsic camera parameters. For example lens distortion can significantly influence the appearance of an object.
	
	(1) Random properties (2) Camera model, distortion model.
	
\subsubsection{Noise.} Noise can be created by different sources. For example is noise introduced by the sensor, but also by camera motion.
	(1) Gray/Colour noise (2) Motion blur

This leads to the formulation of two hypothesis to be investigated:

\begin{enumerate}
	\item[H1] The more similar the properties between $D_s$ and $D_t$ the higher $m_t$ of a fixed model.
	\item[H2] The less similar the properties between $D_s$ and $D_t$ the more complex a model has to be to keep $m_t$.
\end{enumerate}



\section{Experiments}
\label{sec:training:experiments}

The experiments conducted

Initially those domain properties are investigated that can be controlled when creating the training data. The domain shift between synthethi


\todo{Prepare a synthetic dataset of several rooms}
\todo{Prepare a real dataset of at least two rooms}

\todo{Mesaure similarity between generated set and real set: label distribution (h,w, location, angles), domain(?), h divergence}

\todo{Quantify each property. Make a plot performance vs more effects}



\section{Results}
\label{sec:training:results}

\section{Conclusion}
\label{sec:training:conclusion}

%\section{Summaries}
%\subsection{Modeling Camera Effects to Improve Deep Vision for Real and Synthetic Data\cite{Carlson2018}}
%
