	\chapter{Detecting \ac{EWFO} on \ac{MAV}}
	\label{sec:object_detection}
	
	\ac{CNN}-based Object Detectors offer the advantage that they can be trained on any object given labeled training data is available. However, the networks are typically optimized for solid feature rich objects, such as planes, cars or faces. Furthermore, \acp{CNN} architectures are typically optimized for detection performance rather than inference speed. \acp{EWFO} differ substantially from objects usually used in Object Detection. In addition, this thesis investigates the detection of \ac{EWFO} on \acp{MAV}. Hence, inference speed is of high importance. In control it can be better to have a faster detector with less precision/recall rather than a slow but accurate detector. Furthermore, a more lightweight detector requires less energy and is therefore deployable on smaller hardware/lighter drones and/or requires less energy. Therefore in this chapter the detection of \acp{EWFO} with a \acp{CNN} is studied. An off-the-shelve \acp{CNN}-based detector is optimized for the detection of \acp{EWFO}. Furthermore, the trade-off in detection performance and inference speed is studied in the example of this work. The research questions are summarized in the following:
	
\begin{enumerate}
	\item[\textbf{RQ2}]How can the architecture be optimized to detect \acp{EWFO} more efficiently?
	\item[\textbf{RQ3}]What are the trade-offs in detection performance and inference time when the detection model for \acp{EWFO} is deployed on a \ac{MAV}?
%	\item[\textbf{RQ4}]Can the gained insights be used to build a lightweight and robust detection model for racing gates in the \ac{IROS} Autonomous Drone Race?
\end{enumerate}

	\section{Methodology}
	
	A literature review is conducted in order to find a suitable approach for the detection of \ac{EWFO} on \acp{MAV}. \acp{CNN} achieve currently the best performance however, a drawback is their computational requirements and large amount of required examples. We hypothesize that due to the simplicity of \acp{EWFO} it is possible to synthetically generate training data and to simplify the \acp{CNN} architecture to speed up the inference time.
	
	A trade-off between accuracy and inference speed are One-Stage Object Detectors such as \ac{Yolo} and \ac{SSD}. With YoloV3\cite{Redmona} these two approaches differ mainly in their network architectures and training strategy. YoloV3 offers an optimized architecture with 8 layers which is more suitable for the deployment on a \ac{MAV} than the VGG architecture used by \ac{SSD}. Furthermore, the only framework that allows to use hardware accelerated floating point/matrix operations on the target hardware of this thesis is Darknet. As the full low level implementation of a \ac{CNN} on the hardware is beyond the scope of this work we decide to use TinyYoloV3 as our baseline model.
	
	\subsection{Training Goal}
	The original training goal is defined as follows:
	\if false
	\begin{align}
	&\lambda_{coord} \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}[(x_i-\hat{x}_i)^2 + (y_i-\hat{y}_i)^2 ] \\&+ \lambda_{coord} \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2 +(\sqrt{h_i}-\sqrt{\hat{h}_i})^2 ]\\
	&+ \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}(C_i - \hat{C}_i)^2 + \lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{noobj}(C_i - \hat{C}_i)^2 \\
	&+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{obj}\sum_{c \in classes}(p_i(c) - \hat{p}_i(c))^2 \\
	\end{align}
	\fi
	
	\begin{equation}
	\mathcal{L} = \lambda_{loc}\mathcal{L}_{loc} + \lambda_{obj}\mathcal{L}_{obj} + \lambda_{noobj}\mathcal{L}_{noobj} + \lambda_{class}\mathcal{L}_{class}
	\end{equation}
	where $\mathcal{L}_{loc}$ is the target for bounding box regression, $\mathcal{L}_{obj}$ the loss where a object is present, $\mathcal{L}_{noobj}$ the loss where there is no object and $\mathcal{L}_{class}$ the classification loss. $\lambda$ are trade-off parameters between the multiple targets.
	
	For a single class prediction this can be simplified to:
	
	\begin{equation}
	\mathcal{L} = \lambda_{loc}\mathcal{L}_{loc} + \lambda_{obj}\mathcal{L}_{obj} + \lambda_{noobj}\mathcal{L}_{noobj}
	\end{equation}
	
	The object loss is defined as:
	
	\begin{equation}
		\mathcal{L}_{obj} = \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}(\log(1+\exp(-o_{ij} \cdot \hat o_{ij}))
	\end{equation}
	where $o_{ij}$ is a softmax activation assigned to anchor box $i$,$j$,$\hat o_{ij}$ the ground truth label assigned to that box, 1 for object and -1 for no object.  $\mathbb{1}_{ij}^{obj}$ is 1 if the anchor box at $i$,$j$ is responsible to predict a certain object and 0 otherwise. The responsibility is determined by \ac{IoU} with a ground truth box. The box with the highest \ac{IoU} with the ground truth box gets assigned responsible.\todo{double check}
	
	The noobject loss is defined vice versa but triggered by the $\mathbb{1}_{ij}^{noobj}$ binary variable.

	The localization target is defined as:
	
	\begin{equation}
		\mathcal{L}_{loc} = \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{obj}[(x_{ij}-\hat{x}_{ij})^2 + (y_i-\hat{y}_{ij})^2  + (\sqrt{w_{ij}}-\sqrt{\hat{w}_{ij}})^2 +(\sqrt{h_{ij}}-\sqrt{\hat{h}_{ij}})^2 ]
	\end{equation}
	where $x$,$y$ are the center coordinates and $w$,$h$ are the bounding box width and height. $\mathbb{1}_{ij}^{obj}$ is 1 if the set of output nodes at $i$,$j$ is responsible to predict a certain object and 0 otherwise. 
	
	\subsection{Baseline Architecture}
	
	\Cref{fig:tinyyolov3_arch} shows the a diagram of the baseline TinyYoloV3- architecture. The input image of 416x416 is processed by 5 common layers that decrease the spatial resolution stepwise (max pooling) down to 26x26. Thereby the number of filters per layer increases up to 512. From layer 5 the network splits to two branches responsible for smaller and larger objects. The lower branch predicts larger objects at a grid of 13x13. The higher branch predicts smaller objects at a grid of 26x26.
	
	\begin{figure}[hbtp]
			\centering
		\includegraphics[width=0.8\textwidth]{fig/tinyyolov3_arch}
		\caption{The architecture of the baseline TinyYoloV3. In the common part the spatial resolution decreases each layer down to 26x26, while the width increases from 16 to 512. From layer 5 two branches focus on objects corresponding to different scales. }
		\label{fig:tinyyolov3_arch}
	\end{figure}
	
	\section{Depth and Width}
	
	State of the art results of Computer Vision benchmarks are achieved by particularly deep/wide models. The vast amount of parameters and layers enables to model highly non-linear functions and complex image features. However, \acp{EWFO} do not contain very complex shapes. Instead the object parts are of thin structure and are spread across large parts of the image. Hence, we hypothesize that a very deep/wide network will not perform better than a shallower and thinner counterpart. Also, current feature extractors are designed to detect complex objects of multiple classes. In this work we investigate the single class case. We hypothesize that this requires less filters and thus that a thinner and shallower counterpart can detect the object with equal performance.
	
	\subsection{Experiment}
	
	In order to evaluate our hypothesis we conduct a experiment. We train the network with different architectures and evaluate the performance. For training we use the 20 000 from the dataset created in \Cref{sec:training}. For testing we use the test set described in \Cref{sec:training:methodology}.
	
	In a first experiment the TinyYoloV3 architecture is used as baseline and the number of filters is decreased stepwise by a factor of 2. The number of layers is held constant.
	
	In a second experiment the width is held constant and the depth is varied. The exact scheme to change depth is visualized in \Cref{fig:depth_changes}. In order to remove layers the convolutional layers are removed while the pooling layers are kept. For the increase of depth 2 layers are inserted stepwise at the end of both branches.  
	
		\begin{figure}[hbtp]
			\centering
			\includegraphics[width=0.8\textwidth]{fig/depth_changes}
			\caption{Architectural Changes when varying the depth. The upper graph shows in which order layers are removed. The lower graph shows how layers are added. }
			\label{fig:depth_changes}
		\end{figure}
	
	\section{Results}
	
	\Cref{fig:perf_width} shows the performance for thinner and wider networks. It is apparent how the performance only undergoes slight variations despite reducing the total number of weights by a factor 1000. \todo{retrain one in the middle to get variance, it should be more linear}
	
	
	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/perf_width}
		\caption{Performance in simulation when varying the amount of filters per layer. Starting from the baseline architecture with approximately 9 Mio. weights, the amount of filters per layer are decreased stepwise by a factor of 2. Only minor effects on performance can be seen, despite reducing the flexibility of the model.}
		\label{fig:perf_width}
	\end{figure}
	
	\Cref{fig:perf_depth} shows the performance for different amount of layers. It can be seen how the performance increases until 8 layers, whereas afterwards no further performance is gained.
	
	\Cref{fig:depth_ap_size} shows the performance for bins of different object sizes. It can be seen how the deeper models perform better at larger object sizes. The deepest model however looses performance for the smaller object sizes.

	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/perf_depth}
		\caption{Performance in simulation when varying the amount of layers. The exact architectures are shown in TODO. The performance does not increase further with more than 8 layers.}
		\label{fig:perf_depth}
	\end{figure}


	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/depth_ap_size}
		\caption{Performance in simulation of models with varying depth and for bins of different size. It can be seen that the performance for larger objects increases with the amount of layers.}
		\label{fig:depth_ap_size}
	\end{figure}
	
	\Cref{fig:size_bins} shows the distribution of object sizes in the bins used for evaluation. It can be seen that most objects in the test set are further away.
	
	\begin{figure}[hbtp]
		\centering
		\includegraphics[width=\textwidth]{fig/distr_size}
		\caption{Label distribution in bins of different object size. As the field of view is larger for objects further away, the proportion of small objects is higher.}
		\label{fig:size_bins}
	\end{figure}
	
	\subsection{Discussion}
	
	The overall performance is only slightly affected when reducing the number of weights in terms of width and height. We can assume that this is because the objects we investigate are of quite simple structure. Intuitively the features to be considered are color, lines and edges in certain combinations. Hence, it seems logical that only a few filters are necessary to represent these shapes.
	
	The performance in terms of different object varies significantly for models with varying depth. Only deeper networks are able to detect larger objects. However, the complexity of the object does not increase for closer objects. In contrary the closer the objects are the less context is visible. Hence, it is unlikely that increased flexibility is the reason for the increase in performance. Instead, it is more likely that the increased receptive field is the reason.
	
	\subsection{Conclusion}
	
	We investigated how width affects the performance for the detection of \acp{EWFO}. We hypothesized that due to the low variance in the investigated object and the simple features, less filters are required than in the baseline architecture. We can confirm this hypothesis as we see that the width can be decreased by a factor of 10 without loosing performance. This leads to a reduction of weights by a factor of 1000. 
	
	Furthermore, we investigated how depth affects the performance of the model. We hypothesized that a shallow network should be able to detect the object as it only consists of relatively simple features. We can confirm this hypothesis for smaller objects as the performance does not increase anymore after the receptive field covers the whole object. Hence, we assume that depth only improves the performance because it increases the receptive field rather than because it increases flexibility. We investigate alternatives to increase the receptive field in further experiments. 
	
	\section{Receptive Field}
	
	In shallow fully convolutional networks the receptive field of the last layer might not cover the whole image. This is of particular effect for \ac{EWFO} as thus it is possible that an anchor gets assigned responsible which does not see any feature of the object.
	
	A deeper network is one way to increase receptive field but comes at cost of computation. An alternative is the use of a larger kernel size, dilated convolutions or pooling. We compare these methods in terms of computational efficiency and performance.
		
	\section{Reducing Inference Time}
	
	To this end we investigated how to incorporate 
	Next to performance, inference time is a crucial parameter for the detection of \ac{EWFO} on \ac{MAV}. 
	
	 It is determined by the number of computations but also how these computations can be executed. Theoretically the convolutions of one layer in a \acp{CNN} can be executed fully in parallel, however this operation has to be supported by the computational platform. The execution time further depends on how fast a computational platform can execute multiplications, how large the memory is and how fast it can be accessed, but also on the particular low level software implementation. So far we created a model that is 
	 
	 \section{What did we learn?}
	
	\section{Implementation on the \ac{MAV}}