\chapter{Synthesizing Data to Train an Object Detector for Empty Wire Frame Objects on an \ac{MAV}}
\label{sec:training}

Machine learning relies on the assumption that a general concept can be extracted from a limited set of examples. In Supervised Machine Learning, the set of examples $X$ is augmented with a set of labels $Y$ that describes task dependent information that is present in the sample. For Object Detection this could be the coordinates of the bounding box that surrounds the object of interest. The goal is to learn a hypothesis $h$ from a source set $D_S = \{X_{S},Y_{S}\}$ such that it can be applied on a target set of unknown examples $D_T = \{X_{T}\}$ to obtain the corresponding target labels:

$$
h(X_T)\rightarrow Y_T
$$ 

Whether $h$ is performing well, can be evaluated by splitting $D_S$ in a training set $D_{train}$ and test set $D_{test}$. $h$ is learned using the full information of $D_{train}$ and applied on $D_{test}$. By evaluating a performance metric $m$ on $D_{test}$ the expected performance of $h$ on $D_T$ can be estimated and allows evaluate whether model and dataset are suitable for the task.
			
In this setting it is assumed that $D_S$ and $D_T$ are \ac{i.i.d.}. However, this can not be assumed within this thesis. As most of the data is artificially created it will share properties with the real data but only to the extent that they can be modelled during creation. For example a graphic engine can not fully capture visual properties of all materials and light sources. Furthermore, are the environmental conditions for applications of the method such as at the \ac{IROS} 2018 Autonomous Drone Race unknown and can be significantly different to the ones used at training time. When the source domain $S$ only shares a subset of properties with the target domain $T$ this is also referred to as a domain shift scenario.

Learning models in such an environment is concerned by the field of Transfer Learning. The goal is to find a concept $h$ that performs best in the target domain $T$. Similar to $m_A$ and $m$ the performance $m_S$ in $S$ and the performance $m_T$ in $T$ can be used to evaluate whether a suitable model/dataset has been chosen/is available.

The task of learning from synthetic data can be summarized in:

$$
\text{arg}\max\limits_{h,S} m_T
$$

The goal is to create source domain $D_S$ and find a concept $h$ that maximizes its performance $m_T$ in the target domain. This formulation yields two levels at which the domain shift problem has to be addressed: (1) Generating data $S$ and (2) finding a hypothesis $h$ that maximizes the performance.

This thesis addresses the first level. Data is generated to train a model for real world applications. The focus lays on properties that can be influenced with a graphical engine, such as light conditions and camera placement. As the objects of a graphical engine are rendered, another shift in appearance can be expected due to the limitations of the graphic engine to display the real world. However, this domain shift is not considered as it is beyond the scope of this work. Furthermore the conducted research is limited to \ac{CNN}-based object detection as they are investigated in this thesis. The chapter focuses on data generation while the exact model is described in \Cref{sec:object_detection}.

The relevant question to be investigated in this chapter is the following:

\textbf{How can data be generated to train a detection model for \ac{EWFO} detection on a \acp{MAV}?}

\begin{enumerate}
	\item[\textbf{RQ1.1}]What are the implications of the properties of \acp{EWFO} when synthesizing data?
	\item[\textbf{RQ1.2}]What are the properties of the target domain, namely the \ac{FPV} camera of a \ac{MAV} in a GPS denied scenario?
	\item[\textbf{RQ1.3}]Can target domain knowledge be used to improve model performance/simplify model complexity ?
	\item[\textbf{RQ1.4}]How can data be generated in a way that the trained model is robust against expected domain shifts?
\end{enumerate}

RQ1.1 Will be answered by comparing the implications of domain properties on \ac{EWFO} and a standard object. RQ1.2 will be answered by analysing example datasets of a target domain. For RQ1.3 and RQ1.4 several domain properties are defined. The questions will be answered by choosing a state of the art method for object detection and training it with varying properties in $D_S$. By evaluating $m_S$ on synthetic data and $m_T$ on real images the influence of these properties will be measured.

The remaining parts of this chapter are structured as follows: \Cref{sec:training:related} discusses relevant related work. \Cref{sec:training:meth} describes the used methodology. Based on the gained insights \Cref{sec:training:hypothesis} formulates several hypotheses to be investigated. \Cref{sec:training:experiments} outlines the experiments conducted to evaluate the formulated hypotheses. \Cref{sec:training:results} describes the obtained results. \Cref{sec:training:conclusion} discusses the results and answers the research question.

\section{Related Work}
\label{sec:training:related}

Using synthetic images to train Machine Learning models has a long tradition in Computer Vision especially for tasks where not many example images are available or the ground truth labelling is expensive. Methods vary from changing low level properties of images like brightness or scale over pasting 3D-models of objects on real backgrounds to rendering full 3D-environments. Other publications show how the incorporation of sensor effects is important when generating data. However, most of these methods can be combined in different pipelines. Therefore most publications do not use on particular approach but different combinations. The related approaches are categorized in on which image level they operate.

\subsection{Augmenting Real Data}

\subsubsection{Low-Level Image Augmentation}

Low-Level Image Augmentation applies transformations on the samples of a given dataset. It is a common part of current Computer Vision pipelines.

\citeauthor{Krizhevsky2012a} \cite{Krizhevsky2012a} use PCA to incorporate colour variations. \cite{Howard2013} shows how several image transformations can improve the performance of a Classification model. In Object Detection \cite{Redmon} use random scaling and translation of the input image, as well as random variations in saturation and exposure. \citeauthor{Liu} \cite{Liu} additionally crop and flip the image with a certain probability. The common aim of these methods is to make the model more invariant towards changes in the augmented properties.

In contrast \citeauthor{Carlson2018}\cite{Carlson2018} augment the image based on a physical camera model. The proposed pipeline incorporates sensor and lens effects like chromatic aberration, blur, exposure and noise.

Low-Level Image Augmentation is a comparatively cheap method to generate a lot of data. However, it cannot create totally new samples or view points. Furthermore, it cannot change the scene in which an object is placed. Therefore it is restricted to some extent to the dataset that to be augmented.

In this work only the dataset of one room is available which is too small to train a model for multiple rooms with different environmental conditions. Hence, low-level image augmentation cannot be the only method for data generation.

\subsubsection{Augmenting Real Images with 3D - Models}

If a 3D-model of an object is available, it can be placed on real images to artificially create or increase a dataset. 

Several publications demonstrate the usefulness of this method in Object Detection \cite{Girshick2013}, \cite{Peng}.

While the previous methods simply place the object in a scene \citeauthor{Rozantsev} \cite{Rozantsev} propose to estimate rendering parameters first.

In contrast augmenting the images on a low-level this method allows to generate new view points and does not require original images to augment. However, the created image is likely to be relatively artificial as for example light conditions and shadows are not align with the background scene. Furthermore can the object be placed in quite arbitrary positions and thus violate geometric and physical properties that are present in real images. 


\subsection{Fully Synthesizing Data}

\cite{Sadeghi2016,Hinterstoisser2017,Krizhevsky2012a}

While training models only in a simulated environment is common for Control tasks, it is less popular in Computer Vision as often the quality of graphic engines is too poor or the rendering is very time consuming. However, the advances in Computer Graphics and faster processing technologies nowadays allow the generation of more realistic images and various studies tried to incorporate these samples in their training process.

\cite{Ros2016, Gaidon2016} are fully rendered datasets used for Computer Vision. 

\cite{Johnson-Roberson2016} training in simulation only.

\cite{Tobin2017} was one of the first publications that used only synthetic images for the task of Object Localization. The proposed method \ac{DR} uses a high degree of randomization when rendering the image. Thus a model has to learn a general representation, that can also be applicable in the real world. \cite{Tremblay2018a} extends the approach to object detection.

The data is generated by a graphical engine. The following properties of the scene and the object are varied \cite{Tremblay2018a}:

\begin{enumerate}
	\item number, type and texture of objects of interest
	\item number, types, colours, scales of irrelevant objects (distractors)
	\item background image
	\item camera pose
	\item light sources
	\item visibility of ground plane
\end{enumerate}

An advantage of \ac{DR} is its ease of implementation and use, no assumptions about the target distribution have to be made neither are samples or labels of the target domain required. However, the method can fail to capture important patterns if the randomization is too strong. For example the movement pattern of a \ac{MAV} is ignored when placing the camera randomly.


\subsection{Augmenting Synthetic Data}

\subsection{Others}
Approaches that require samples of the target domain \cite{Chen2018c} \cite{Xu2017} \cite{Inoue} 

\cite{Peng2017} includes task-irrelevant samples and a source classifier to make the final network robust (?). Called zero-shot domain adaptation as no samples of the target domain are required.


\section{Methodology}
\label{sec:training:meth}

In the following methods used in this thesis are described. We define several domain properties and model them during data generation.

The models are implemented in a toolchain that applies the different steps. An overview can be seen in \Cref{fig:training:toolchain_datagen} \todo{Put example images}. The individual steps are described in the following.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{fig/Toolchain_datagen}
	\caption{Overview of the data generation process. The environment model determines background and lightning conditions and produces a 3D-Scene. The motion model determines the camera pose and location and thus the view point. Projecting the 3D-Scene on the image plane of the camera delivers the 2D-View. A final post processing step incorporates sensor and lens effects.}
	\label{fig:training:toolchain_datagen}
\end{figure}

\subsection{Environment Model}
\label{sec:training:scene}
We define environment properties as illumination and background. For example a scene can be set outdoor in the forest at night or indoor with artificial light sources. Two methods are used two create an environment.

\subsubsection{Pasting a 3D-Model on Real Images}

Similarly to several methods in literature \cite{Girshick2013, Peng, Rozantsev} a 3D-Model of the \ac{TO} is pasted on real images.

The 3D-Model is provided by TODO and a tool to render these models is implemented using OpenGL. A scene with black background is created and several objects are placed on a virtual ground plane with different rotations to each other. Additionally light sources are placed at different locations and with different intensities. The exact light model is part of OpenGL and will not be further discussed in this thesis. 

After creating a scene the black background is replaced with an image obtained from a dataset. As the light conditions between background image and \ac{TO} are different, the created image can be quite artificial. Hence a postprocessing step applies a Gaussian blur kernel along the edges of the \ac{TO}. This leads to a better embedding of the \ac{TO} in the background.
Examples can be seen in \Cref{fig:random_bg}

The parameters and locations of objects and light sources as well as the selected backgrounds are parameters of the implemented tool that can be varied when creating a dataset.

\begin{figure}[hbtp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/shot}
	\end{minipage}
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/random_bg2}
	\end{minipage}
	\caption{Examples for pasting the 3D-Model on various backgrounds. A scene with background is rendered (left), subsequently the black background is replaced with an image from a dataset.}
	\label{fig:random_bg}
\end{figure}

\subsubsection{Full Rendering of a Scene}

Following \cite{Ros2016, Gaidon2016, Johnson-Roberson2016, Tobin2017, Tremblay2018a} the second approach to create a scene is fully rendering the environment using a graphic engine, namely the UnrealEngine including the AirSim plugin.

The UnrealEditor allows to manually create environments and provides high quality rendering of objects, textures and light conditions. In total three environments, namely three indoor scenarios are created. This resembles GPS-denied scenarios as they are targeted in this thesis. Within the environment the light conditions, background textures, object locations can be changed manually.

In total three base environments are created. Examples can be seen in \Cref{fig:environments}.

\begin{figure}[hbtp]
	\centering
	\begin{minipage}{\textwidth}
		\includegraphics[width=\textwidth,height=5cm]{fig/basement_perspective}
	\end{minipage}


\begin{minipage}{\textwidth}
	\includegraphics[width=\textwidth,height=5cm]{fig/daylight_perspective}
\end{minipage}


\begin{minipage}{\textwidth}
	\includegraphics[width=\textwidth,height=5cm]{fig/iros_perspective}
\end{minipage}
\caption{Base environments when rendering a full scene. From top to bottom: \textit{Basement}, \textit{Daylight}, \textit{IROS}.}
\label{fig:environments}
\end{figure}

\subsection{Motion Model}

The motion model refers to the camera's point of view. For the implemented tools in \Cref{sec:training:scene} the camera view point can be set during data creation. The camera pose is determined by:

$$
\text{Translation: }t = [x y z] \text{ and Rotation: } r = [\phi, \theta, \psi]
$$
Where the left-handed coordinate system is used with $z$ pointing in the image.

\subsubsection{Random Placement}
	
The value for each dimension of $r$ and $t$ are drawn from a probability distribution. The chosen distributions have to follow certain limitations, for example the gate should still be visible in most images.
	
\subsubsection{Quad-rotor Model}
	
$r$ and $t$ follow a the motion model of a quad-rotor \ac{MAV}.  The development of this model has not been done within this thesis but is summarized here for completeness: \todo{put shuos model here}.

Using this motion model, the camera follows a certain trajectory through the 3D-Environment. Storing the current image at a frequency of 2 Hz creates the corresponding samples.
	
\subsection{Sensor Model}

In the real world camera and lens have significant influence on the image appearance as well as the performance of neural networks \cite{Andreopoulos2012,Dodge2016a}. In order to model these effects a post-processing step is applied on the images obtained in the previous steps. The experiments in \cite{Carlson2018} show how such a step can be particularly beneficial when applied on artificially generated data. 

Inspired by \cite{Carlson2018} the model for chromatic aberration, exposure, blur and noise is included. However, on the camera of the \ac{MAV} we assume that we can process the raw image signal. Hence, no model for information loss due to post-processing is applied. Instead all images are converted into YUV-colour space, a format that is obtained from most visual sensors. The pipeline is extended by two models: (1) A model for lens distortion since \ac{MAV} often used wide angle lenses to increase their \ac{FoV}; (2) A model for motion blur since fast camera movements are to be expected.

The post-processing pipeline is summarized as:

\begin{equation}
	I' = \phi_{noise}(\phi_{exposure}(\phi_{FocusBlur}(\phi_{MotionBlur}(\phi_{Chrom.}(\phi_{Lense}(I))))))
	\label{eq:postprocess}
\end{equation}

The individual models are described in the following.

\subsection{Lens Distortion}

\todo{Explain what lense does and its effects}

Lens distortion is applied using the model for wide-angle lenses from \cite{Vass}. It models the \textit{removal} of lens distortion as combination of radial and non-radial part, that is approximated with a second order Taylor expansion:

\begin{equation}
\begin{pmatrix}
	p_x' \\
	p_y'
\end{pmatrix} = \begin{pmatrix}
p_x(1 + \kappa_1 p_x^2 + \kappa_1 (1 + \lambda _x)p_y^2 + \kappa_2(p_x^2 + p_y^2)^2) \\
p_y(1 + \frac{\kappa_1 }{s}p_x^2 + \frac{\kappa_1}{s} (1 + \lambda _y)p_y^2 + \frac{\kappa_2}{s}(p_x^2 + p_y^2)^2)
\end{pmatrix} 
\label{eq:distortion}
\end{equation}
Where:
\begin{itemize}
	\item $p_x'$ and $p_y'$ are the undistorted coordinates.
	\item $\kappa_1$ controls the primary distortion (default 0)
	\item $\kappa_2$ controls the secondary distortion (default 0)
	\item $s$ controls the squeeze factor (default 1)
	\item $\lambda_x$ and $\lambda_y$ controls asymmetric distortion (default 0)
\end{itemize}
 
Applying the lens distortion to an image is done using the inverse of \Cref{eq:distortion}. However, as there is no closed form solution,the Newton-approximation is used:

\begin{equation}
	p_i = p_{i-1} - \nabla p^{-1} (f(p)_i-p')
\end{equation}

Where $f$ is the function defined in \Cref{eq:distortion}. For derivation and implementation the interested reader is referred to \cite{Vass}.

In \Cref{fig:distortion} an example is shown. The distortion is applied on the original image (left) using $k_1=0.8, k_2=0$ for the radial and non-radial part. As the distortion leads to an increased \ac{FoV}, this creates black borders in the image. Therefore the center image part is cropped and rescaled to the original image size. The right image shows the same image after the crop and rescale operation without the distortion. Thus we see the original scene (left), one possible view on that scene with a wide-angle lens (center) and the same view without the lens (right). 

\begin{figure}[htbp]
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example}
	\end{minipage}
\begin{minipage}{0.33\textwidth}
	\includegraphics[width=\textwidth]{fig/gate_example_distorted}
\end{minipage}
\begin{minipage}{0.33\textwidth}
	\includegraphics[width=\textwidth]{fig/gate_example_undistorted}
\end{minipage}
\caption{Example Lens Distortion. Original scene (left), distorted view (center), undistorted view (right)}
\label{fig:distortion}
\end{figure}

\subsubsection{Motion Noise}

Motion blur is a phenomenon depending on camera properties as well as the motion of camera and objects. Although a full modelling of this process might benefit the learning process, it requires a complex pipeline and is computationally expensive. Therefore a strong simplification is used, namely a one-dimensional Gaussian median filter:

\begin{equation}
K_v = \begin{pmatrix}
...				 \\
\mathcal{N}(\mu-1) \\
\mathcal{N}(\mu)  \\
\mathcal{N}(\mu+1)	 \\
	...					
\end{pmatrix} \quad
K_h = \begin{pmatrix}
...	& \mathcal{N}(\mu-1)	&	\mathcal{N}(\mu) &	\mathcal{N}(\mu+1) & ...\\
\end{pmatrix}
	\label{eq:motion_noise}
\end{equation}

Where $\mathcal{N}$ is a Gaussian-PDF with mean $\mu$ and variance $\sigma$,  $K_v$ models vertical motion blur, $K_h$ horizontal motion blur. The size of the kernel is chosen by $k$.

\begin{figure}[htbp]
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_motionblur_v}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_motionblur_h}
	\end{minipage}
	\caption{Example Motion Blur with $k=15$, $\sigma=5.0$. Original scene (left), vertical movement (center), horizontal movement (right)}
	\label{fig:motionblur}
\end{figure}

\subsubsection{Chromatic Aberration}

Similarly to \cite{Carlson2018}, chromatic aberration is applied by scaling the locations of the green channel, as well as applying translations on all channels. The model can be implemented as affine transformation of the pixel locations for each channel:

\begin{equation}
	\begin{pmatrix}
	x' \\
	y' \\
	1
	\end{pmatrix} = \begin{pmatrix}
	S & 0 & t_x \\
	0 & S & t_y \\
	0 & 0 & 1
	\end{pmatrix} \begin{pmatrix}
	x \\
	y \\
	1
	\end{pmatrix}
\end{equation}

\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example}
	\end{minipage}
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_chromatic}
	\end{minipage}
	\caption{Example Chromatic Aberration with $S=[1, 1.01, -5.0]$, $t_x=[10,0.1,0.01]$, $t_y=[0.1,-0.1,0.0.01]$. Original scene (left), after transformation (right)}
	\label{fig:chromatic}
\end{figure}

\subsubsection{Out-of-focus Blur}

Next to motion, sensor noise can lead to blurry images. For the blur operation a 2D Gaussian kernel is applied on the input image with:

\begin{equation}
 k = \frac{1}{2\sigma_x\sigma_y\pi}e^{-\sqrt{\frac{x^2 + y^2}{2\sigma_x\sigma_y}}} 
\end{equation}

\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example}
	\end{minipage}
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_focusblur}
	\end{minipage}
	\caption{Example Out-of-Focus blur.}
	\label{fig:focusblur}
\end{figure}

\subsubsection{Exposure}

With the exposure model:
 
\begin{equation}
 I = f(S) = \frac{255}{1 + e^{-A S}}
\end{equation}
where $A$ is a constant term for contrast and $S$ the exposure.

Following the model from \cite{Carlson2018} the image is re-exposed using:

\begin{equation}
	I' = f(S+\Delta S)
\end{equation}

where $S$ is obtained from :
\begin{equation}
S = f^{-1}(I) = \frac{\ln(\frac{255}{I}-1)}{-A}
\end{equation}

\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example}
	\end{minipage}
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_exposure}
	\end{minipage}
	\caption{Example Exposure with $\Delta S = 2.0$, $A = 1.0$.}
	\label{fig:exposure}
\end{figure}

\subsubsection{Noise}


\subsection{Artificial Augmentation}

Inspired by \cite{Howard2013, Redmon, Liu} the application of several artificial image transformations is studied. The overall goal is to generate more variations in the input signal and thus to make the model more invariant against changes in those properties. We do not use image scaling or translation for augmentation as it is easier to incorporate such variations using the motion model.

\begin{enumerate}
	\item \textbf{Brightness} In order to obtain a model that is more robust against illumination changes image brightness is alternated. Therefore a scaling on the V-channel in HSV-colour space is applied. The scaling is drawn from a uniform distribution in between $b_1$ and $b_2$.
	
	\item \textbf{Grayscale} By transforming a subset of samples into grayscale images, the model is forced to learn more color invariant features. A grayscale conversion is applied with a probability of $g$.
	
	\item \textbf{Histogram Equalization} Changes in the environmental conditions can also affect contrast. By applying a histogram equalization on a subset of images, variations in contrast are achieved. \todo{histeq}
	
	\item \textbf{Flip} By mirroring the image vertically, more variations in object locations are achieved. The operation is applied with a probability of $f$.
\end{enumerate}


\subsection{Hypothesis}
\label{sec:training:hypothesis}

\subsection{Implications of \ac{EWFO}}

The described approaches mainly focus on objects that have complex structures and therefore provide robust features that can be learned by an object detection model. For example a face provides eyes and a nose, which appearances are influenced to some extend by light conditions but relatively independent from the image background. \ac{EWFO} on the other hand do not contain this kind of complex structure but mainly consist of image background. A comparable work is \cite{Madaan2017} which tries to detect wires that are also of thin structure. However, their experiments focus on a single domain, namely wires in the sky and thus the variations in background are comparatively small.

Therefore, we first investigate whether there are implications of the object shape on data generation. We hypothesize, that \ac{EWFO} are particular sensitive to changes in image background and lightning conditions. 

\subsection{Generalization versus Specialization}
\label{sec:training:genvsspc}

While some approaches discussed in \Cref{sec:training:related} aim to learn a general representation that is robust against sensor noise or changes in illumination, other methods try to incorporate more target domain knowledge to improve performance. These two ways can be seen as generalization and specialization.

We hypothesize that there is a trade-off between these two entities. Assuming one model that is trained in rooms with different lightning conditions and another model that is trained in an environment with constant lightning conditions model trained in different lightning conditions might perform we

For example placing the \ac{TO} at random locations in an image with various backgrounds forces a model to learn a general representation that is invariant to the background. This holds especially for \acp{EWFO} where a large part of the surrounding bounding box is occupied by background.

On the other hand, placing the \ac{TO} align with a scene, only in positions that are physically possible reduces the variance greatly and should simplify the learning problem. Additionally it allows to exploit context cues such as that certain objects can not fly and therefore usually don't appear in the sky.

Hence, it is expected that the performance $m_{S}$ in the source domain  of a certain model $h$ improves with increasing specialization. Moreover, domain specialization should allow a more lightweight model to learn the same task equally well as a more complex model. That is with increasing specialization it should be possible to reduce the number of parameters $w$ while $m_{S}$ stays constant. Finally, if increasing specialization of a training set improves $m_{S}$ target performance $m_T$ should also improve as long as the specialization does not omit patterns that are present in the target domain. For example, if the model is applied in the real world it can safely be assumed that objects obey physical laws.

In this thesis object detection for wire frame objects on \acp{MAV} is investigated. As in this scenario computational resources are limited, simple models are the preferred solution. Following \Cref{sec:training:genvsspc} it should be possible to use a model with less parameters and thus faster computation time, if more knowledge of the target domain is included. Therefore different domain properties are defined and it is studied how their incorporation in the training process affects model performance.

The aforementioned hypotheses are summarized in the following:

\begin{enumerate}
	\item[$\mathcal{H}_1$] \ac{EWFO} are more sensitive to domain shifts than other objects. Hence, the performance drop between $m_T$ and $m_S$ of a model $h$ trained in $S$ where the environment conditions in $S$ are very different to the ones in $T$ will be larger for a \ac{EWFO} than for an object with rich structure.
	
	\item[$\mathcal{H}_2$] The more general a model has to be the harder the learning problem becomes. Hence $m_{train}$ drops with an increasing degree of generalization $g$.
	
	\item[$\mathcal{H}_3$] The more specialized a model is, the less parameters are necessary to achieve the same performance. Hence, $m_train$ remains constant when reducing the degree of generalization and the number of parameters $w$.
	
	\item[$\mathcal{H}_4$] The more properties of $T$ can be incorporated in $S$ the less complex a model has to be to achieve similar performance. That is the same $m_T$ can be achieved with a model with smaller $w$ when the properties in $S$ and $D$ are closer.
\end{enumerate}







\section{Experiments}
\label{sec:training:experiments}


The experiments conducted

Initially those domain properties are investigated that can be controlled when creating the training data. The domain shift between synthethi


\todo{Prepare a synthetic dataset of several rooms}
\todo{Prepare a real dataset of at least two rooms}

\todo{Mesaure similarity between generated set and real set: label distribution (h,w, location, angles), domain(?), h divergence}

\todo{Quantify each property. Make a plot performance vs more effects}



\section{Results}
\label{sec:training:results}

\section{Conclusion}
\label{sec:training:conclusion}

%\section{Summaries}
%\subsection{Modeling Camera Effects to Improve Deep Vision for Real and Synthetic Data\cite{Carlson2018}}
%
