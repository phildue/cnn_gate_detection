\documentclass{article}
\usepackage{lscape}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{multicol}
\usepackage[a4paper,margin=0.5in]{geometry}
\begin{document}
	\title{Literature Review}
	\author{Philipp Duernay}
	\maketitle
	\begin{multicols}{2}
		
	\section{Summaries}
	\subsection{Object Detection}
		\subsubsection{Scalable Object Detection using Deep Neural Networks\cite{Erhan}}
		\begin{itemize}
			\item[-] Generates number of bounding boxes as object candidates (class agnostic) and confidences for each box
			\item[-] For each Bounding Box a classifier is run e.g. DNN
			\item[-] Training: If the number of boxes k is larger than the number of objects b, only b boxes are matched while the confidence of the others is minimized
			\item[-] Assignment problem $$F_{match}(x,l) = \frac{1}{2}\sum_{i,j}x_{ij}||l_i - g_j||^2_2$$ where $x_ij$ is one if the ith prediction is assigned to the jth ground truth object
			\item[-] Confidence: 
			$$F_{conf}(x,c) = - \sum_{i,j}x_{ij}*\log(c_i)-\sum_{i}(1-\sum_{j}x_{ij})\log{1-c_j}$$
			\item[-] Speed up training by clustering (kmeans) of ground truth and using it as prior (prior matching)
			\item[-] Can be defined to output boxes only for a particular class by training the bounding boxes on that class
			\item[-] Number of parameters grows linearly with number of classes
			\item[-] Authors argue two step process (region proposal + classification) is better
			\item[-] Architecture based on AlexNet
			\item[-] Predicted boxes are merged using non-maxima surpression
			\item[-] One shot(50\%), +2scales (75\%)
			\item[-] OverFeat/ Selective Search are faster but much more expensive
		\end{itemize}
		\subsection{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\cite{Rena}}
		\begin{itemize}
			\item[-] Introduced Region Proposal Networks, Fully Convolutional Networks that generate region proposals. Feature maps are shared with detection networks and can be trained end-to-end
			\item[-] Introduces Anchor Boxes:
			\begin{itemize}
				\item A fully convolutional network predicts a feature map, the feature map is fed into two fully connected sibling networks for box regression and classification(Object/No object)
				\item The fully connected layers work in sliding window fashion. For each window location is predicted: 4 coordinates with respect to the anchor box. An anchor box is located at a the center of the window and associated with an aspect ratio
				\item[-] (General Remark) Anchor boxes are used to spatially constrain regression heads. Otherwise they would always interfere with each other. What if certain objects only appear at certain locations in the image? Aren't the regression heads then only trained on those objects?
			\end{itemize}
			\item[-] TODO
		\end{itemize}
		\subsection{Yolo}
		TODO
		\subsection{Yolo v2}
		\begin{itemize}
			\item[-] Better
			\begin{itemize}
				\item[+] Batch Normalization (gets rid of dropout, improves mAP)
				\item[+] Higher resolution 416x416, improves mAP
				\item[+] Anchor Boxes are predicted by convolutional layer, that predicts offsets. Offsets are easier to learn. Improve in Recall although decrease in accuracy.
				\item[+] Center of image should be single box
			\end{itemize}
		\end{itemize}
			 \subsubsection{Single Shot Multibox Detector \cite{Liu}}
			 \begin{itemize}
			 	\item[-] Evaluates feature maps with different scales for all (a few) boxes in the image
			 	\item[-] Scale of feature map decreases each layer (feature map with different scales is a key difference to yolo and overfeat)
			 	\item[-] Based on VGG-16
			 	\item[-] Uses convolutional layers for classification instead of fully connected layers (yolo)
			 	\item[-] Similar to other box predictors, the ground truth box has to be chosen for training. Here this is done with the jaccard overlap (>0.5). Boxes can overlap.
			 	\item[-] LOSS FCN
			 	\item[-] Uses lower level feature maps in later state for prediction
			 	\item[-] Data augmentation significantly increases performance (~9\%)
			 	\item[-] More default boxes is better
			 	\item[-] Uses atrous algorithm to cover up holes when changing top layer
			 	\item[-] Replaces pooling with feature map at different scales
			 	\item[-] Non-maxima surpression at the end to get rid of the big amount of boxes (1.7ms)
			 	\item[-] Most time spent in base network and nms
			 	\item[-] Default boxes can have different aspect ratios
			 \end{itemize}
	\subsection{(Re-) Localization}
	\subsubsection{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization \cite{Kendall}}
	\begin{itemize}
			\item[-] Relocalizes, is trained on images from the scenes where it is applied	\item[-] Accuracy 2m and 3$\degree$  in 50 km$^2$ outdoors, 0.5m and 5$\degree$ indoors, 5ms per frame
		\item[-] ConvNet 23 layers, Image resolution 224x224
		\item[-] transfer learning from recognition/classification datasets (ConvNet is trained on classification tasks)
		\item[-] based on GoogleNet, affine regressors instead of softmax\item[-] automatic training data generation (structure from motion)
		\item[-] learns p from arbitrary global reference frame
		\item[-] $loss(I) = || \hat{x}-x||_2 + \beta*||\hat{q}-\frac{q}{||q||}||_2$
		\item[-] separating position/orientation led to drop in performance
		\item[-] PoseNet evaluation at single center crop + Dense PoseNet 128 uniformly spaced crops (time increase 95ms, only slight accuracy increase)
		\item[-] Training data generated using structure from motion (Cambridge Scene) and 7 Scenes (Microsoft) for indoor

	\end{itemize}
	 \subsubsection{A Deep Learning Based 6 Degree-of-Freedom
	 	Localization Method for Endoscopic Capsule Robots \cite{Turan2017}}
	 	\begin{itemize}
	 		\item[-] not published yet?
	 		\item[-] Uses 6-DOF camera pose directly
	 		\item[-] based on GoogleNet (9 Inception modules) trained on ImageNet
	 		\item[-] $loss(I) = ||\hat{x}-x||_2 + ||\hat{q}-q||_2$
	 		\item[-] Dataset of 10 000 frames taken from LM103 - EDG (EsophagoGastroDuodenoscopy) Simulator
	 		\item[-] 0.18cm RMSE on a trajectory of 18cm
	 		\item[-] Although 3 different cameras are used and the frames are separated for training and testing, its still the same "stomach". With 10 000 frames on a trajectory of 18 cm, won't the system just recognize the position?
	 		\item[-] Ground truth determined by seperate cameras 
	 	\end{itemize}
	 	\subsection{Object Pose Estimation}
	 	\subsubsection{3D generic object categorization localization and pose estimation \cite{Savarese}}
	 	\begin{itemize}
	 		\item[-] Other approaches use different class for different poses
	 		\item[-] Object model is separated in different parts of the object based on different view points (front view)
	 		\item[-] Different parts are connected when another part is visible from the front view via affine transformation
	 		\item[-] Generally such models can't handle inter class variations very good or increase in complexity as number of parts is increased. In this paper this is apparently not the case
	 	\end{itemize}
	 	\subsubsection{Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image
	 		\cite{Brachmann}}
	 	\begin{itemize}
	 	\item[-] Intermediate representation are object coordinates, continious part labeling that are jointly regressed for every pixel in the image
	 	\item[-] Based on auto context (Classifiers with several stages)
	 	\item[-] (1) (Auto context) Random forest with L1 regularization predicts labels and object coordinates for every pixel (2) Ransac predicts poses from 2d-3d correspondences guided by uncertainty labels
	 	(3) Refinement
	 	\item[-] Random forest predicts (probability to belong to object + 3d coordinate|given belonging to object)
	 	\item[-] Stacked Forests (Auto context) refine output on previous smoothed output (Geodesic Forest). The smoothing is done to enforce coupling of neighbors 
	 	\item[-] RANSAC formulates hypothesis by drawing 4 correspondences and solving PnP
	 	\item[-] Outperforms PoseNet in indoor localization
	 	\item[-] 6D within 5cm and 5 degree only 40 \% (With RGB-D 82.5\%), on other set 50 \% with unknown scene average median error 8.5cm 3.3Â°
	 	\item[-] Biggest translational error in z direction
	 	\item[-] Multi object detection/pose estimation in 1-4 seconds, not optimized, most time spend in searching for object hypothesis
	 	\end{itemize}
	 	\subsubsection{A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimation\cite{Elhoseiny}}
	 	\begin{itemize}
	 		\item[-] While detection needs pose invariant features, pose estimation needs the pose
		 	\item[-] Single instance 3d model
		 	\item[-] Discrete pose approaches (pose as classification)
		 	\item[-] Trains pose regressor and classifier on output of different levels to measure quality of features
		 	\item[-] Later layers "forget" about pose, paper suggests early branching
	 	\end{itemize}
	\subsection{Other}
		 \subsubsection{Deformable Convolutional Networks \cite{Dai}}
		 \begin{itemize}
		 	\item[-] Addresses problem of modeling geometric transformations
		 	\item[-] Introduces \textit{Deformable Convolution} which adds 2D offsets to the regular sampling grid. The offsets are learned from the data.
		 	\item[-] Introduces \textit{Deformable RoI pooling} which adds offsets to bins of pooling layers. The offsets are also learned from the data.
		 	\item[-] Further alternatives to have more variable feature maps: Spatial Transformer Networks, Active Convolution, Effective Receptive Field, Atrous Convolution, DeepID-Net, Spatial manipulation in RoI pooling (handcrafted), DPM (handcrafted)
		 	\item[-] Light-weight version of STN, easier to train and to integrate
		 	\item[-] Receptive fields seem to scale with the size of objects
		 	\item[-] Model complexity is increased by only 1-2%
		 	\end{itemize}
		 	
\end{multicols}	
\begin{landscape}

\begin{table}[]
	
	\caption{Object Detection}
	\label{my-label}
	\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
		\hline
		& \multicolumn{3}{l|}{Traditional} & \multicolumn{4}{l|}{Deep}   \\ \hline
		& Viola\&Jones    				   & HoG    & DPM   		   & R-CNN    & YOLO         & SSD & OverFeat \\ \hline
		Feature Detector & Haar					   & HoG    & Multiple Hogs and virtual springs   & Learned by CNN     &  Learned by CNN            & & \\ \hline
		Detection & \multicolumn{3}{l|}{Sliding Window, high filter responses indicate there is an object} & NN in sliding window detects regions for possible objects, For each proposed region a classification is run & Image is split in Grid each Grid spawns Bounding boxes and gives class probabilities & & \\
		\hline
		Accuracy (voc) &  & & & 73.2 mAP & 63.4 mAP & 74.3 mAP & \\ 
		\hline
		Speed & & & & 7 FPS (Faster-RCNN) & 45 FPS & 59 FPS & \\
		\hline
		Strengths & & & & & &  &\\
		\hline
		Weaknesses & & & & & &  &\\
		\hline
		\end{tabular}
		
		\end{table}
		\end{landscape}	
\bibliography{literature.bib}
\bibliographystyle{acm}

\end{document}
