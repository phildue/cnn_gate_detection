@article{Loquercio,
	author = {Loquercio, Antonio and Maqueda, Ana I and Del-Blanco, Carlos R and Scaramuzza, Davide},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loquercio et al. - Unknown - DroNet Learning to Fly by Driving.pdf:pdf},
	title = {{DroNet: Learning to Fly by Driving}},
	url = {http://rpg.ifi.uzh.ch/docs/RAL18{\_}Loquercio.pdf}
}
@article{Kong2016,
	abstract = {Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.},
	archivePrefix = {arXiv},
	arxivId = {1604.00600},
	author = {Kong, Tao and Yao, Anbang and Chen, Yurong and Sun, Fuchun},
	eprint = {1604.00600},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong et al. - 2016 - HyperNet Towards Accurate Region Proposal Generation and Joint Object Detection.pdf:pdf},
	month = {apr},
	title = {{HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection}},
	url = {http://arxiv.org/abs/1604.00600},
	year = {2016}
}
@article{Cai2016,
	abstract = {A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1607.07155v1},
	author = {Cai, Zhaowei and Fan, Quanfu and Feris, Rogerio S and Vasconcelos, Nuno},
	eprint = {arXiv:1607.07155v1},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai et al. - 2016 - A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection.pdf:pdf},
	keywords = {()},
	title = {{A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection}},
	url = {https://arxiv.org/pdf/1607.07155.pdf},
	year = {2016}
}
@article{Winter,
	abstract = {3D reconstruction and pose estimation have been huge areas of re-search in recent years. Success in these two areas have allowed enormous strides in augmented reality, 3D scanning, and interac-tive gaming. However, utilizing machine learning in this realm has been a difficulty since creating relevant datasets is extremely time consuming. Additionally, 3D annotated video datasets do not exist yet. By rendering 3D objects, we create a dataset of image se-quences to emulate video frame data. Using a VGG net [Simonyan and Zisserman 2014] and a RNN, we introduce a model that can predict a object's 3D bounding box based on synthetic video-like image sequences. Our model can be extended to predict 3D feature points to create point cloud data for 3D reconstruction.},
	author = {Winter, Hanna K},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winter - Unknown - 3D Point Estimation Using A Recursive Neural Network.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{3D Point Estimation Using A Recursive Neural Network}},
	url = {http://cs229.stanford.edu/proj2016/report/Winter{\_}3DPositionEstimationUsingARecursiveNeuralNetwork{\_}report.pdf}
}
@article{TekinEPFL,
	abstract = {We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [10] that only predicts an approx-imate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a re-sult, it is much faster – 50 fps on a Titan X (Pascal) GPU – and more suitable for real-time processing. The key com-ponent of our method is a new CNN architecture inspired by [27, 28] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our ap-proach substantially outperforms other recent CNN-based approaches [10, 25] when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.},
	author = {{Tekin EPFL}, Bugra and Sinha, Sudipta N and {Fua EPFL}, Pascal},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tekin EPFL, Sinha, Fua EPFL - Unknown - Real-Time Seamless Single Shot 6D Object Pose Prediction.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{Real-Time Seamless Single Shot 6D Object Pose Prediction}},
	url = {https://arxiv.org/pdf/1711.08848.pdf}
}
@article{Mousavian,
	abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current tech-niques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then com-bines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The sec-ond output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation im-posed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Al-though conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmenta-tion and flat ground priors [4] and sub-category detec-tion [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
	author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Ko{\v{s}}eck{\'{a}}, Jana},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mousavian et al. - Unknown - 3D Bounding Box Estimation Using Deep Learning and Geometry.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{3D Bounding Box Estimation Using Deep Learning and Geometry}},
	url = {https://arxiv.org/pdf/1612.00496.pdf}
}
@article{Poirson,
	abstract = {For applications in navigation and robotics, estimating the 3D pose of objects is as important as detection. Many approaches to pose estimation rely on detecting or track-ing parts or keypoints [11, 21]. In this paper we build on a recent state-of-the-art convolutional network for sliding-window detection [10] to provide detection and rough pose estimation in a single shot, without intermediate stages of detecting parts or initial bounding boxes. While not the first system to treat pose estimation as a categorization problem, this is the first attempt to combine detection and pose esti-mation at the same level using a deep learning approach. The key to the architecture is a deep convolutional network where scores for the presence of an object category, the off-set for its location, and the approximate pose are all es-timated on a regular grid of locations in the image. The resulting system is as accurate as recent work on pose esti-mation (42.4{\%} 8 View mAVP on Pascal 3D+ [21]) and sig-nificantly faster (46 frames per second (FPS) on a TITAN X GPU). This approach to detection and rough pose estima-tion is fast and accurate enough to be widely applied as a pre-processing step for tasks including high-accuracy pose estimation, object tracking and localization, and vSLAM.},
	author = {Poirson, Patrick and Ammirato, Phil and Fu, Cheng-Yang and Liu, Wei and Ko{\v{s}}eck{\'{a}}, Jana and Berg, Alexander C},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poirson et al. - Unknown - Fast Single Shot Detection and Pose Estimation.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{Fast Single Shot Detection and Pose Estimation}},
	url = {https://arxiv.org/pdf/1609.05590.pdf}
}
@article{Kehl,
	abstract = {We present a novel method for detecting 3D model in-stances and estimating their 6D poses from RGB data in a single shot. To this end, we extend the popular SSD paradigm to cover the full 6D pose space and train on syn-thetic model data only. Our approach competes or sur-passes current state-of-the-art methods that leverage RGB-D data on multiple challenging datasets. Furthermore, our method produces these results at around 10Hz, which is many times faster than the related methods. For the sake of reproducibility, we make our trained networks and detec-tion code publicly available.},
	author = {Kehl, Wadim and Manhardt, Fabian and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kehl et al. - Unknown - SSD-6D Making RGB-Based 3D Detection and 6D Pose Estimation Great Again.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again}},
	url = {https://arxiv.org/pdf/1711.10006.pdf}
}
@article{Fiaz,
	abstract = {—Visual object tracking is an important computer vision problem with numerous real-world applications includ-ing human-computer interaction, autonomous vehicles, robotics, motion-based recognition, video indexing, surveillance and se-curity. In this paper, we aim to extensively review the latest trends and advances in the tracking algorithms and evaluate the robustness of trackers in the presence of noise. The first part of this work comprises a comprehensive survey of recently proposed tracking algorithms. We broadly categorize trackers into correlation filter based trackers and the others as non-correlation filter trackers. Each category is further classified into various types of trackers based on the architecture of the tracking mechanism. In the second part of this work, we experimentally evaluate tracking algorithms for robustness in the presence of additive white Gaussian noise. Multiple levels of additive noise are added to the Object Tracking Benchmark (OTB) 2015, and the precision and success rates of the tracking algorithms are eval-uated. Some algorithms suffered more performance degradation than others, which brings to light a previously unexplored aspect of the tracking algorithms. The relative rank of the algorithms based on their performance on benchmark datasets may change in the presence of noise. Our study concludes that no single tracker is able to achieve the same efficiency in the presence of noise as under noise-free conditions; thus, there is a need to include a parameter for robustness to noise when evaluating newly proposed tracking algorithms.},
	author = {Fiaz, Mustansar and Mahmood, Arif and {Ki Jung}, Soon},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiaz, Mahmood, Ki Jung - Unknown - Tracking Noisy Targets A Review of Recent Object Tracking Approaches.pdf:pdf},
	keywords = {Deep learning,Index Terms—Background modeling,Inpaint-ing,foreground detection},
	mendeley-groups = {Tracking},
	title = {{Tracking Noisy Targets: A Review of Recent Object Tracking Approaches}},
	url = {https://arxiv.org/pdf/1802.03098.pdf}
}
@article{Li,
	abstract = {—Image-based localization, or camera relocalization, is a fundamental problem in computer vision and robotics, and it refers to estimating camera pose from an image. Recent state-of-the-art approaches use learning based methods, such as Random Forests (RFs) and Convolutional Neural Networks (CNNs), to regress for each pixel in the image its corresponding position in the scene's world coordinate frame, and solve the final pose via a RANSAC-based optimization scheme using the predicted correspondences. In this paper, instead of in a patch-based manner, we propose to perform the scene coordinate regression in a full-frame manner to make the computation efficient at test time and, more importantly, to add more global context to the regression process to improve the robustness. To do so, we adopt a fully convolutional encoder-decoder neural network architecture which accepts a whole image as input and produces scene coordinate predictions for all pixels in the image. However, using more global context is prone to overfitting. To alleviate this issue, we propose to use data augmentation to generate more data for training. In addition to the data augmentation in 2D image space, we also augment the data in 3D space. We evaluate our approach on the publicly available 7-Scenes dataset, and experiments show that it has better scene coordinate predictions and achieves state-of-the-art results in localization with improved robustness on the hardest frames (e.g., frames with repeated structures).},
	author = {Li, Xiaotian and Ylioinas, Juha and Kannala, Juho},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Ylioinas, Kannala - Unknown - Full-Frame Scene Coordinate Regression for Image-Based Localization.pdf:pdf},
	mendeley-groups = {Camera (Re)Localization},
	title = {{Full-Frame Scene Coordinate Regression for Image-Based Localization}},
	url = {https://arxiv.org/pdf/1802.03237.pdf}
}
@article{Kimura,
	abstract = {In this paper, we propose imitation networks, a sim-ple but effective method for training neural net-works with a limited amount of training data. Our approach inherits the idea of knowledge distillation that transfers knowledge from a deep or wide ref-erence model to a shallow or narrow target model. The proposed method employs this idea to mimic predictions of reference estimators that are much more robust against overfitting than the network we want to train. Different from almost all the previ-ous work for knowledge distillation that requires a large amount of labeled training data, the proposed method requires only a small amount of training data. Instead, we introduce pseudo training exam-ples that are optimized as a part of model param-eters. Experimental results for several benchmark datasets demonstrate that the proposed method out-performed all the other baselines, such as naive training of the target model and standard knowl-edge distillation.},
	author = {Kimura, Akisato and Ghahramani, Zoubin and Takeuchi, Koh and Iwata, Tomoharu and Ueda, Naonori},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kimura et al. - Unknown - Imitation networks Few-shot learning of neural networks from scratch.pdf:pdf},
	mendeley-groups = {Training},
	title = {{Imitation networks: Few-shot learning of neural networks from scratch}},
	url = {https://arxiv.org/pdf/1802.03039.pdf}
}
@article{Ashraf,
	abstract = {The ability to automatically detect other vehicles on the road is vital to the safety of partially-autonomous and fully-autonomous vehicles. Most of the high-accuracy techniques for this task are based on R-CNN or one of its faster variants. In the research community, much emphasis has been applied to using 3D vision or complex R-CNN variants to achieve higher accuracy. However, are there more straightforward modifications that could deliver higher accuracy? Yes. We show that increasing input image resolution (i.e. upsampling) offers up to 12 percentage-points higher accuracy compared to an off-the-shelf baseline. We also find situations where earlier/shallower layers of CNN provide higher accuracy than later/deeper layers. We further show that shallow models and upsampled images yield competitive accuracy. Our findings contrast with the current trend towards deeper and larger models to achieve high accuracy in domain specific detection tasks.},
	author = {Ashraf, Khalid and Wu, Bichen and Iandola, Forrest N and Moskewicz, Mattthew W and Keutzer, Kurt},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashraf et al. - Unknown - Shallow Networks for High-Accuracy Road Object-Detection.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Shallow Networks for High-Accuracy Road Object-Detection}},
	url = {https://arxiv.org/pdf/1606.01561.pdf}
}
@article{Huang,
	abstract = {The goal of this paper is to serve as a guide for se-lecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern con-volutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base fea-ture extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [31], R-FCN [6] and SSD [26] systems, which we view as " meta-architectures " and trace out the speed/accuracy trade-off curve created by using alterna-tive feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and mem-ory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance mea-sured on the COCO detection task.},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin and Research, Google},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - Speedaccuracy trade-offs for modern convolutional object detectors.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
	url = {https://arxiv.org/pdf/1611.10012.pdf}
}
@article{Xiang,
	abstract = {SSD [18] is one of the state-of-the-art object detection algorithms, and it combines high detection accuracy with real-time speed. However, it is widely recognized that SSD is less accurate in detecting small objects compared to large objects, because it ignores the context from out-side the proposal boxes. In this paper, we present CSSD– a shorthand for context-aware single-shot multibox object detector. CSSD is built on top of SSD, with additional lay-ers modeling multi-scale contexts. We describe two vari-ants of CSSD, which differ in their context layers, using di-lated convolution layers (DiCSSD) and deconvolution lay-ers (DeCSSD) respectively. The experimental results show that the multi-scale context modeling significantly improves the detection accuracy. In addition, we study the relation-ship between effective receptive fields (ERFs) and the the-oretical receptive fields (TRFs), particularly on a VGGNet. The empirical results further strengthen our conclusion that SSD coupled with context layers achieves better detection results especially for small objects (+3.2{\%}AP @0.5 on MS-COCO compared to the newest SSD [19]), while maintain-ing comparable runtime performance.},
	author = {Xiang, Wei and Zhang, Dong-Qing and Athitsos, Vassilis and Yu, Heather},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiang et al. - Unknown - Context-aware Single-Shot Detector.pdf:pdf},
	mendeley-groups = {Detection,Detection/singleshots},
	title = {{Context-aware Single-Shot Detector}},
	url = {https://pdfs.semanticscholar.org/a299/fd58b86c7d92ac617395b2ada496bc097236.pdf}
}
@article{Linb,
	abstract = {This paper presents a fast vehicle detector which can be deployed on NVIDIA DrivePX2 under real-time constraints. The network predicts bounding boxes with different aspect ratio and scale priors from the specifically-designed prediction module given concatenated multi-scale feature map. A new data augmentation strategy is proposed to systematically generate a lot of vehicle training images whose appearance is randomly truncated so our detector could detect occluded vehicles better. Besides, we propose a non-region-based online hard example mining framework which performs fine-tuning by picking (1) hard examples and (2) detection results with insufficient IOU. Compared to other classical object detectors, this work achieves very competitive result in terms of average precision (AP) and computational speed. For the newly-defined vehicle class (car+bus) on VOC2007 test, our detector achieves 85.32 AP and runs at 48 FPS and 30 FPS on NVIDIA Titan X {\&} GP106 (DrivePX2), respectively.},
	author = {Lin, Che-tsung and Santoso, Patrisia Sherryl and Chen, Shu-ping and Lin, Hung-jin and Lai, Shang-hong},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Fast Vehicle Detector for Autonomous Driving.pdf:pdf},
	keywords = {2,2Intelligent Mobility Division,Che-Tsung Lin1,Hsinchu,Hung-Jin Lin1,Industrial Technology Research Institute,Mechanical and Systems Research Laboratories,National Tsing Hua University,Patrisia Sherryl Santoso2,Shang-Hong Lai1 1Department of Computer Science,Shu-Ping Chen1,Taiwan},
	mendeley-groups = {Detection},
	title = {{Fast Vehicle Detector for Autonomous Driving}},
	url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w3/Lin{\_}Fast{\_}Vehicle{\_}Detector{\_}ICCV{\_}2017{\_}paper.pdf}
}
@inproceedings{TripathiSanDiego,
	abstract = {Deep convolutional Neural Networks (CNN) are the state-of-the-art performers for object detection task. It is well known that object detection requires more computation and memory than image classification. Thus the consolidation of a CNN-based object detection for an embedded system is more challenging. In this work, we propose LCDet, a fully-convolutional neural network for generic object detection that aims to work in embedded systems. We design and develop an end-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit quantization on the learned weights. We use face detection as a use case. Our TF-Slim based network can predict different faces of different shapes and sizes in a single forward pass. Our experimental results show that the proposed method achieves comparative accuracy comparing with state-of-the-art CNN-based face detection methods, while reducing the model size by 3x and memory-BW by {\~{}}4x comparing with one of the best real-time CNN-based object detector such as YOLO. TF 8-bit quantized model provides additional 4x memory reduction while keeping the accuracy as good as the floating point model. The proposed model thus becomes amenable for embedded implementations.},
	archivePrefix = {arXiv},
	arxivId = {1705.05922},
	author = {Tripathi, Subarna and Dane, Gokce and Kang, Byeongkeun and Bhaskaran, Vasudev and Nguyen, Truong},
	booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	doi = {10.1109/CVPRW.2017.56},
	eprint = {1705.05922},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tripathi San Diego et al. - Unknown - LCDet Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems.pdf:pdf},
	isbn = {9781538607336},
	issn = {21607516},
	mendeley-groups = {Detection,Detection/singleshots},
	pages = {411--420},
	title = {{LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems}},
	url = {https://vision.cornell.edu/se3/wp-content/uploads/2017/07/LCDet{\_}CVPRW.pdf},
	volume = {2017-July},
	year = {2017}
}
@article{Tripathi2017,
	abstract = {Deep convolutional Neural Networks (CNN) are the state-of-the-art performers for object detection task. It is well known that object detection requires more computation and memory than image classification. Thus the consolidation of a CNN-based object detection for an embedded system is more challenging. In this work, we propose LCDet, a fully-convolutional neural network for generic object detection that aims to work in embedded systems. We design and develop an end-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit quantization on the learned weights. We use face detection as a use case. Our TF-Slim based network can predict different faces of different shapes and sizes in a single forward pass. Our experimental results show that the proposed method achieves comparative accuracy comparing with state-of-the-art CNN-based face detection methods, while reducing the model size by 3x and memory-BW by {\~{}}4x comparing with one of the best real-time CNN-based object detector such as YOLO. TF 8-bit quantized model provides additional 4x memory reduction while keeping the accuracy as good as the floating point model. The proposed model thus becomes amenable for embedded implementations.},
	archivePrefix = {arXiv},
	arxivId = {1705.05922},
	author = {Tripathi, Subarna and Dane, Gokce and Kang, Byeongkeun and Bhaskaran, Vasudev and Nguyen, Truong},
	eprint = {1705.05922},
	month = {may},
	title = {{LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems}},
	url = {http://arxiv.org/abs/1705.05922},
	year = {2017}
}
@article{Jung,
	abstract = {We present an robust visual detection method for indoor autonomous drone racing (ADR). Our unmanned micro aerial vehicle (MAV), that is built with low-cost, off-the-shelf hardware, detect the racing gates with a monocular camera using deep-learning method. The biggest chal-lenging tasks for this ADR is to detect quickly and reliably race gates while avoiding collisions. In this paper, we introduce convolution neural network (CNN) called single shot detector (SSD) to robustly detect the race gate in various indoor light condition and environment. All vision processing, except flight control, runs in real time on the NVIDIA embedded GPU (Tegra X1) and the learning process is implemented on the desktop GPU (NVIDIA GTX980) hardware platform. We provide details about the hardware system and software algorithms used to implement the proposed solution. Based on the learning results, we show that our proposed solution can provide quick and reliable results in the indoor environment through experiments that pass through the race gate. I. Nomenclature N = number of matched default boxes l = predicted box g = ground truth box L con f = confidence loss L loc = localization loss $\alpha$ = weight term c = multiple classes confidences s min = lowest layer scale s max = highest layer scale m = number of feature maps II. Introduction D racing is gaining popularity as one of the new sports. The improved performance of avionics, FPV devices and quadrotor-type MAV makes it possible [1]. Furthermore, autonomous unmanned race competitions were tried in line with the development of indoor navigation technology and the flow of autonomous robot [2]. Vision-based object detection is an important component in such MAV applications. As the visual device getting smaller and the development of GPU computing algorithms, robot applications using complex vision processing tasks are now available in the MAV platforms. In the drone race, the drone must pass through the race gates placed along the course. Thus, fast and accurate gate detection is an important factor for a successful race. In the drone-racing arena a number of gates are placed with the same shape and color along the course. Therefore, it is difficult to detect the gate with classical image recognition techniques due to the gate duplication problem. Classical method are mainly tuned by people according to feature information (i.e., shape or color) thus, it is quite sensitive to the circumstance. In particular, in the case of two or more gates are overlapped each other with a depth and slight position difference, there is a high possibility that the two gates recognized as one large gate. Therefore, other vision approaches are need to solve this classical image recognition problem.},
	author = {Jung, Sunggoo and Lee, Hanseob and Shim, David Hyunchul},
	doi = {10.2514/6.2018-2138},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung, Lee, Shim - Unknown - Real Time Embedded System Framework for Autonomous Drone Racing using Deep Learning Techniques(2).pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Real Time Embedded System Framework for Autonomous Drone Racing using Deep Learning Techniques}}
}
@article{Lin,
	abstract = {0 0.2 0.4 0.6 0.8 1 probability of ground truth class 0 1 2 3 4 5 loss = 0 = 0.5 = 1 = 2 = 5 well-classiied examples well-classiied examples CE(pt) = − log(pt) FL(pt) = −(1 − pt) $\gamma$ log(pt) Figure 1. We propose a novel loss we term the Focal Loss that adds a factor (1 − pt) $\gamma$ to the standard cross entropy criterion. Setting $\gamma$ {\textgreater} 0 reduces the relative loss for well-classified examples (pt {\textgreater} .5), putting more focus on hard, misclassified examples. As our experiments will demonstrate, the proposed focal loss enables training highly accurate dense object detectors in the presence of vast numbers of easy background examples. Abstract The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object lo-cations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the ex-treme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelm-ing the detector during training. To evaluate the effective-ness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of pre-vious one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at:},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Focal Loss for Dense Object Detection.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Focal Loss for Dense Object Detection}},
	url = {https://github.com/facebookresearch/Detectron.}
}
@article{Inoue,
	abstract = {— Robotic learning in simulation environments pro-vides a faster, more scalable, and safer training methodology than learning directly with physical robots. Also, synthesizing images in a simulation environment for collecting large-scale image data is easy, whereas capturing camera images in the real world is time consuming and expensive. However, learning from only synthetic images may not achieve the desired performance in real environments due to the gap between synthetic and real images. We thus propose a method that transfers learned capability of detecting object position from a simulation environment to the real world. Our method enables us to use only a very limited dataset of real images while leveraging a large dataset of synthetic images using multiple variational autoencoders. It detects object positions 6 to 7 times more precisely than the baseline of directly learning from the dataset of the real images. Object position estimation under varying environmental conditions forms one of the underlying requirement for standard robotic manipulation tasks. We show that the proposed method performs robustly in different lighting conditions or with other distractor objects present for this requirement. Using this detected object position, we transfer pick-and-place or reaching tasks learned in a simulation environment to an actual physical robot without re-training.},
	author = {Inoue, Tadanobu and Chaudhury, Subhajit and {De Magistris}, Giovanni and Dasgupta, Sakyasingha},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue et al. - Unknown - Transfer learning from synthetic to real images using variational autoencoders for robotic applications.pdf:pdf},
	mendeley-groups = {Training},
	title = {{Transfer learning from synthetic to real images using variational autoencoders for robotic applications}},
	url = {https://arxiv.org/pdf/1709.06762.pdf}
}
@article{He,
	abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition ac-curacy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under con-strained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complex-ity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8{\%} top-5 error, 10-view test), yet is 20{\%} faster than " AlexNet " [14] (16.0{\%} top-5 error, 10-view test).},
	author = {He, Kaiming and Sun, Jian},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Sun - Unknown - Convolutional Neural Networks at Constrained Time Cost.pdf:pdf},
	mendeley-groups = {Speed},
	title = {{Convolutional Neural Networks at Constrained Time Cost}},
	url = {https://arxiv.org/pdf/1412.1710.pdf}
}
@article{Laskar,
	abstract = {We propose a new deep learning based approach for camera relocalization. Our approach localizes a given query image by using a convolutional neural network (CNN) for first retrieving similar database images and then predicting the relative pose between the query and the database images, whose poses are known. The camera lo-cation for the query image is obtained via triangulation from two relative translation estimates using a RANSAC based approach. Each relative pose estimate provides a hy-pothesis for the camera orientation and they are fused in a second RANSAC scheme. The neural network is trained for relative pose estimation in an end-to-end manner using training image pairs. In contrast to previous work, our ap-proach does not require scene-specific training of the net-work, which improves scalability, and it can also be ap-plied to scenes which are not available during the training of the network. As another main contribution, we release a challenging indoor localisation dataset covering 5 differ-ent scenes registered to a common coordinate frame. We evaluate our approach using both our own dataset and the standard 7 Scenes benchmark. The results show that the proposed approach generalizes well to previously unseen scenes and compares favourably to other recent CNN-based methods .},
	author = {Laskar, Zakaria and Melekhov, Iaroslav and Kalia, Surya and Kannala, Juho},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laskar et al. - Unknown - Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network.pdf:pdf},
	mendeley-groups = {Camera (Re)Localization},
	title = {{Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network}},
	url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w17/Laskar{\_}Camera{\_}Relocalization{\_}by{\_}ICCV{\_}2017{\_}paper.pdf}
}
@article{Melekhov,
	abstract = {This paper presents a convolutional neural network based approach for estimating the relative pose between two cameras. The proposed network takes RGB images from both cameras as input and directly produces the relative rotation and translation as output. The system is trained in an end-to-end manner utilising transfer learning from a large scale classification dataset. The introduced approach is com-pared with widely used local feature based methods (SURF, ORB) and the results indicate a clear improvement over the baseline. In addition, a variant of the proposed architecture containing a spatial pyramid pooling (SPP) layer is evaluated and shown to further improve the performance.},
	author = {Melekhov, Iaroslav and Ylioinas, Juha and Kannala, Juho and Rahtu, Esa},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Melekhov et al. - Unknown - Relative Camera Pose Estimation Using Convolutional Neural Networks.pdf:pdf},
	keywords = {deep neural networks,relative camera pose estimation,spa-tial pyramid pooling},
	mendeley-groups = {Camera (Re)Localization},
	title = {{Relative Camera Pose Estimation Using Convolutional Neural Networks}},
	url = {https://users.aalto.fi/{~}kannalj1/publications/acivs2017.pdf}
}
@article{YoungwanLee,
	abstract = {Since convolutional neural network(CNN)models emerged,several tasks in computer vision have actively deployed CNN models for feature extraction. However,the conventional CNN models have a high computational cost and require high memory capacity, which is impractical and unaffordable for commercial applications such as real-time on-road object detection on embedded boards or mobile platforms. To tackle this limitation of CNN models, this paper proposes a wide-residual-inception (WR-Inception) network, which constructs the architecture based on a residual inception unit that captures objects of various sizes on the same feature map, as well as shallower and wider layers, compared to state-of-the-art networks like ResNet. To verify the proposed networks, this paper conducted two experiments; one is a classification task on CIFAR-10/100 and the other is an on-road object detection task using a Single-Shot Multi-box Detector(SSD) on the KITTI dataset.},
	archivePrefix = {arXiv},
	arxivId = {1702.01243},
	author = {Lee, Youngwan and Kim, Huien and Park, Eunsoo and Cui, Xuenan and Kim, Hakil and Engineering, Communication},
	eprint = {1702.01243},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Youngwan Lee et al. - Unknown - Wide - Residual - Inception Networks for R eal - time O bject D etection.pdf:pdf},
	mendeley-groups = {Detection},
	month = {feb},
	pages = {2--8},
	title = {{Wide-Residual-Inception Networks for Real-time Object Detection Youngwan}},
	url = {https://arxiv.org/pdf/1702.01243.pdf http://arxiv.org/abs/1702.01243},
	year = {2016}
}
@inproceedings{ChengchengNing2017,
	author = {{Chengcheng Ning} and {Huajun Zhou} and {Yan Song} and {Jinhui Tang}},
	booktitle = {2017 IEEE International Conference on Multimedia {\&} Expo Workshops (ICMEW)},
	doi = {10.1109/ICMEW.2017.8026312},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
	isbn = {978-1-5386-0560-8},
	mendeley-groups = {Detection,Detection/singleshots},
	month = {jul},
	pages = {549--554},
	publisher = {IEEE},
	title = {{Inception Single Shot MultiBox Detector for object detection}},
	url = {http://ieeexplore.ieee.org/document/8026312/},
	year = {2017}
}
@article{Cao2016,
	abstract = {—Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. Additionally, we propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several datasets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.01706v1},
	author = {Cao, Yuanzhouhan and Shen, Chunhua and Shen, Heng Tao},
	eprint = {arXiv:1610.01706v1},
	file = {:home/phil/Documents/research/papers/2016/Cao, Shen, Shen - 2016 - Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation.pdf:pdf},
	journal = {APPEARING IN IEEE TRANSACTIONS ON IMAGE PROCESSING},
	keywords = {Index Terms—Object detection,deep convolutional networks,depth estimation,semantic segmentation},
	mendeley-groups = {Detection},
	number = {1},
	title = {{Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation}},
	url = {https://arxiv.org/pdf/1610.01706.pdf},
	year = {2016}
}
@article{Crivellaro1109,
	abstract = {—We present an algorithm for estimating the pose of a rigid object in real-time under challenging conditions. Our method effectively handles poorly textured objects in cluttered, changing environments, even when their appearance is corrupted by large occlusions, and it relies on grayscale images to handle metallic environments on which depth cameras would fail. As a result, our method is suitable for practical Augmented Reality applications including industrial environments. At the core of our approach is a novel representation for the 3D pose of object parts: We predict the 3D pose of each part in the form of the 2D projections of a few control points. The advantages of this representation is three-fold: We can predict the 3D pose of the object even when only one part is visible; when several parts are visible, we can easily combine them to compute a better pose of the object; the 3D pose we obtain is usually very accurate, even when only few parts are visible. We show how to use this representation in a robust 3D tracking framework. In addition to extensive comparisons with the state-of-the-art, we demonstrate our method on a practical Augmented Reality application for maintenance assistance in the ATLAS particle detector at CERN.},
	author = {Crivellaro, Alberto and Rad, Mahdi and Verdie, Yannick and {Moo Yi}, Kwang and Fua, Pascal and Lepetit, Vincent},
	doi = {10.1109/TPAMI.2017.2708711},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crivellaro et al. - 1109 - IEEE Transactions on Pattern Analysis and Machine Intelligence Robust 3D Object Tracking from Monocular Image.pdf:pdf},
	keywords = {3D Tracking !,Index Terms—3D Detection},
	mendeley-groups = {Detection},
	title = {{IEEE Transactions on Pattern Analysis and Machine Intelligence Robust 3D Object Tracking from Monocular Images using Stable Parts}},
	url = {http://www.ieee.org/publications{\_}standards/publications/rights/index.html},
	year = {1109}
}
@article{Ren,
	abstract = {—Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them " Networks on Convolutional feature maps " (NoCs). We discover that aside from deep feature maps, a deep and convolutional per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly lead to good detection accuracy without using such a per-region classifier. We show by experiments that despite the effective ResNets and Faster R-CNN systems, the design of NoCs is an essential element for the 1st-place winning entries in ImageNet and MS COCO challenges 2015.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
	file = {:home/phil/Documents/research/papers/Unknown/Ren et al. - Unknown - Object Detection Networks on Convolutional Feature Maps.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Object Detection Networks on Convolutional Feature Maps}},
	url = {https://arxiv.org/pdf/1504.06066.pdf}
}
@article{Chen,
	abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and pre-dicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view fe-ature fusion. The proposal network generates 3D candi-date boxes efficiently from the bird's eye view representa-tion of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25{\%} and 30{\%} AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our appro-ach obtains 14.9{\%} higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
	author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
	file = {:home/phil/Documents/research/papers/Unknown/Chen et al. - Unknown - Multi-View 3D Object Detection Network for Autonomous Driving.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Multi-View 3D Object Detection Network for Autonomous Driving}},
	url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Chen{\_}Multi-View{\_}3D{\_}Object{\_}CVPR{\_}2017{\_}paper.pdf}
}
@article{Lina,
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid rep-resentations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to con-struct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extrac-tor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model en-tries including those from the COCO 2016 challenge win-ners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	author = {Lin, Tsung-Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	file = {:home/phil/Documents/research/papers/Unknown/Lin et al. - Unknown - Feature Pyramid Networks for Object Detection.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Feature Pyramid Networks for Object Detection}},
	url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Lin{\_}Feature{\_}Pyramid{\_}Networks{\_}CVPR{\_}2017{\_}paper.pdf}
}
@article{OndUska,
	abstract = {This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or sys-tem identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an un-supervised manner, only based on raw, occluded sen-sor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset de-signed to mimic the task of tracking objects in 2D laser data – as commonly encountered in robotics applica-tions – and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.},
	author = {{Ond U{\v{s}}ka}, Peter and Posner, Ingmar},
	file = {:home/phil/Documents/research/papers/Unknown/Ond U{\v{s}}ka, Posner - Unknown - Deep Tracking Seeing Beyond Seeing Using Recurrent Neural Networks.pdf:pdf},
	mendeley-groups = {Tracking},
	title = {{Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks}},
	url = {https://arxiv.org/pdf/1602.00991.pdf}
}
@article{Milan,
	abstract = {We present a novel approach to online multi-target tracking based on recurrent neural networks (RNNs). Tracking mul-tiple objects in real-world scenes involves many challenges, including a) an a-priori unknown and time-varying number of targets, b) a continuous state estimation of all present targets, and c) a discrete combinatorial problem of data association. Most previous methods involve complex models that require tedious tuning of parameters. Here, we propose for the first time, an end-to-end learning approach for online multi-target tracking. Existing deep learning methods are not designed for the above challenges and cannot be trivially applied to the task. Our solution addresses all of the above points in a prin-cipled way. Experiments on both synthetic and real data show promising results obtained at ≈300 Hz on a standard CPU, and pave the way towards future research in this direction.},
	author = {Milan, Anton and {Hamid Rezatofighi}, S and Dick, Anthony and Reid, Ian and Schindler, Konrad},
	file = {:home/phil/Documents/research/papers/Unknown/Milan et al. - Unknown - Online Multi-Target Tracking Using Recurrent Neural Networks.pdf:pdf},
	mendeley-groups = {Tracking},
	title = {{Online Multi-Target Tracking Using Recurrent Neural Networks}},
	url = {http://www.milanton.de/files/aaai2017/aaai2017-anton-rnntracking.pdf}
}
@article{Fang,
	abstract = {As deep neural networks revolutionize many fundamental computer vision prob-lems, there have not been many works using neural networks to track objects. In this project, we design and implement a tracking pipeline using convolutional neural networks and recurrent neural networks. Our model can handle detection and tracking jointly using appearance and motion features. We use MOT data challenge as a highly occluded single object tracking dataset. We demonstrate good qualitative and quantitative results of our model and discuss how to extend the pipeline to multi-object tracking.},
	author = {Fang, Kuan},
	file = {:home/phil/Documents/research/papers/Unknown/Fang - Unknown - Track-RNN Joint Detection and Tracking Using Recurrent Neural Networks.pdf:pdf},
	mendeley-groups = {Thesis,Tracking},
	title = {{Track-RNN: Joint Detection and Tracking Using Recurrent Neural Networks}},
	url = {https://web.stanford.edu/class/cs231a/prev{\_}projects{\_}2016/final{\_}report (7).pdf}
}
@article{Ye,
	abstract = {Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource-limited scenarios. A widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the compu-tations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a compu-tational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to repro-duce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.},
	author = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
	file = {:home/phil/Documents/research/papers/Unknown/Ye et al. - Unknown - RETHINKING THE SMALLER-NORM-LESS- INFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS.pdf:pdf},
	mendeley-groups = {Thesis},
	title = {{RETHINKING THE SMALLER-NORM-LESS- INFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS}},
	url = {https://arxiv.org/pdf/1802.00124.pdf}
}
@article{Shimobaba2018,
	abstract = {—Digital holography enables us to reconstruct objects in three-dimensional space from holograms captured by an imaging device. For the reconstruction, we need to know the depth position of the recoded object in advance. In this study, we propose depth prediction using convolutional neural network (CNN)-based regression. In the previous researches, the depth of an object was estimated through reconstructed images at different depth positions from a hologram using a certain metric that indicates the most focused depth position; however, such a depth search is time-consuming. The CNN of the proposed method can directly predict the depth position with millimeter precision from holograms.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.00664v1},
	author = {Shimobaba, Tomoyoshi and Kakue, Takashi and Ito, Tomoyoshi},
	eprint = {arXiv:1802.00664v1},
	file = {:home/phil/Documents/research/papers/2018/Shimobaba, Kakue, Ito - 2018 - Convolutional neural network-based regression for depth prediction in digital holography.pdf:pdf},
	keywords = {Index Terms—digital holography,convolutional neural net-work,depth prediction,multiple regression},
	mendeley-groups = {Thesis},
	title = {{Convolutional neural network-based regression for depth prediction in digital holography}},
	url = {https://arxiv.org/pdf/1802.00664.pdf},
	year = {2018}
}
@article{Zhang,
	abstract = {This paper reviews recent studies in emerging di-rections of understanding neural-network represen-tations and learning neural networks with inter-pretable/disentangled middle-layer representations. Although deep neural networks have exhibited su-perior performance in various tasks, the inter-pretability is always an Achilles' heel of deep neu-ral networks. At present, deep neural networks obtain a high discrimination power at the cost of low interpretability of their black-box representa-tions. We believe that the high model interpretabil-ity may help people to break several bottlenecks of deep learning, e.g. learning from very few an-notations, learning via human-computer communi-cations at the semantic level, and semantically de-bugging network representations. In this paper, we focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN repre-sentations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Fi-nally, we discuss prospective trends of explainable artificial intelligence.},
	author = {Zhang, Quanshi and Zhu, Song-Chun},
	file = {:home/phil/Documents/research/papers/Unknown/Zhang, Zhu - Unknown - Visual Interpretability for Deep Learning a Survey.pdf:pdf},
	mendeley-groups = {Other,Model Interpretation},
	title = {{Visual Interpretability for Deep Learning: a Survey}},
	url = {https://arxiv.org/pdf/1802.00614.pdf}
}
@article{,
	doi = {10.1080/10095020.2017.1420509},
	file = {:home/phil/Documents/research/papers/Unknown/Unknown - Unknown - Geo-spatial Information Science.pdf:pdf},
	issn = {1009-5020},
	mendeley-groups = {Thesis/Drone Navigation},
	title = {{Geo-spatial Information Science}},
	url = {http://www.tandfonline.com/action/journalInformation?journalCode=tgsi20}
}
@article{Falanga,
	abstract = {— We address one of the main challenges towards autonomous quadrotor flight in complex environments, which is flight through narrow gaps. While previous works relied on off-board localization systems or on accurate prior knowledge of the gap position and orientation in the world reference frame, we rely solely on onboard sensing and computing and estimate the full state by fusing gap detection from a single onboard camera with an IMU. This problem is challenging for two reasons: (i) the quadrotor pose uncertainty with respect to the gap increases quadratically with the distance from the gap; (ii) the quadrotor has to actively control its orientation towards the gap to enable state estimation (i.e., active vision). We solve this problem by generating a trajectory that considers geometric, dynamic, and perception constraints: during the approach maneuver, the quadrotor always faces the gap to allow state estimation, while respecting the vehicle dynamics; during the traverse through the gap, the distance of the quadrotor to the edges of the gap is maximized. Furthermore, we replan the trajectory during its execution to cope with the varying uncertainty of the state estimate. We successfully evaluate and demonstrate the proposed approach in many real experiments, achieving a success rate of 80{\%} and gap orientations up to 45 • . To the best of our knowledge, this is the first work that addresses and achieves autonomous, aggressive flight through narrow gaps using only onboard sensing and computing and without prior knowledge of the pose of the gap.},
	author = {Falanga, Davide and Mueggler, Elias and Faessler, Matthias and Scaramuzza, Davide},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Falanga et al. - Unknown - Aggressive Quadrotor Flight through Narrow Gaps with Onboard Sensing and Computing using Active Vision.pdf:pdf},
	mendeley-groups = {Thesis,Thesis/Drone Navigation},
	title = {{Aggressive Quadrotor Flight through Narrow Gaps with Onboard Sensing and Computing using Active Vision}},
	url = {http://rpg.ifi.uzh.ch/aggressive{\_}flight.html}
}
@article{DeBrabandere,
	abstract = {Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a rep-resentation of the image that can easily be clustered into instances with a simple post-processing step. The loss func-tion encourages the network to map each pixel to a point in feature space so that pixels belonging to the same in-stance lie close together while different instances are sepa-rated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and dis-tinct from recent efforts in instance segmentation. In con-trast to previous works, our method does not rely on ob-ject proposals or recurrent mechanisms. A key contribu-tion of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on-par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popu-lar detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmenta-tion benchmarks.},
	author = {{De Brabandere}, Bert and Neven, Davy},
	file = {:home/phil/Documents/research/papers/Unknown/De Brabandere, Neven - Unknown - Semantic Instance Segmentation with a Discriminative Loss Function.pdf:pdf},
	mendeley-groups = {Thesis},
	title = {{Semantic Instance Segmentation with a Discriminative Loss Function}},
	url = {https://arxiv.org/pdf/1708.02551.pdf}
}
@article{Chena,
	abstract = {In this work, we tackle the problem of instance segmen-tation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction predic-tion. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining seman-tic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different se-mantic classes including background, while the direction prediction, estimating each pixel's direction towards its cor-responding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incor-porating recent successful methods from both segmentation and detection (e.g., atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance seg-mentation benchmark and shows comparable performance with other state-of-art models.},
	author = {Chen, Liang-Chieh and Hermans, Alexander and Papandreou, George and Schroff, Florian and Wang, Peng and Adam, Hartwig},
	file = {:home/phil/Documents/research/papers/Unknown/Chen et al. - Unknown - MaskLab Instance Segmentation by Refining Object Detection with Semantic and Direction Features.pdf:pdf},
	mendeley-groups = {Thesis},
	title = {{MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features}},
	url = {https://arxiv.org/pdf/1712.04837.pdf}
}
@article{Long,
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build " fully convolutional " networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [5] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative im-provement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	file = {:home/phil/Documents/research/papers/Unknown/Long, Shelhamer, Darrell - Unknown - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
	mendeley-groups = {Thesis},
	title = {{Fully Convolutional Networks for Semantic Segmentation}},
	url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf}
}
@article{Abolafia2018,
	abstract = {We consider the task of program synthesis in the presence of a reward function over the output of programs, where the goal is to find programs with maximal rewards. We employ an iterative optimization scheme, where we train an RNN on a dataset of K best programs from a priority queue of the generated programs so far. Then, we synthesize new programs and add them to the priority queue by sampling from the RNN. We benchmark our algorithm, called priority queue train-ing (or PQT), against genetic algorithm and reinforcement learning baselines on a simple but expressive Turing complete programming language called BF. Our ex-perimental results show that our simple PQT algorithm significantly outperforms the baselines. By adding a program length penalty to the reward function, we are able to synthesize short, human readable programs.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1801.03526v1},
	author = {Abolafia, Daniel A and Norouzi, Mohammad and Le, Quoc V and Brain, Google},
	eprint = {arXiv:1801.03526v1},
	file = {:home/phil/Documents/research/papers/2018/Abolafia et al. - 2018 - NEURAL PROGRAM SYNTHESIS WITH PRIORITY QUEUE TRAINING.pdf:pdf},
	mendeley-groups = {Other},
	title = {{NEURAL PROGRAM SYNTHESIS WITH PRIORITY QUEUE TRAINING}},
	url = {https://arxiv.org/pdf/1801.03526.pdf},
	year = {2018}
}
@article{Zhang,
	abstract = {Which image is more similar to the reference image? Reference Patch 1 Patch 0 Reference Patch 1 Patch 0 Reference Patch 1 Patch 0 L2/PSNR, SSIM, FSIM Random Networks Unsupervised Networks Self-Supervised Networks Supervised Networks Humans Figure 1: Which patch (left or right) is " closer " to the middle patch in these examples? In each case, the tradi-tional metrics (L2/PSNR, SSIM, FSIM) disagree with human judgments. But deep networks, even across architectures (Squeezenet [18], AlexNet [25], VGG [48]) and supervision type (supervised [44], self-supervised [12, 37, 40, 59], and even unsupervised [24]), provide an emergent embedding which agrees surprisingly well with humans. We further cal-ibrate existing deep embeddings on a large-scale database of perceptual judgments; models and data can be found at https://www.github.com/richzhang/PerceptualSimilarity. Abstract While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the under-lying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called " percep-tual losses " ? What elements are critical for their success? To answer these questions, we introduce a new Full Refer-ence Image Quality Assessment (FR-IQA) dataset of per-ceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG fea-tures, but holds across different deep architectures and lev-els of supervision (supervised, self-supervised, or even un-supervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual represen-tations.},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver and Research, Adobe},
	file = {:home/phil/Documents/research/papers/Unknown/Zhang et al. - Unknown - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:pdf},
	mendeley-groups = {Other},
	title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
	url = {https://arxiv.org/pdf/1801.03924.pdf}
}
@article{Li,
	abstract = {Deploying deep neural networks on mobile devices is a chal-lenging task. Current model compression methods such as matrix decomposition effectively reduce the deployed model size, but still cannot satisfy real-time processing requirement. This paper first discovers that the major obstacle is the ex-cessive execution time of non-tensor layers such as pooling and normalization without tensor-like trainable parameters. This motivates us to design a novel acceleration framework: DeepRebirth through " slimming " existing consecutive and parallel non-tensor and tensor layers. The layer slimming is executed at different substructures: (a) streamline slimming by merging the consecutive non-tensor and tensor layer ver-tically; (b) branch slimming by merging non-tensor and ten-sor branches horizontally. The proposed optimization oper-ations significantly accelerate the model execution and also greatly reduce the run-time memory cost since the slimmed model architecture contains less hidden layers. To maximally avoid accuracy loss, the parameters in new generated layers are learned with layer-wise fine-tuning based on both theoret-ical analysis and empirical verification. As observed in the ex-periment, DeepRebirth achieves more than 3x speed-up and 2.5x run-time memory saving on GoogLeNet with only 0.4{\%} drop of top-5 accuracy on ImageNet. Furthermore, by com-bining with other model compression techniques, DeepRe-birth offers an average of 65ms inference time on the CPU of Samsung Galaxy S6 with 86.5{\%} top-5 accuracy, 14{\%} faster than SqueezeNet which only has a top-5 accuracy of 80.5{\%}.},
	author = {Li, Dawei and Wang, Xiaolong and Kong, Deguang},
	file = {:home/phil/Documents/research/papers/Unknown/Li, Wang, Kong - Unknown - DeepRebirth Accelerating Deep Neural Network Execution on Mobile Devices.pdf:pdf},
	mendeley-groups = {Other},
	title = {{DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices}},
	url = {https://arxiv.org/pdf/1708.04728.pdf}
}
@article{Fong,
	abstract = {In an effort to understand the meaning of the intermedi-ate representations captured by deep networks, recent pa-pers have tried to associate specific semantic concepts to individual neural network filter responses, where interest-ing correlations are often found, largely by focusing on ex-tremal filter responses. In this paper, we show that this ap-proach can favor easy-to-interpret cases that are not neces-sarily representative of the average behavior of a represen-tation. A more realistic but harder-to-study hypothesis is that se-mantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec frame-work, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a con-cept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better char-acterize the meaning of a representation and its relationship to other concepts.},
	author = {Fong, Ruth and Vedaldi, Andrea},
	file = {:home/phil/Documents/research/papers/Unknown/Fong, Vedaldi - Unknown - Net2Vec Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks.pdf:pdf},
	mendeley-groups = {Other},
	title = {{Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks}},
	url = {https://arxiv.org/pdf/1801.03454.pdf}
}
@article{Bentzen2018,
	abstract = {We present a formalization and computational implemen-tation of the second formulation of Kant's categorical im-perative. This ethical principle requires an agent to never treat someone merely as a means but always also as an end. Here we interpret this principle in terms of how persons are causally affected by actions. We introduce Kantian causal agency models in which moral patients, actions, goals, and causal influence are represented, and we show how to formal-ize several readings of Kant's categorical imperative that cor-respond to Kant's concept of strict and wide duties towards oneself and others. Stricter versions handle cases where an action directly causally affects oneself or others, whereas the wide version maximizes the number of persons being treated as an end. We discuss limitations of our formalization by pointing to one of Kant's cases that the machinery cannot handle in a satisfying way.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1801.03160v1},
	author = {Bentzen, Martin Mose and Lindner, Felix},
	eprint = {arXiv:1801.03160v1},
	file = {:home/phil/Documents/research/papers/2018/Bentzen, Lindner - 2018 - A Formalization of Kant's Second Formulation of the Categorical Imperative.pdf:pdf},
	mendeley-groups = {Other},
	title = {{A Formalization of Kant's Second Formulation of the Categorical Imperative}},
	url = {https://arxiv.org/pdf/1801.03160.pdf},
	year = {2018}
}
@article{Tang,
	abstract = {—Deep CNN-based object detection systems have achieved remarkable success on several large-scale object detection benchmarks. However, training such detectors requires a large number of labeled bounding boxes, which are more difficult to obtain than image-level annotations. Previous work addresses this issue by transforming image-level classifiers into object detectors. This is done by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations. We improve this previous work by incorporating knowledge about object similarities from visual and semantic domains during the transfer process. The intuition behind our proposed method is that visually and semantically similar categories should exhibit more common transferable properties than dissimilar categories, e.g. a better detector would result by transforming the differences between a dog classifier and a dog detector onto the cat class, than would by transforming from the violin class. Experimental results on the challenging ILSVRC2013 detection dataset demonstrate that each of our proposed object similarity based knowledge transfer methods outperforms the baseline methods. We found strong evidence that visual similarity and semantic relatedness are complementary for the task, and when combined notably improve detection, achieving state-of-the-art detection performance in a semi-supervised setting.},
	author = {Tang, Yuxing and Wang, Josiah and Wang, Xiaofang and Gao, Boyang and Delland, Emmanuel and Gaizauskas, Robert and Chen, Liming},
	file = {:home/phil/Documents/research/papers/Unknown/Tang et al. - Unknown - Large Scale Semi-supervised Object Detection Using Visual and Semantic Knowledge Transfer.pdf:pdf},
	keywords = {Index Terms—Object detection,convolutional neural networks,semantic similarity,semi-supervised learning,transfer learning,visual similarity},
	mendeley-groups = {Detection},
	title = {{Large Scale Semi-supervised Object Detection Using Visual and Semantic Knowledge Transfer}},
	url = {https://arxiv.org/pdf/1801.03145.pdf}
}
@article{Jha,
	abstract = {In this paper, we describe the mechanical design, system overview, integration and control techniques associated with SKALA, a unique large-sized robot for carrying a person with physical disabilities, up and down staircases. As a reg-ular wheelchair is unable to perform such a maneuver, the system functions as a non-conventional wheelchair with sev-eral intelligent features. We describe the unique mechanical design and the design choices associated with it. We show-case the embedded control architecture that allows for sev-eral different modes of teleoperation, all of which have been described in detail. We further investigate the architecture associated with the autonomous operation of the system.},
	author = {Jha, Siddharth and Chaudhary, Himanshu and Kharagpur, Iit and Satardey, Swapnil and Kumar, Piyush and Roy, Ankush and Deshmukh, Aditya},
	file = {:home/phil/Documents/research/papers/Unknown/Jha et al. - Unknown - Design, Analysis {\&}amp Prototyping of a Semi-Automated Staircase-Climbing Rehabilitation Robot.pdf:pdf},
	keywords = {Caterpillar Drive,Embedded systems,Keywords Mechatronics,Rehabilitation Robotics,Sen-sors and actuators,Stair-Climbing Wheelchair},
	mendeley-groups = {Robotics},
	title = {{Design, Analysis {\&} Prototyping of a Semi-Automated Staircase-Climbing Rehabilitation Robot}},
	url = {https://arxiv.org/pdf/1801.03425.pdf}
}
@article{Li2017,
	abstract = {Recently, learning equivariant representations has attracted considerable research attention. Dieleman et al. introduce four operations which can be inserted to CNN to learn deep representations equivariant to rotation. However, feature maps should be copied and rotated four times in each layer in their approach, which causes much running time and memory overhead. In order to address this problem, we propose Deep Rotation Equivariant Network(DREN) consisting of cycle layers, isotonic layers and decycle layers.Our proposed layers apply rotation transformation on filters rather than feature maps, achieving a speed up of more than 2 times with even less memory overhead. We evaluate DRENs on Rotated MNIST and CIFAR-10 datasets and demonstrate that it can improve the performance of state-of-the-art architectures. Our codes are released on GitHub.},
	archivePrefix = {arXiv},
	arxivId = {1705.08623},
	author = {Li, Junying and Yang, Zichen and Liu, Haifeng and Cai, Deng},
	eprint = {1705.08623},
	file = {:home/phil/Documents/research/papers/2017/Li et al. - 2017 - Deep Rotation Equivariant Network.pdf:pdf},
	mendeley-groups = {Architecture},
	month = {may},
	title = {{Deep Rotation Equivariant Network}},
	url = {http://arxiv.org/abs/1705.08623},
	year = {2017}
}
@article{Rena,
	abstract = {—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with " attention " mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	file = {:home/phil/Documents/research/papers/Unknown/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks(2).pdf:pdf},
	keywords = {Convolutional Neural Network,Index Terms—Object Detection,Region Proposal},
	mendeley-groups = {Detection},
	title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
	url = {https://arxiv.org/pdf/1506.01497.pdf}
}
@article{Mohta2017,
	abstract = {One of the most challenging tasks for a flying robot is to autonomously navigate between target locations quickly and reliably while avoiding obstacles in its path, and with little to no a-priori knowledge of the operating environment. This challenge is addressed in the present paper. We describe the system design and software architecture of our proposed solution, and showcase how all the distinct components can be integrated to enable smooth robot operation. We provide critical insight on hardware and software component selection and development, and present results from extensive experimental testing in real-world warehouse environments. Experimental testing reveals that our proposed solution can deliver fast and robust aerial robot autonomous navigation in cluttered, GPS-denied environments.},
	archivePrefix = {arXiv},
	arxivId = {1712.02052},
	author = {Mohta, Kartik and Watterson, Michael and Mulgaonkar, Yash and Liu, Sikang and Qu, Chao and Makineni, Anurag and Saulnier, Kelsey and Sun, Ke and Zhu, Alex and Delmerico, Jeffrey and Karydis, Konstantinos and Atanasov, Nikolay and Loianno, Giuseppe and Scaramuzza, Davide and Daniilidis, Kostas and Taylor, Camillo Jose and Kumar, Vijay},
	eprint = {1712.02052},
	file = {:home/phil/Documents/research/papers/2017/Mohta et al. - 2017 - Fast, Autonomous Flight in GPS-Denied and Cluttered Environments.pdf:pdf},
	mendeley-groups = {Robotics},
	month = {dec},
	title = {{Fast, Autonomous Flight in GPS-Denied and Cluttered Environments}},
	url = {http://arxiv.org/abs/1712.02052},
	year = {2017}
}
@article{Elhoseiny,
	abstract = {In the Object Recognition task, there exists a di-chotomy between the categorization of objects and estimating object pose, where the former ne-cessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different cate-gories of objects. With the rise of deep archi-tectures, the prime focus has been on object cat-egory recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose estimation using these approaches has received relatively less attention. In this work, we study how Convolutional Neural Net-works (CNN) architectures can be adapted to the task of simultaneous object recognition and pose estimation. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representa-tions. We extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art.},
	author = {Elhoseiny, Mohamed and El-Gaaly, Tarek and Bakry, Amr and Elgammal, Ahmed},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elhoseiny et al. - Unknown - A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimatio.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimation}},
	url = {http://proceedings.mlr.press/v48/elhoseiny16.pdf}
}
@article{Brachmann,
	abstract = {In recent years, the task of estimating the 6D pose of object instances and complete scenes, i.e. camera localiza-tion, from a single input image has received considerable attention. Consumer RGB-D cameras have made this fea-sible, even for difficult, texture-less objects and scenes. In this work, we show that a single RGB image is sufficient to achieve visually convincing results. Our key concept is to model and exploit the uncertainty of the system at all stages of the processing pipeline. The uncertainty comes in the form of continuous distributions over 3D object coor-dinates and discrete distributions over object labels. We give three technical contributions. Firstly, we develop a regularized, auto-context regression framework which iter-atively reduces uncertainty in object coordinate and object label predictions. Secondly, we introduce an efficient way to marginalize object coordinate distributions over depth. This is necessary to deal with missing depth information. Thirdly, we utilize the distributions over object labels to de-tect multiple objects simultaneously with a fixed budget of RANSAC hypotheses. We tested our system for object pose estimation and camera localization on commonly used data sets. We see a major improvement over competing systems.},
	author = {Brachmann, Eric and Michel, Frank and Krull, Alexander and Yang, Michael Ying and Gumhold, Stefan and Rother, Carsten},
	file = {:home/phil/Documents/research/papers/Unknown/Brachmann et al. - Unknown - Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Brachmann{\_}Uncertainty-Driven{\_}6D{\_}Pose{\_}CVPR{\_}2016{\_}paper.pdf}
}
@article{Savarese,
	abstract = {We propose a novel and robust model to represent and learn generic 3D object categories. We aim to solve the problem of true 3D object categorization for handling arbi-trary rotations and scale changes. Our approach is to cap-ture a compact model of an object category by linking to-gether diagnostic parts of the objects from different viewing points. We emphasize on the fact that our " parts " are large and discriminative regions of the objects that are composed of many local invariant features. Instead of recovering a full 3D geometry, we connect these parts through their mu-tual homographic transformation. The resulting model is a compact summarization of both the appearance and geom-etry information of the object class. We propose a frame-work in which learning is done via minimal supervision compared to previous works. Our results on categorization show superior performances to state-of-the-art algorithms such as [23]. Furthermore, we have compiled a new 3D ob-ject dataset that consists of 10 different object categories. We have tested our algorithm on this dataset and have ob-tained highly promising results.},
	author = {Savarese, Silvio and Fei-Fei, Li},
	file = {:home/phil/Documents/research/papers/Unknown/Savarese, Fei-Fei - Unknown - 3D generic object categorization, localization and pose estimation.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{3D generic object categorization, localization and pose estimation}},
	url = {http://vision.stanford.edu/documents/SavareseFei-Fei{\_}ICCV2007.pdf}
}
@article{Peng,
	abstract = {Crowdsourced 3D CAD models are becoming easily ac-cessible online, and can potentially generate an infinite number of training images for almost any object category. We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real train-ing data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD-rendered images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of in-variance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previ-ous methods on the PASCAL VOC2007 dataset when learn-ing in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.},
	author = {Peng, Xingchao and Sun, Baochen and Ali, Karim and Saenko, Kate},
	file = {:home/phil/Documents/research/papers/Unknown/Peng et al. - Unknown - Learning Deep Object Detectors from 3D Models.pdf:pdf},
	mendeley-groups = {Training},
	title = {{Learning Deep Object Detectors from 3D Models}},
	url = {http://www.karimali.org/publications/PSAS{\_}ICCV15.pdf}
}
@article{Mueller,
	abstract = {We present an approach for real-time, robust and accu-rate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing meth-ods typically fail for hand-object interactions in cluttered scenes imaged from egocentric viewpoints—common for virtual or augmented reality applications. Our approach uses two subsequently applied Convolutional Neural Net-works (CNNs) to localize the hand and regress 3D joint locations. Hand localization is achieved by using a CNN to estimate the 2D position of the hand center in the input, even in the presence of clutter and occlusions. The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time. For added accuracy, robustness and temporal stability, we refine the pose estimates using a kine-matic pose tracking energy. To train the CNNs, we intro-duce a new photorealistic dataset that uses a merged reality approach to capture and synthesize large amounts of anno-tated data of natural hand interaction in cluttered scenes. Through quantitative and qualitative evaluation, we show that our method is robust to self-occlusion and occlusions by objects, particularly in moving egocentric perspectives.},
	author = {Mueller, Franziska and Sridhar, Srinath and Mehta, Dushyant and Casas, Dan and Sotnychenko, Oleksandr and Theobalt, Christian},
	file = {:home/phil/Documents/research/papers/Unknown/Mueller et al. - Unknown - Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor.pdf:pdf},
	mendeley-groups = {Tracking},
	title = {{Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor}},
	url = {https://arxiv.org/pdf/1704.02201.pdf}
}
@article{Lepetit2005,
	abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Aug-mented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated inter-faces can use. Computer Vision offers solutions that are cheap, practical and non-invasive. This survey reviews the different techniques and approaches that have been developed by industry and research. First, important math-ematical tools are introduced: Camera representation, robust estima-tion and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery con-cludes with the different possible choices that should be made when},
	author = {Lepetit, Vincent and Fua, Pascal},
	file = {:home/phil/Documents/research/papers/2005/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects A Survey.pdf:pdf},
	journal = {Computer Graphics and Vision},
	mendeley-groups = {Tracking},
	number = {1},
	pages = {1--89},
	title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.8962{\&}rep=rep1{\&}type=pdf},
	volume = {1},
	year = {2005}
}
@article{Rad,
	abstract = {We introduce a novel method for 3D object detection and pose estimation from color images only. We first use seg-mentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a " holistic " approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes, as proposed in [3] for the pose of objects' parts. This, however, is not sufficient for han-dling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this prob-lem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional ad-ditional step that refines the predicted poses as in [17] for hand pose estimation. We improve the state-of-the-art on the LINEMOD dataset from 73.7{\%} [2] to 89.3{\%} of correctly registered RGB frames. We are also the first to report re-sults on the Occlusion dataset [1] using color images only. We obtain 54{\%} of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, com-pared to the 67{\%} of the state-of-the-art [10] on the same sequences which uses both color and depth. The full ap-proach is also scalable, as a single network can be trained for multiple objects simultaneously.},
	author = {Rad, Mahdi and Lepetit, Vincent},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rad, Lepetit - Unknown - BB8 A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth}},
	url = {https://arxiv.org/pdf/1703.10896.pdf}
}
@article{Hodan,
	abstract = {We introduce T-LESS, a new public dataset for estimat-ing the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit sym-metries and mutual similarities in shape and/or size. Com-pared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are pro-vided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training im-ages depict individual objects against a black background. Test images originate from twenty test scenes having vary-ing complexity, which increases from simple scenes with several isolated objects to very challenging ones with mul-tiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a sys-tematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with sig-nificant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.},
	author = {Hodaň, Tom{\'{a}}{\v{s}} and Haluza, Pavel and Obdr{\v{z}}{\'{a}}lek, St{\v{e}}p{\'{a}}n and Matas, Jiř{\'{i}} and Lourakis, Manolis and Zabulis, Xenophon},
	doi = {10.1109/WACV.2017.103},
	file = {:home/phil/Documents/research/papers/Unknown/Hodaň et al. - Unknown - T-LESS An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects}},
	url = {https://pdfs.semanticscholar.org/e2a8/4869c68e73f76c2d326cb2b97c9795562f0c.pdf}
}
@article{Doumanoglou,
	abstract = {In this paper we tackle the problem of estimating the 3D pose of object instances, using convolutional neural networks. State of the art methods usually solve the challenging problem of regression in angle space indirectly, focusing on learning discriminative features that are later fed into a separate architecture for 3D pose estimation. In contrast, we propose an end-to-end learning framework for directly regressing object poses by exploiting Siamese Networks. For a given image pair, we enforce a similarity measure between the representation of the sample images in the feature and pose space respectively, that is shown to boost regression performance. Furthermore, we argue that our pose-guided feature learning using our Siamese Regression Network generates more discriminative features that outperform the state of the art. Last, our feature learning formulation provides the ability of learning features that can perform under severe occlusions, demonstrating high performance on our novel hand-object dataset.},
	author = {Doumanoglou, Andreas and Balntas, Vassileios and Kouskouridas, Rigas and Kim, Tae-Kyun},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doumanoglou et al. - Unknown - Siamese Regression Networks with Efficient mid-level Feature Extraction for 3D Object Pose Estimation.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{Siamese Regression Networks with Efficient mid-level Feature Extraction for 3D Object Pose Estimation}},
	url = {https://arxiv.org/pdf/1607.02257.pdf}
}
@article{Turan2017,
	abstract = {We present a robust deep learning based 6 degrees-of-freedom (DoF) localization system for endoscopic capsule robots. Our system mainly focuses on localization of endoscopic capsule robots inside the GI tract using only visual information captured by a mono camera integrated to the robot. The proposed system is a 23-layer deep convolutional neural network (CNN) that is capable to esti-mate the pose of the robot in real time using a standard CPU. The dataset for the evaluation of the system was recorded inside a surgical human stomach model with realistic surface texture, softness, and surface liquid properties so that the pre-trained CNN architecture can be transferred confidently into a real endoscopic scenario. An average error of 7.1{\%} and 3.4{\%} for translation and rotation has been obtained, respectively. The results accomplished from the ex-periments demonstrate that a CNN pre-trained with raw 2D endoscopic images performs accurately inside the GI tract and is robust to various challenges posed by reflection distortions, lens imperfections, vignetting, noise, motion blur, low resolution, and lack of unique landmarks to track.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.05435v1},
	author = {Turan, Mehmet and Almalioglu, Yasin and Konukoglu, Ender and Sitti, Metin},
	eprint = {arXiv:1705.05435v1},
	file = {:home/phil/Documents/research/papers/2017/Turan et al. - 2017 - A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots.pdf:pdf},
	keywords = {CNN,Capsule endoscope robot,deep learning,localization},
	mendeley-groups = {Camera (Re)Localization},
	title = {{A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots}},
	url = {https://arxiv.org/pdf/1705.05435.pdf},
	year = {2017}
}
@article{Lepetit2005,
	abstract = {Monocular Model-Based 3D Tracking of Rigid Objects: A Survey},
	author = {Lepetit, Vincent and Fua, Pascal},
	doi = {10.1561/0600000001},
	file = {:home/phil/Documents/research/papers/2005/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects A Survey.pdf:pdf},
	issn = {1572-2740},
	journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
	keywords = {3D reconstruction and image-based modeling,Computer Graphics},
	mendeley-groups = {Tracking},
	number = {1},
	pages = {1--89},
	publisher = {Now Publishers, Inc.},
	title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
	url = {http://www.nowpublishers.com/article/Details/CGV-001},
	volume = {1},
	year = {2005}
}
@article{Chen,
	abstract = {—In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed " DeepLab " system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.pdf:pdf},
	keywords = {Atrous Convolution,Conditional Random Fields,Index Terms—Convolutional Neural Networks,Semantic Segmentation},
	mendeley-groups = {Segmentation},
	title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
	url = {https://arxiv.org/pdf/1606.00915.pdf}
}
@article{Badrinarayanan,
	abstract = {—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	file = {:home/phil/Documents/research/papers/Unknown/Badrinarayanan, Kendall, Cipolla - Unknown - SegNet A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.pdf:pdf},
	keywords = {Decoder,Encoder,Index Terms—Deep Convolutional Neural Networks,Indoor Scenes,Pooling,Road Scenes,Semantic Pixel-Wise Segmentation,Upsampling},
	mendeley-groups = {Segmentation},
	title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
	url = {http://mi.eng.cam.ac.uk/projects/segnet/.}
}
@article{Garon,
	abstract = {Fig. 1. From an input RGBD sequence (top), our method leverages a deep neural network to automatically track the 6-DOF pose of an object even under significant clutter and occlusion (bottom). We demonstrate, through extensive experiments on a novel dataset of real objects with known ground truth pose, that our approach outperforms the state of the art both in terms of accuracy and robustness to occlusions. Abstract—We present a temporal 6-DOF tracking method which leverages deep learning to achieve state-of-the-art performance on challenging datasets of real world capture. Our method is both more accurate and more robust to occlusions than the existing best performing approaches while maintaining real-time performance. To assess its efficacy, we evaluate our approach on several challenging RGBD sequences of real objects in a variety of conditions. Notably, we systematically evaluate robustness to occlusions through a series of sequences where the object to be tracked is increasingly occluded. Finally, our approach is purely data-driven and does not require any hand-designed features: robust tracking is automatically learned from data.},
	author = {Garon, Mathieu and Lalonde, Jean-Fran{\c{c}}ois},
	file = {:home/phil/Documents/research/papers/Unknown/Garon, Lalonde - Unknown - Deep 6-DOF Tracking.pdf:pdf},
	keywords = {Augmented Reality,Deep Learning,Index Terms—Tracking},
	mendeley-groups = {Tracking},
	title = {{Deep 6-DOF Tracking}},
	url = {https://arxiv.org/pdf/1703.09771.pdf}
}
@article{Kehl,
	abstract = {We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.},
	author = {Kehl, Wadim and Milletari, Fausto and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
	file = {:home/phil/Documents/research/papers/Unknown/Kehl et al. - Unknown - Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation}},
	url = {http://campar.in.tum.de/pub/kehl2016eccv/kehl2016eccv.pdf}
}
@article{Rastegari,
	abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32× mem-ory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily bi-nary operations. This results in 58× faster convolutional operations and 32× memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9{\%} less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16{\%} in top-1 accuracy.},
	author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	file = {:home/phil/Documents/research/papers/Unknown/Rastegari et al. - Unknown - XNOR-Net ImageNet Classification Using Binary Convolutional Neural Networks.pdf:pdf},
	keywords = {Binary Convolution,Binary Deep Learning,Binary Neural Networks,Convolutional Neural Network,Deep Learning},
	mendeley-groups = {Classification},
	title = {{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}},
	url = {https://pjreddie.com/media/files/papers/xnor.pdf}
}
@article{Redmon,
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	archivePrefix = {arXiv},
	arxivId = {1612.08242},
	author = {Szegedy, Christian and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew and Simon, Marcel and Rodner, Erik and Denzler, Joachim and Redmon, Joseph and Farhadi, Ali and Ioffe, Sergey and Szegedy, Christian and Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-yang and Berg, Alexander C and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew and Shlens, Jonathon and Wojna, Zbigniew and Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt and He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian and Chen, Tianqi and Guestrin, Carlos},
	doi = {10.1142/9789812771728_0012},
	eprint = {1612.08242},
	file = {:home/phil/Documents/research/papers/2016/Szegedy et al. - 2016 - YOLO9000 Better, Faster, Stronger.pdf:pdf},
	isbn = {1879-2057 (Electronic)$\backslash$n0001-4575 (Linking)},
	issn = {0146-4833},
	journal = {Data Mining with Decision Trees},
	keywords = {convolutional neural network,deep learning,denoising auto-encoder,image denoising,large-scale machine learning,real-time object detection},
	mendeley-groups = {Detection},
	number = {3},
	pages = {352350},
	pmid = {23021419},
	title = {{YOLO9000: Better, Faster, Stronger}},
	url = {https://arxiv.org/abs/1612.08242},
	volume = {7},
	year = {2016}
}
@article{Felzenszwalb,
	abstract = {This paper describes a discriminatively trained, multi-scale, deformable part model for object detection. Our sys-tem achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person de-tection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hid-den CRF, leads to a non-convex training problem. How-ever, a latent SVM is semi-convex and the training prob-lem becomes convex once latent information is specified for the positive examples. We believe that our training meth-ods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.},
	author = {Felzenszwalb, Pedro and Mcallester, David and Ramanan, Deva},
	file = {:home/phil/Documents/research/papers/Unknown/Felzenszwalb, Mcallester, Ramanan - Unknown - A Discriminatively Trained, Multiscale, Deformable Part Model.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{A Discriminatively Trained, Multiscale, Deformable Part Model}},
	url = {http://people.cs.uchicago.edu/{~}pff/papers/latent.pdf}
}
@article{Uijlings,
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmen-tation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possi-ble object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 {\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced num-ber of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:},
	author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
	doi = {10.1007/s11263-013-0620-5},
	file = {:home/phil/Documents/research/papers/Unknown/Uijlings et al. - Unknown - Selective Search for Object Recognition.pdf:pdf},
	journal = {Int J Comput Vis},
	mendeley-groups = {Detection},
	title = {{Selective Search for Object Recognition}},
	url = {http://disi.}
}
@article{,
	abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. We describe a version of capsules in which each capsule has a logis-tic unit to represent the presence of an entity and a 4x4 pose matrix which could learn to represent the relationship between that entity and the viewer. A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by viewpoint-invariant transformation matri-ces that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated using the EM algorithm such that the output of each capsule is routed to a cap-sule in the layer above that receives a cluster of similar votes. The whole system is trained discriminatively by unrolling 3 iterations of EM between each pair of adjacent layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45{\%} compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attack than our baseline convolutional neural network.},
	archivePrefix = {arXiv},
	arxivId = {1710.09829},
	eprint = {1710.09829},
	file = {:home/phil/Documents/research/papers/Unknown/Unknown - Unknown - Matrix Capsules With Em Routing.pdf:pdf},
	mendeley-groups = {Other},
	title = {{Matrix Capsules With Em Routing}},
	url = {https://openreview.net/pdf?id=HJWLfGWRb}
}
@article{Sermanet2014,
	abstract = {We present an integrated framework for using Convolutional Networks for classi-fication, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound-aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul-taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1312.6229v4},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and Lecun, Yann},
	eprint = {arXiv:1312.6229v4},
	file = {:home/phil/Documents/research/papers/2014/Sermanet et al. - 2014 - OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Detection},
	title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
	url = {https://arxiv.org/pdf/1312.6229.pdf},
	year = {2014}
}
@article{Liu,
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines pre-dictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into sys-tems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 in-put, SSD achieves 74.3{\%} mAP 1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a compa-rable state-of-the-art Faster R-CNN model. Compared to other single stage meth-ods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
	file = {:home/phil/Documents/research/papers/Unknown/Liu et al. - Unknown - SSD Single Shot MultiBox Detector.pdf:pdf},
	keywords = {Convolutional Neural Network,Real-time Object Detection},
	mendeley-groups = {Detection},
	title = {{SSD: Single Shot MultiBox Detector}},
	url = {https://www.cs.unc.edu/{~}wliu/papers/ssd.pdf}
}
@article{Wu,
	abstract = {Object detection is a crucial task for autonomous driv-ing. In addition to requiring high accuracy to ensure safety, object detection for autonomous driving also requires real-time inference speed to guarantee prompt vehicle control, as well as small model size and energy efficiency to enable embedded system deployment. In this work, we propose SqueezeDet, a fully convolu-tional neural network for object detection that aims to si-multaneously satisfy all of the above constraints. In our network we use convolutional layers not only to extract fea-ture maps, but also as the output layer to compute bound-ing boxes and class probabilities. The detection pipeline of our model only contains a single forward pass of a neu-ral network, thus it is extremely fast. Our model is fully-convolutional, which leads to small model size and bet-ter energy efficiency. Finally, our experiments show that our model is very accurate, achieving state-of-the-art ac-curacy on the KITTI [9] benchmark. The source code of SqueezeDet is open-source released 1 .},
	author = {Wu, Bichen and Iandola, Forrest and Jin, Peter H and Keutzer, Kurt},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - Unknown - SqueezeDet Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autono.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving}},
	url = {https://arxiv.org/pdf/1612.01051.pdf}
}
@inproceedings{Kendall,
	abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
	archivePrefix = {arXiv},
	arxivId = {1505.07427},
	author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
	doi = {10.1109/ICCV.2015.336},
	eprint = {1505.07427},
	file = {:home/phil/Documents/research/papers/2015/Kendall, Grimes, Cipolla - 2015 - PoseNet A convolutional network for real-time 6-dof camera relocalization.pdf:pdf},
	isbn = {9781467383912},
	issn = {15505499},
	keywords = {Deep,Monocular,Relocalization},
	mendeley-groups = {Camera (Re)Localization},
	mendeley-tags = {Deep,Monocular,Relocalization},
	month = {may},
	pages = {2938--2946},
	title = {{PoseNet: A convolutional network for real-time 6-dof camera relocalization}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Kendall{\_}PoseNet{\_}A{\_}Convolutional{\_}ICCV{\_}2015{\_}paper.pdf http://arxiv.org/abs/1505.07427},
	volume = {2015 Inter},
	year = {2015}
}
@article{Zitnick,
	abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We pro-pose a novel method for generating object bounding box proposals us-ing edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring propos-als. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
	author = {Zitnick, C Lawrence and Doll{\'{a}}r, Piotr},
	file = {:home/phil/Documents/research/papers/Unknown/Zitnick, Doll{\'{a}}r - Unknown - Edge Boxes Locating Object Proposals from Edges.pdf:pdf},
	keywords = {edge detection,object detection,object proposals},
	mendeley-groups = {Detection},
	title = {{Edge Boxes: Locating Object Proposals from Edges}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.453.5208{\&}rep=rep1{\&}type=pdf}
}
@article{Uijlings,
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmen-tation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possi-ble object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 {\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced num-ber of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:},
	author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
	doi = {10.1007/s11263-013-0620-5},
	file = {:home/phil/Documents/research/papers/Unknown/Uijlings et al. - Unknown - Selective Search for Object Recognition.pdf:pdf},
	journal = {Int J Comput Vis},
	mendeley-groups = {Detection},
	title = {{Selective Search for Object Recognition}},
	url = {http://disi.}
}
@article{Dai,
	abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
	archivePrefix = {arXiv},
	arxivId = {1703.06211},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1703.06211},
	file = {:home/phil/Documents/research/papers/2017/Dai et al. - 2017 - Deformable Convolutional Networks.pdf:pdf},
	isbn = {2004012439},
	issn = {0004-6361},
	mendeley-groups = {Architecture},
	pmid = {23459267},
	title = {{Deformable Convolutional Networks}},
	url = {http://arxiv.org/abs/1703.06211},
	year = {2017}
}
@article{Ouyang,
	abstract = {In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric con-straint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averag-ing. The proposed approach improves the mean averaged precision obtained by RCNN [14], which was the state-of-the-art, from 31{\%} to 50.3{\%} on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1{\%}. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline.},
	author = {Ouyang, Wanli and Wang, Xiaogang and Zeng, Xingyu and Qiu, Shi and Luo, Ping and Tian, Yonglong and Li, Hongsheng and Yang, Shuo and Wang, Zhe and Loy, Chen-Change and Tang, Xiaoou},
	file = {:home/phil/Documents/research/papers/Unknown/Ouyang et al. - Unknown - DeepID-Net Deformable Deep Convolutional Neural Networks for Object Detection.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Ouyang{\_}DeepID-Net{\_}Deformable{\_}Deep{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Girshick,
	abstract = {Deformable part models (DPMs) and convolutional neu-ral networks (CNNs) are two widely used tools for vi-sual recognition. They are typically viewed as distinct ap-proaches: DPMs are graphical models (Markov random fields), while CNNs are " black-box " non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a synthesis of the two ideas. Our con-struction involves unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. From this perspective, it is natural to replace the standard im-age features used in DPMs with a learned feature extractor. We call the resulting model a DeepPyramid DPM and ex-perimentally validate it on PASCAL VOC object detection. We find that DeepPyramid DPMs significantly outperform DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while run-ning significantly faster.},
	author = {Girshick, Ross and Iandola, Forrest and Darrell, Trevor and Malik, Jitendra},
	file = {:home/phil/Documents/research/papers/Unknown/Girshick et al. - Unknown - Deformable Part Models are Convolutional Neural Networks.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Deformable Part Models are Convolutional Neural Networks}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Girshick{\_}Deformable{\_}Part{\_}Models{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Liang,
	abstract = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many prop-erties with the visual system of the brain. A prominent dif-ference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abun-dant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the in-put is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the ac-tivities of its neighboring units. This property enhances the ability of the model to integrate the context informa-tion, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning pro-cess. The model is tested on four benchmark object recog-nition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increas-ing the number of parameters leads to even better perfor-mance. These results demonstrate the advantage of the re-current structure over purely feed-forward structure for ob-ject recognition.},
	author = {Liang, Ming and Hu, Xiaolin},
	file = {:home/phil/Documents/research/papers/Unknown/Liang, Hu - Unknown - Recurrent Convolutional Neural Network for Object Recognition.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Recurrent Convolutional Neural Network for Object Recognition}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Liang{\_}Recurrent{\_}Convolutional{\_}Neural{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Forsyth,
	author = {Forsyth, David},
	file = {:home/phil/Documents/research/papers/Unknown/Forsyth - Unknown - Object Detection with Discriminatively Trained Part-Based Models.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Object Detection with Discriminatively Trained Part-Based Models}},
	url = {https://pdfs.semanticscholar.org/ad5a/b85d8f8302e04ec40e139d364574083aa951.pdf}
}
@article{Erhan,
	abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
	archivePrefix = {arXiv},
	arxivId = {1312.2249},
	author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
	eprint = {1312.2249},
	file = {:home/phil/Documents/research/papers/2013/Erhan et al. - 2013 - Scalable Object Detection using Deep Neural Networks.pdf:pdf},
	mendeley-groups = {Detection},
	month = {dec},
	title = {{Scalable Object Detection using Deep Neural Networks}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2014/papers/Erhan{\_}Scalable{\_}Object{\_}Detection{\_}2014{\_}CVPR{\_}paper.pdf http://arxiv.org/abs/1312.2249},
	year = {2013}
}
@article{Caicedo,
	abstract = {We present an active detection model for localizing ob-jects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identi-fying the correct location of a target object. This agent learns to deform a bounding box using simple transforma-tion actions, with the goal of determining the most spe-cific location of target objects following top-down reason-ing. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object af-ter analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
	author = {Caicedo, Juan C and Lazebnik, Svetlana},
	file = {:home/phil/Documents/research/papers/Unknown/Caicedo, Lazebnik - Unknown - Active Object Localization with Deep Reinforcement Learning.pdf:pdf},
	mendeley-groups = {Other},
	title = {{Active Object Localization with Deep Reinforcement Learning}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Caicedo{\_}Active{\_}Object{\_}Localization{\_}ICCV{\_}2015{\_}paper.pdf}
}
@article{Wu2016,
	abstract = {Object detection is a crucial task for autonomous driving. In addition to requiring high accuracy to ensure safety, object detection for autonomous driving also requires real-time inference speed to guarantee prompt vehicle control, as well as small model size and energy efficiency to enable embedded system deployment. In this work, we propose SqueezeDet, a fully convolutional neural network for object detection that aims to simultaneously satisfy all of the above constraints. In our network we use convolutional layers not only to extract feature maps, but also as the output layer to compute bounding boxes and class probabilities. The detection pipeline of our model only contains a single forward pass of a neural network, thus it is extremely fast. Our model is fully-convolutional, which leads to small model size and better energy efficiency. Finally, our experiments show that our model is very accurate, achieving state-of-the-art accuracy on the KITTI benchmark.},
	archivePrefix = {arXiv},
	arxivId = {1612.01051},
	author = {Wu, Bichen and Iandola, Forrest and Jin, Peter H. and Keutzer, Kurt},
	doi = {10.1109/CVPRW.2017.60},
	eprint = {1612.01051},
	isbn = {9781538607336},
	issn = {21607516},
	journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	month = {dec},
	pages = {446--454},
	title = {{SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving}},
	url = {http://arxiv.org/abs/1612.01051},
	volume = {2017-July},
	year = {2016}
}
@article{Redmon,
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	annote = {- mostly localization error
	- feature extraction, classification, localization is one pipeline
	- single class detecters can be highly optimized
	- uses google net convolutional layer
	- image is split in SxS grid
	- each grid proposes: B Bounding boxes, and C classes
	- a bounding box gets "responsible" for a certain object if the center of it falls into the box, this is calculated by the highest intersection over union in that grid cell
	- dropout, extensive data augmentation},
	archivePrefix = {arXiv},
	arxivId = {1506.02640},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	doi = {10.1109/CVPR.2016.91},
	eprint = {1506.02640},
	file = {:home/phil/Documents/research/papers/2015/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {01689002},
	mendeley-groups = {Detection},
	pmid = {27295650},
	title = {{You Only Look Once: Unified, Real-Time Object Detection}},
	url = {http://arxiv.org/abs/1506.02640},
	year = {2015}
}
@article{Krizhevsky2012,
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
	annote = {- first "deep" network
	- limited by cpu/gpu constraints
	- applied image augmentation to increase sample size
	- variable learning rate},
	archivePrefix = {arXiv},
	arxivId = {1102.0183},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
	eprint = {1102.0183},
	file = {:home/phil/Documents/research/papers/2012/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
	isbn = {9781627480031},
	issn = {10495258},
	journal = {Advances In Neural Information Processing Systems},
	mendeley-groups = {Classification},
	pages = {1--9},
	pmid = {7491034},
	title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	year = {2012}
}
@article{Viola2004,
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distin-guished by three key contributions. The first is the introduction of a new image representation called the Integral Image which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small num-ber of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a cascade which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detec-tion the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	author = {Viola, P ; and Jones},
	file = {:home/phil/Documents/research/papers/2004/Viola, Jones - 2004 - Rapid Object Detection Using a Boosted Cascade of Simple Features.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Rapid Object Detection Using a Boosted Cascade of Simple Features}},
	url = {http://www.merl.com},
	year = {2004}
}
@article{Lienhart,
	abstract = {Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10{\%} lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5{\%}. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3{\%} on the CMU face set [7].},
	author = {Lienhart, Rainer and Maydt, Jochen},
	file = {:home/phil/Documents/research/papers/Unknown/Lienhart, Maydt - Unknown - n Extended Set of Haar-like Features for Rapid Object Detection.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{n Extended Set of Haar-like Features for Rapid Object Detection}},
	url = {http://www.videoanalysis.org/Prof.{\_}Dr.{\_}Rainer{\_}Lienhart/Publications{\_}files/ICIP2002.pdf}
}
@article{Ren,
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Pro-posal Network (RPN) that shares full-image convolutional features with the de-tection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and ob-jectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster{\_}rcnn.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	file = {:home/phil/Documents/research/papers/Unknown/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
	mendeley-groups = {Detection},
	title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
	url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}
@article{LeCun,
	abstract = {M achine-learning technology powers many aspects of modern society: from web searches to content filtering on social net-works to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users' interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, con-structing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a fea-ture extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representa-tion, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence commu-nity for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applica-ble to many domains of science, business and government. In addition to beating records in image recognition},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	doi = {10.1038/nature14539},
	file = {:home/phil/Documents/research/papers/Unknown/LeCun, Bengio, Hinton - Unknown - Deep learning.pdf:pdf},
	mendeley-groups = {Architecture},
	title = {{Deep learning}},
	url = {http://pages.cs.wisc.edu/{~}dyer/cs540/handouts/deep-learning-nature2015.pdf}
}
@article{Naseer,
	abstract = {— Precise localization of robots is imperative for their safe and autonomous navigation in both indoor and outdoor environments. In outdoor scenarios, the environment typically undergoes significant perceptual changes and requires robust methods for accurate localization. Monocular camera-based approaches provide an inexpensive solution to such challenging problems compared to 3D LiDAR-based methods. Recently, approaches have leveraged deep convolutional neu-ral networks (CNNs) to perform place recognition and they turn out to outperform traditional handcrafted features under challenging perceptual conditions. In this paper, we propose an approach for directly regressing a 6-DoF camera pose using CNNs and a single monocular RGB image. We leverage the idea of transfer learning for training our network as this technique has shown to perform better when the number of training samples are not very high. Furthermore, we propose novel data augmentation in 3D space for additional pose coverage which leads to more accurate localization. In contrast to the traditional visual metric localization approaches, our resulting map size is constant with respect to the database. During localization, our approach has a constant time complexity of O(1) and is independent of the database size and runs in real-time at∼80 Hz using a single GPU. We show the localization accuracy of our approach on publicly available datasets and that it outperforms CNN-based state-of-the-art methods.},
	author = {Naseer, Tayyab and Burgard, Wolfram},
	file = {:home/phil/Documents/research/papers/Unknown/Naseer, Burgard - Unknown - Deep Regression for Monocular Camera-based 6-DoF Global Localization in Outdoor Environments.pdf:pdf},
	mendeley-groups = {Camera (Re)Localization},
	title = {{Deep Regression for Monocular Camera-based 6-DoF Global Localization in Outdoor Environments}},
	url = {http://ais.informatik.uni-freiburg.de/publications/papers/naseer17iros.pdf}
}
@article{Kendall,
	abstract = {PoseNet: Convolutional neural network monocular camera relocalization. Relocalization results for an input image (top), the predicted camera pose of a visual reconstruction (middle), shown again overlaid in red on the original image (bottom). Our system relocalizes to within approximately 2m and 6 • for large outdoor scenes spanning 50, 000m 2 . For an online demonstration, please see our project webpage: mi.eng.cam.ac.uk/projects/relocalisation/ Abstract We present a robust and real-time monocular six de-gree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF cam-era pose from a single RGB image in an end-to-end man-ner with no need of additional engineering or graph op-timisation. The algorithm can operate indoors and out-doors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 • accuracy for large scale outdoor scenes and 0.5m and 10 • accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demon-strating that convnets can be used to solve complicated out of image plane regression problems. This was made possi-ble by leveraging transfer learning from large scale classi-fication data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT reg-istration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.},
	author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
	file = {:home/phil/Documents/research/papers/Unknown/Kendall, Grimes, Cipolla - Unknown - PoseNet A Convolutional Network for Real-Time 6-DOF Camera Relocalization.pdf:pdf},
	mendeley-groups = {Camera (Re)Localization},
	title = {{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization}},
	url = {https://arxiv.org/pdf/1505.07427.pdf}
}
@misc{Lepetit,
	author = {Lepetit, Vincent},
	file = {:home/phil/Documents/research/papers/Unknown/Lepetit - Unknown - Using Deep Learning for ! Localization from Images.pdf:pdf},
	mendeley-groups = {Camera (Re)Localization},
	title = {{Using Deep Learning for  ! Localization from Images}},
	url = {https://www.labri.fr/perso/vlepetit/teaching/visum17/03{\_}localization.pdf}
}
@article{Rubio,
	abstract = {— We propose a robust and efficient method to estimate the pose of a camera with respect to complex 3D textured models of the environment that can potentially contain more than 100, 000 points. To tackle this problem we follow a top down approach where we combine high-level deep network classifiers with low level geometric approaches to come up with a solution that is fast, robust and accurate. Given an input image, we initially use a pre-trained deep network to compute a rough estimation of the camera pose. This initial estimate constrains the number of 3D model points that can be seen from the camera viewpoint. We then establish 3D-to-2D corres-pondences between these potentially visible points of the model and the 2D detected image features. Accurate pose estimation is finally obtained from the 2D-to-3D correspondences using a novel PnP algorithm that rejects outliers without the need to use a RANSAC strategy, and which is between 10 and 100 times faster than other methods that use it. Two real experiments dealing with very large and complex 3D models demonstrate the effectiveness of the approach.},
	author = {Rubio, A and Villamizar, M and Ferraz, L and Penate-Sanchez, A and Ramisa, A and Simo-Serra, E and Sanfeliu, A and Moreno-Noguer, F},
	file = {:home/phil/Documents/research/papers/Unknown/Rubio et al. - Unknown - Efficient Monocular Pose Estimation for Complex 3D Models.pdf:pdf},
	mendeley-groups = {Pose Estimation},
	title = {{Efficient Monocular Pose Estimation for Complex 3D Models}},
	url = {http://www.iri.upc.edu/files/scidoc/1638-Efficient-monocular-pose-estimation-for-complex-3D-models.pdf}
}
@article{Kniaz,
	abstract = {Accurate estimation of camera external orientation with respect to a known object is one of the central problems in photogrammetry and computer vision. In recent years this problem is gaining an increasing attention in the field of UAV autonomous flight. Such application requires a real-time performance and robustness of the external orientation estimation algorithm. The accuracy of the solution is strongly dependent on the number of reference points visible on the given image. The problem only has an analytical solution if 3 or more reference points are visible. However, in limited visibility conditions it is often needed to perform external orientation with only 2 visible reference points. In such case the solution could be found if the gravity vector direction in the camera coordinate system is known. A number of algorithms for external orientation estimation for the case of 2 known reference points and a gravity vector were developed to date. Most of these algorithms provide analytical solution in the form of polynomial equation that is subject to large errors in the case of complex reference points configurations. This paper is focused on the development of a new computationally effective and robust algorithm for external orientation based on positions of 2 known reference points and a gravity vector. The algorithm implementation for guidance of a Parrot AR.Drone 2.0 micro-UAV is discussed. The experimental evaluation of the algorithm proved its computational efficiency and robustness against errors in reference points positions and complex configurations.},
	author = {Kniaz, V V},
	doi = {10.5194/isprsarchives-XLI-B5-63-2016},
	file = {:home/phil/Documents/research/papers/Unknown/Kniaz - Unknown - ROBUST VISION-BASED POSE ESTIMATION ALGORITHM FOR AN UAV WITH KNOWN GRAVITY VECTOR.pdf:pdf},
	keywords = {UAV,external orientation estimation,machine vision,motion capture system},
	mendeley-groups = {Pose Estimation},
	title = {{ROBUST VISION-BASED POSE ESTIMATION ALGORITHM FOR AN UAV WITH KNOWN GRAVITY VECTOR}},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B5/63/2016/isprs-archives-XLI-B5-63-2016.pdf}
}
@article{Erol2007,
	abstract = {Direct use of the hand as an input device is an attractive method for providing natural human-computer interaction (HCI). Currently, the only technology that satisfies the advanced requirements of hand-based input for HCI is glove-based sensing. This technology, however, has several drawbacks including that it hinders the ease and naturalness with which the user can interact with the computer-controlled environment, and it requires long calibration and setup procedures. Computer vision (CV) has the potential to provide more natural, non-contact solutions. As a result, there have been considerable research efforts to use the hand as an input device for HCI. In particular, two types of research directions have emerged. One is based on gesture classification and aims to extract high-level abstract information corresponding to motion patterns or postures of the hand. The second is based on pose estimation systems and aims to capture the real 3D motion of the hand. This paper presents a literature review on the latter research direction, which is a very challenging problem in the context of HCI. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
	archivePrefix = {arXiv},
	arxivId = {1412.0065},
	author = {Erol, Ali and Bebis, George and Nicolescu, Mircea and Boyle, Richard D and Twombly, Xander},
	doi = {10.1016/j.cviu.2006.10.012},
	eprint = {1412.0065},
	file = {:home/phil/Documents/research/papers/2007/Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf:pdf},
	isbn = {1077-3142},
	issn = {10773142},
	journal = {Computer Vision and Image Understanding},
	keywords = {Gesture recognition,Gesture-based HCI,Hand pose estimation},
	mendeley-groups = {Pose Estimation},
	number = {1-2},
	pages = {52--73},
	pmid = {17533767},
	title = {{Vision-based hand pose estimation: A review}},
	url = {https://ac.els-cdn.com/S1077314206002281/1-s2.0-S1077314206002281-main.pdf?{\_}tid=60899afe-c307-11e7-b84d-00000aacb35e{\&}acdnat=1509982391{\_}fb739a4053e355313c7abb81c58a13a1},
	volume = {108},
	year = {2007}
}
@article{,
	doi = {10.1016/J.CVIU.2006.10.012},
	issn = {1077-3142},
	journal = {Computer Vision and Image Understanding},
	mendeley-groups = {Pose Estimation},
	month = {oct},
	number = {1-2},
	pages = {52--73},
	publisher = {Academic Press},
	title = {{Vision-based hand pose estimation: A review}},
	url = {http://www.sciencedirect.com/science/article/pii/S1077314206002281},
	volume = {108},
	year = {2007}
}
@article{Lepetit2005,
	abstract = {Monocular Model-Based 3D Tracking of Rigid Objects: A Survey},
	author = {Lepetit, Vincent and Fua, Pascal},
	doi = {10.1561/0600000001},
	file = {:home/phil/Documents/research/papers/2005/Lepetit, Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects A Survey.pdf:pdf},
	issn = {1572-2740},
	journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
	keywords = {3D reconstruction and image-based modeling,Computer Graphics},
	mendeley-groups = {Tracking},
	number = {1},
	pages = {1--89},
	publisher = {Now Publishers, Inc.},
	title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
	url = {http://www.nowpublishers.com/article/Details/CGV-001},
	volume = {1},
	year = {2005}
}
@article{Murphy-Chutorian,
	abstract = {— The capacity to estimate the head pose of another person is a common human ability that presents a unique chal-lenge for computer vision systems. Compared to face detection and recognition, which have been the primary foci of face-related vision research, identity-invariant head pose estimation has fewer rigorously evaluated systems or generic solutions. In this paper, we discuss the inherent difficulties in head pose estimation and present an organized survey describing the evolution of the field. Our discussion focuses on the advantages and disadvantages of each approach and spans 90 of the most innovative and characteristic papers that have been published on this topic. We compare these systems by focusing on their ability to estimate coarse and fine head pose, highlighting approaches that are well suited for unconstrained environments.},
	author = {Murphy-Chutorian, Erik and Trivedi, Mohan Manubhai},
	doi = {10.1109/TPAMI.2008.106},
	file = {:home/phil/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy-Chutorian, Trivedi - Unknown - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Head Pose Estimation in Computer Vi.pdf:pdf},
	keywords = {Face Analysis,Facial Land Marks,Gesture Analysis,Human Computer In-terfaces,Index Terms— Head Pose Estimation},
	mendeley-groups = {Pose Estimation},
	title = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Head Pose Estimation in Computer Vision: A Survey}},
	url = {https://pdfs.semanticscholar.org/cf87/e167446bd22e9422445dfed4f74a3f0579f9.pdf}
}
@article{Madsen2001,
	author = {Madsen, O and Ayromlou, M and Beltran, C},
	file = {:home/phil/Documents/research/papers/2001/Madsen, Ayromlou, Beltran - 2001 - Model and Vision Based Pose Estimation for Mobile Robots.pdf:pdf},
	journal = {IASTED International Conference on Robotics and Applications},
	keywords = {mobile robots,model based vision,pose estimation},
	mendeley-groups = {Pose Estimation},
	pages = {1--8},
	title = {{Model and Vision Based Pose Estimation for Mobile Robots}},
	url = {http://83.212.134.96/robotics/wp-content/uploads/2011/12/Model-and-Vision-Based-Pose-Estimation-for-Mobile-Robots.pdf},
	year = {2001}
}
@article{Rudol2006,
	author = {Rudol, Piotr and Wzorek, Mariusz and Conte, Gianpaolo and Doherty, Patrick},
	file = {:home/phil/Documents/research/papers/2006/Rudol et al. - 2006 - Vision-based Pose Estimation for Autonomous Indoor Navigation of Micro Unmanned Aircraft Systems.pdf:pdf},
	isbn = {9781424450404},
	journal = {Image (Rochester, N.Y.)},
	mendeley-groups = {Robotics},
	pages = {2030--2030},
	title = {{Vision-based Pose Estimation for Autonomous Indoor Navigation of Micro Unmanned Aircraft Systems}},
	volume = {12},
	year = {2006}
}
@article{Bonin-font2008,
	abstract = {Mobile robot vision-based navigation has been the source of countless research contributions, from the domains of both vision and control. Vision is becoming more and more common in applications such as localization, automatic map construction, autonomous navigation, path following, inspection, monitoring or risky situation detection. This survey presents those pieces of work, from the nineties until nowadays, which constitute a wide progress in visual navigation techniques for land, aerial and autonomous underwater vehicles. The paper deals with two major approaches: map-based navigation and mapless navigation. Map-based navigation has been in turn subdivided in metric map-based navigation and topological map-based navigation. Our outline to mapless navigation includes reactive techniques based on qualitative characteristics extraction, appearance-based localization, optical flow, features tracking, plane ground detection/tracking, etc... The recent concept of visual sonar has also been revised.},
	author = {Bonin-font, Francisco and Ortiz, Alberto and Oliver, Gabriel and Alberto, Francisco Bonin-font and Gabriel, Ortiz},
	doi = {10.1007/s10846-008-9235-4},
	file = {:home/phil/Documents/research/papers/2008/Bonin-font et al. - 2008 - Visual Navigation for Mobile Robots A Survey.pdf:pdf},
	isbn = {0921-0296},
	issn = {0921-0296},
	journal = {Journal of Intelligent and Robotic Systems},
	keywords = {Mobile robots,Visual navigation,and feder funding,lista{\_}filtrada,mobile robots,supported by dpi 2005-09001-c03-02,this work is partially,toread,visual navigation},
	mendeley-groups = {Robotics},
	pages = {263--296},
	title = {{Visual Navigation for Mobile Robots: A Survey}},
	url = {http://dx.doi.org/10.1007/s10846-008-9235-4{\%}5Cnhttp://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42549109228{\&}partnerID=40{\&}rel=R8.0.0},
	volume = {53},
	year = {2008}
}
