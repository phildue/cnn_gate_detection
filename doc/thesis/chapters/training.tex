\chapter{Learning with Limited Data}
\label{sec:training}

\begin{center}
	\textbf{How can a detection model be learned with a limited amount of annotated examples?}
\end{center}

\todo{Explain relation between training data and test data. Explain domain shift.  Why do we use synthetic data? }

The task of any machine learning approach is to generalize a concept obtained from training examples to new unseen data.
The bias-variance dilemma \todoref{bias-variance} relates model complexity and available training data: More complex models introduce a lower bias but larger variance. Lowering the bias can only be done by either reducing model complexity or increasing the amount of training data. This also means the more data is available the more complex models can be used. Thus one can say the available data is never enough, as more data would allow more complex and thus potentially better performing models.

A way to avoid the overfitting of a model to the training set is regularization. That is to limit the training parameters of a model while training.

For the given task only a very limited set of training examples is available. In addition the annotated examples stem from a single room. This not only limits the design space to simple models it also introduces the problem of domain shift: As only a very limited amount of possible backgrounds and light conditions is present, a machine learning model is likely to overfit to the domain and can not be generalized to unseen examples from other domains. \todo{describe this more formally, proof that a domain shift/covariate shift scenario is reasonable to assume}

A common method in computer vision/ object detection is the artificial augmentation of images while training. For example by converting some images of the training set to grayscale images one can force the model to learn a more color independent representation.

Another way to increase training data is to artificially create training examples. Modern graphic engines allow the generation of almost photorealistic images. However, even then one has to take the domain shift into account.



\section{Background}
Few shot learning:
   \cite{Moysset2016}, \cite{Chen2018a}

Domain Adaption/Synthetic Data:
 \cite{Chen2018c}, \cite{Xu2017}
\cite{Tremblay2018a} ,\cite{Inoue},\cite{Peng},
\cite{Rozantsev},  \cite{Le}, \cite{Liu2017}, \cite{Peng2017}

Incorporating Camera Effects:
\cite{Carlson2018},\cite{Vass}

Image Augmentation
 \cite{Bai2017},
 
\section{Hypothesis}

\todo{It should be possible to learn a model from synthetic data, incorporating background and camera effects should improve performance}
 
\section{Experiments}

\todo{Compare performance from synthetic to real data}
\todo{How important is context?}
\todo{How much is the influence of camera effects?}

\section{Conclusion}

%As illustrated in the introduction the amount of training data is limited. This is critical for the performance of machine learning methods as they heavily depend on the available training data.



%Limited data can happen in multiple ways:
%\begin{enumerate}
%	\item The amount of data is too small thus the parameters can not be tuned effectively. Subsequently the model underfits or overfits the training data.
%	
%	\item The captured variety in the data set is limited. If the labeled examples do not sufficiently represent the objects in the real world, the model will fail in the wild.
%	
%	\item There is a domain shift between the training data and the test data.
%\end{enumerate}

%
%\section{Summaries}
%\subsection{Modeling Camera Effects to Improve Deep Vision for Real and Synthetic Data\cite{Carlson2018}}
%\begin{itemize}
%	\item heaps of references in how camera properties influence object detectors
%	\item introduces image augmentation pipeline based on physical model: chromatic aberration, blur, exposure, noise and color shift
%	\item input parameters are modelled by hand, then randomly selected within "realistic" range
%	\item no lense distortion
%	\item method seem to benefit for small objects and when oversaturation applies due to camera effects
%\end{itemize}
%\subsection{Domain Randomization\cite{Tremblay2018a}}
%\begin{itemize}
%	\item uses ue4 engine with custom plugin
%	\item model is supposed to learn independent of domain
%	\item textures are randomly applied
%	\item object models are randomly selected
%	\item approach cant capture patterns like how cars are parked (similar could happend when randomly placing the gates)
%	\item Tuning lower layers on simulated data fine tuning on real data
%	\item Effects of DR-parameters: light augmentation 0.1\%, light placement 10, \% texture 4\%, data augmentation 1.6\%, flying distractors 1.1 \%
%	\item pretraining on imagenet helps a lot, freezing weights when training on synthetic data does not help
%\end{itemize}