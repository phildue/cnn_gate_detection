\chapter{Synthesizing Data for Object Detection on \ac{MAV}}
\label{sec:training}

Deep learning based Computer Vision models benefit from large training sets which are particularly hard to obtain for \ac{EWFO} detection on \acp{MAV}. Hence, this chapter addresses the generation of data for this application. 

The relevant question to be investigated in this chapter is the following:

\textbf{How can data be generated to train a detection model for \ac{EWFO} detection on a \acp{MAV}?} 

This main question is split in multiple sub questions:
\begin{enumerate}
	\item[\textbf{RQ1.1}] What are the implications of the shape of \acp{EWFO} when synthesizing training data for their detection?
	\item[\textbf{RQ1.2}] Can the incorporation of target domain knowledge in the data generation process improve the detection performance?
	\item[\textbf{RQ1.3}] How do domain shifts between training and test data affect the detection performance?
\end{enumerate}

In order to evaluate different aspects of the data generation process a pip

This chapter focuses on data generation while the exact model is described in \Cref{sec:object_detection}.


In the following, methods used in this thesis are described. The different steps are implemented in a data generation pipeline and the effect of the individual steps are studied in various experiments. An overview can be seen in \Cref{fig:training:toolchain_datagen}. The individual steps are described in the following.

The environment model determines background and lightning conditions and produces a 3D-Scene. The motion model determines the camera pose and location and thus the view point. Projecting the 3D-Scene on the image plane of the camera delivers the 2D-View. A final post processing step incorporates sensor and lens effects.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{fig/Toolchain_datagen}
	\caption{Overview of the data generation process.}
	\label{fig:training:toolchain_datagen}
\end{figure}

\section{Related Work}
\label{sec:training:related}

Related methods vary from changing low level properties of the image over using CAD models in combination with real background up to rendering full 3D-environments. Often various combinations of synthesized and real data are applied. 

\subsection{Low-Level Image Augmentation}

A common part of current Computer Vision pipelines is to augment a given data set by transforming low level properties of the image. By artificially increasing variations in the input signal, a model that is more invariant to the augmented properties shall be obtained.

\citeauthor{Krizhevsky2012a} \cite{Krizhevsky2012a} use \ac{PCA} to incorporate colour variations. \citeauthor{Howard2013} \cite{Howard2013} shows how several image transformations can improve the performance of a \ac{CNN}-based Classification model. The proposed pipeline includes variations in the crop of the input image as well as variations in brightness, color and contrast. In \ac{CNN}-based Object Detection \citeauthor{Redmon} \cite{Redmon} uses random scaling and translation of the input image, as well as random variations in saturation and exposure. \citeauthor{Liu} \cite{Liu} additionally crop and flip each image with a certain probability.

Since most methods use image augmentation and \citeauthor{Krizhevsky2012a} \cite{Krizhevsky2012a} mentions it to be the particularly reason for superior performance at ILSVRC2012 competition it can be assumed to be beneficial for Computer Vision models. Unfortunately, none of the publications measures the improvements gained by the different operations. This work includes a subset of the proposed techniques and measures its effect on model performance.

While the aforementioned approaches add artificial variation to the input data, \citeauthor{Carlson2018}\cite{Carlson2018} augment the image based on a physical camera model. The proposed pipeline is applied for Object Detection and incorporates models for sensor and lens effects like chromatic aberration, blur, exposure and noise. While being of minor effect for the augmentation of real data (0.1\% - 1.62\% \ac{mAP}70) the reported results show an improvement when training on fully synthesized datasets. Here the reported gains vary between 1.26 and 6.5 \% \ac{mAP}70.

Low-level image augmentation is a comparatively cheap method to increase the variance in a dataset. However, it cannot create totally new samples or view points. Furthermore, it cannot change the scene in which an object is placed. Therefore it needs a sufficiently large base dataset that is augmented. This work addresses the case when no real training data is available. Hence, low-level image augmentation is incorporated in the training process but can not be the only method applied.

\subsection{Augmenting Real Images with CAD - Models}

\ac{CAD}-models describe the 3D-shape of an object. In a lot of industrial applications such models are available as they are also used to produce the object. For Computer Vision it can be placed on real images to artificially create or increase the size of a dataset. 

\citeauthor{Peng}\cite{Peng} study the use of \ac{CAD}-models in the context of \ac{CNN}-based Object Detection. The authors particularly address how modelling of image cues like texture, colour and background affects model performance. The experiments show how the used \acp{CNN} are relatively insensitive towards context but use shape as primary, texture and colour as secondary most important features. This enables competitive performance even when the object of interest is placed only on uniformly covered backgrounds. However, the study only covers solid objects such as birds, bicycles and airplanes. \ac{EWFO} are substantially different and we hypothesize that other image cues must be relevant.

\citeauthor{Madaan2017}\cite{Madaan2017} study the segmentation of wires based on synthetic training. As wires similarly to \ac{EWFO} only consist of thin edges, the application is quite close to this work. However, the experiments focus on a single domain, namely sky images and thus the variations in background are comparatively small. We hypothesize that \ac{EWFO} are particularly sensitive to such variations and address the application in multiple domains.

\citeauthor{Hinterstoisser2017} \cite{Hinterstoisser2017} propose to use a base network that has been trained on real images and to continue training on images with \ac{CAD}-models. During training the base network is frozen and only the last layers are updated. The method does not use real data but requires a suitable base network. As most available feature extractors (further discussed in \Cref{sec:object_detection}) are of a size that is computationally prohibitive for \ac{MAV} the method is not really applicable for this work. 

The use of CAD-models in combination with real backgrounds allows to generate totally new view points for the object of interest. Furthermore, the image background consists of real data and thus the synthetic textures only concern the rendered object. However, the geometric properties like perspective as well as the physical properties like object placement are violated and therefore create an artificial scene. Despite this fact, literature shows that such images can benefit model performance in various cases. Yet, most of the approaches still use real data and/or focus on solid objects with rich textures and complex shape. We hypothesize that since \ac{EWFO} do not provide these kind of structures the results do not apply in the same way. Hence, we incorporate the method to generate data and investigate how it can be applied for the detection of \ac{EWFO}.

\subsection{Fully Synthesizing Data}

Training models only in a simulated environment is common for Control tasks. In Computer Vision poor quality of graphic engines and long rendering time made the method less popular. However, advances in Computer Graphics and faster processing technologies enabled the generation of more realistic images and led to the creation of fully synthesized datasets\cite{Ros2016, Gaidon2016}. Various studies tried to incorporate such data in their training process.

\citeauthor{Johnson-Roberson2016} \cite{Johnson-Roberson2016} train an Object Detection model entirely in simulation. The results show an improvement towards data annotated by humans especially when using vast amounts of simulated data. However, the created environment is highly detailed and therefore requires a lot of engineering work. \todo{There should be more examples for this}

In contrast \cite{Sadeghi2016, Tobin2017, Tremblay2018a} use a relatively simple environment but a high degree of randomization to address the reality gap. The aim is to learn an abstract representation by strongly varying textures, light conditions and object locations. \citeauthor{Tobin2017} introduced this technique as \ac{DR}. The drawback of the approach is that a too high degree of randomization may omit pattern in the target domain that could otherwise be exploited by the model. 

Training a model in a fully synthesized environment enables the full control of all properties present in an image. The object of interest can be placed according to physical laws, shadows fall correctly and geometric properties of an image are followed. However, if the graphical models do not fully capture the detail of real world objects, the generated data might look too artificial. Methods in literature address this problem in two ways: (1) Creating a virtual environment that resembles the real world. Although a lot of engineering effort and processing power is required, a lot of properties of the real domain can be incorporated; (2) Creating a lot of variance in the data generation to obtain an abstract representation. While bearing the risk of omitting properties of the target domain, this method requires less engineering effort.

This work addresses the application of \acp{MAV} in \ac{GPS} denied scenarios. Such scenarios cover a wide range of possible environmental conditions and a full modelling of all these possible domains is beyond the scope of this work. On the other hand, we hypothesize that a model with a high degree of abstraction might perform well across domains but poor in a particular domain. Hence, we investigate this trade-off for the detection of \acp{EWFO}.


\subsection{Transfer Learning}

The field of transfer learning particularly addresses domain shifts in the modelling process. Hence, a common application is the learning from synthetic data.

A common approach in \ac{CNN}-based models is the incorporation of a domain classifier in the model. By augmenting the data with domain labels, the classifier learns to distinguish the two domains. Subsequently a gradient reverse layer is applied and thus the weights are updated in such a way that a domain agnostic representation is learned. Examples of the approach can be found in \cite{Chen2018c} \cite{Xu2017}.

While the aforementioned approaches require labelled samples from the target domain, \citeauthor{Peng2017} \cite{Peng2017} propose to include task-irrelevant samples and a source classifier. As a result no samples of the target domain are required.

While transfer learning provides the theoretical framework as well as methods to deal with domain shifts, it does not allow to generate data. Furthermore, it often requires samples of the target domain. This work addresses the case when no real data is used for training. The field is interesting to be incorporated in the data generation pipeline investigated in this thesis but it can not be used as a start off point. Hence, the use of transfer learning in the modelling process is denoted as future work.

\subsection{Generative Adversarial Networks}

\cite{Inoue} 
\todo{write}



\subsection{Scene Generation}
\label{sec:training:scene}

The first step creates a scene in which the object is embedded. Therefore it determines the context as well as light conditions.

\subsubsection{Pasting a CAD-Model on Real Images}

Inspired by \cite{Girshick2013, Peng, Rozantsev} a 3D-Model of the object of interest is pasted on real images.

The 3D-Model is provided by \todo{TODO} TODO and a tool to render these models is implemented using \textit{OpenGL}. A scene with black background is created and several objects are placed on a virtual ground plane with different rotations to each other. Additionally light sources are placed at different locations and with different intensities. 

After creating a scene the black background is replaced with an image obtained from a dataset. As the light conditions between background image and rendered scene are different, the created image can be quite artificial. Hence a post-processing step applies a Gaussian blur kernel along the edges of the CAD-model. This leads to a better embedding of the object in the background. Example outcomes can be seen in \Cref{fig:random_bg}

The parameters and locations of objects and light sources as well as the selected backgrounds can be varied when creating a dataset.

\begin{figure}[hbtp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/shot}
	\end{minipage}
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/random_bg2}
	\end{minipage}
	\caption{Examples for pasting the 3D-Model on various backgrounds. A scene with background is rendered (left), subsequently the black background is replaced with an image from a dataset.}
	\label{fig:random_bg}
\end{figure}

\subsubsection{Full Rendering of a Scene}

Following \cite{Ros2016, Gaidon2016, Johnson-Roberson2016, Tobin2017, Tremblay2018a} the second approach to create a scene is to fully render the environment using a graphic engine, namely the \textit{UnrealEngine} including the \textit{AirSim} plugin.

The \textit{UnrealEditor} allows to manually create environments and provides high quality rendering of objects, textures and light conditions. In total three indoor environments are created. This resembles \ac{GPS}-denied scenarios as they are targeted in this thesis. Within the environment light conditions, background textures, object locations can be changed manually.

In total three base environments are created. Examples can be seen in \Cref{fig:environments}. The three environments are described in the following:
\begin{enumerate}
	\item \textit{Basement:} The environment is a room without windows, only containing artificial light sources. 
	\item \textit{Daylight:} The environment is a room with windows along all walls that allow daylight to illuminate the room. The windows can lead to strong variations in the contrast between different parts of the object.
	\item \textit{IROS2018:} The environment resembles the room of the \ac{IROS} Autonomous Drone Race 2018. The light sources stem from a window front at one side of the room, as well as artificial light sources at the ceiling. Depending on the view point, the object might appear against bright or dark background.
\end{enumerate}

\begin{figure}[hbtp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\includegraphics[width=\textwidth]{fig/basement_perspective}
	\end{minipage}
	\begin{minipage}{0.49\textwidth}
	\includegraphics[width=\textwidth]{fig/basement_perspective}
\end{minipage}
\caption{\textit{Basement} Environment.}
\begin{minipage}{0.49\textwidth}
	\includegraphics[width=\textwidth]{fig/daylight_perspective}
\end{minipage}
\begin{minipage}{0.49\textwidth}
	\includegraphics[width=\textwidth]{fig/daylight_perspective}
\end{minipage}
\caption{\textit{Daylight} Environment.}
\begin{minipage}{0.49\textwidth}
	\includegraphics[width=\textwidth]{fig/iros_perspective}
\end{minipage}
\begin{minipage}{0.49\textwidth}
	\includegraphics[width=\textwidth]{fig/iros_perspective}
\end{minipage}
\caption{\textit{IROS2018} Environment.}
\label{fig:environments}
\end{figure}

\subsection{Camera Placement}

The second step places the camera in the scene and creates a 2D-image. Hence, it determines the perspective of the image on the scene. The camera pose is determined by:

$$
\text{Translation: }t = [x y z] \text{ and Rotation: } r = [\phi, \theta, \psi]
$$
Where the \ac{NED} coordinate system is used originating from the initial position of the camera. 

\subsubsection{Random Placement}
	
A straightforward way of placing the camera is the random placement in the scene. The value for each dimension of $r$ and $t$ are drawn from a probability distribution. The chosen distributions have to follow certain limitations, for example the gate should still be visible in most images.
	
\subsubsection{Quad-rotor Model}
	
$r$ and $t$ follow the motion model of a quad-rotor \ac{MAV}.  The development of this model has not been done within this thesis but is summarized here for completeness: \todo{put shuos model here}.

Using this motion model, the camera follows a certain trajectory through the 3D-Environment. Storing the current image at a frequency of 2 Hz creates the corresponding samples.
	
\subsection{Post-processing}

In the final step low-level image transformations are applied. Steps include sensor model based an artificial augmentation.

\subsubsection{Model-based augmentation}

The applied pipeline is strongly based on \cite{Carlson2018}. However, on the camera of the \ac{MAV} we assume that the raw image signal can be processed. Hence, the model for information loss due to post-processing is not included. Instead all images are converted into YUYV-colour space, a format that is obtained from most visual sensors as well as the target platform of this thesis. The pipeline is extended by two models: (1) A model for lens distortion since \ac{MAV} often use wide angle lenses to increase their \ac{FoV}; (2) A model for motion blur since fast camera movements are to be expected. The individual models are described in the following.

\paragraph{Lens Distortion}

Lens distortion is a form of optical aberration which causes light to not fall in a single point but a region of space. For \acp{MAV} commonly used wide-angle lenses, this leads to barrel distortion and thus to straight lines appearing as curves in the image.

The effect is applied using the model for wide-angle lenses from \cite{Vass}. It models the removal of lens distortion as combination of radial and non-radial part, that is approximated with a second order Taylor expansion:

\begin{equation}
\begin{pmatrix}
	p_x' \\
	p_y'
\end{pmatrix} = \begin{pmatrix}
p_x(1 + \kappa_1 p_x^2 + \kappa_1 (1 + \lambda _x)p_y^2 + \kappa_2(p_x^2 + p_y^2)^2) \\
p_y(1 + \kappa_1 p_x^2 + \kappa_1 (1 + \lambda _y)p_y^2 + \kappa_2(p_x^2 + p_y^2)^2)
\end{pmatrix} 
\label{eq:distortion}
\end{equation}
Where:
\begin{itemize}
	\item $p_x'$ and $p_y'$ are the undistorted coordinates.
	\item $\kappa_1$ controls the primary distortion (default 0)
	\item $\kappa_2$ controls the secondary distortion (default 0)
	\item $\lambda_x$ and $\lambda_y$ controls asymmetric distortion (default 0)
\end{itemize}
 
Applying the lens distortion to an image is done using the inverse of \Cref{eq:distortion}. However, as there is no closed form solution, so the Newton-approximation is used:

\begin{equation}
	p_i = p_{i-1} - \nabla p^{-1} (f(p)_i-p')
\end{equation}

Where $f$ is the function defined in \Cref{eq:distortion}. 

An example with $\kappa_1 = 0.5, \kappa_2 = 0.5$ is displayed in \Cref{fig:distortion}. It can be seen how the previously straight lines appear as circular shape.

\paragraph{Chromatic Aberration.}

Chromatic Aberration is caused when different wavelengths of light do not end up in the same locations of the visual sensor. This leads to a shift in the colour channels of the image.

Similarly to \cite{Carlson2018}, chromatic aberration is applied by scaling the locations of the green channel, as well as applying translations on all channels. The model can be implemented as affine transformation of the pixel locations for each channel:

\begin{equation}
\begin{pmatrix}
x' \\
y' \\
1
\end{pmatrix} = \begin{pmatrix}
S & 0 & t_x \\
0 & S & t_y \\
0 & 0 & 1
\end{pmatrix} \begin{pmatrix}
x \\
y \\
1
\end{pmatrix}
\end{equation}

An example is displayed in \Cref{fig:chromatic}. It can be seen how the red and green channel are shifted relative to each other. Thus two bars appear in the image.

\paragraph{Motion Noise.}

Motion noise is caused when light falls in different locations of the images sensor due to a fast movement of the camera. It leads to blurry images based on the sensor motion.

The phenomenon depends on camera properties as well as the motion of camera and objects. Although a full modelling of this process might benefit the learning process, it requires a complex pipeline and is computationally expensive. Therefore a strong simplification is used, namely a one-dimensional Gaussian filter:

\begin{equation}
K_v = \begin{pmatrix}
...				 \\
\mathcal{N}(\mu-1) \\
\mathcal{N}(\mu)  \\
\mathcal{N}(\mu+1)	 \\
	...					
\end{pmatrix} \quad
K_h = \begin{pmatrix}
...	& \mathcal{N}(\mu-1)	&	\mathcal{N}(\mu) &	\mathcal{N}(\mu+1) & ...\\
\end{pmatrix}
	\label{eq:motion_noise}
\end{equation}

Where $\mathcal{N}$ is a Gaussian-PDF with mean $\mu$ and variance $\sigma$,  $K_v$ models vertical motion blur, $K_h$ horizontal motion blur. The size of the kernel is chosen by $k$.

An example for vertical blur is displayed in \Cref{fig:motionblur}. It can be seen how particularly horizontal lines appear softer.

\paragraph{Out-of-focus Blur.}

Next to motion, sensor noise can lead to blurry images. For the blur operation a 2D Gaussian kernel is applied on the input image with:

\begin{equation}
 k = \frac{1}{2\sigma_x\sigma_y\pi}e^{-\sqrt{\frac{x^2 + y^2}{2\sigma_x\sigma_y}}} 
\end{equation}

\paragraph{Exposure.}

Exposure is the time the sensor records light in order to create an image. Over- and Underexposure are caused when this time is too short or too long, leading to too dark or too bright images.

Following the model from \cite{Carlson2018}:
 
\begin{equation}
 I = f(S) = \frac{255}{1 + e^{-A S}}
\end{equation}
where $A$ is a constant term for contrast and $S$ the exposure.

The image can be re-exposed with:

\begin{equation}
	I' = f(S+\Delta S)
\end{equation}

where $S$ is obtained from :
\begin{equation}
S = f^{-1}(I) = \frac{\ln(\frac{255}{I}-1)}{-A}
\end{equation}

An example for overexposure is displayed in \Cref{fig:exposure}. It can be seen how lighter areas appear particularly light, while dark areas remain dark.

\paragraph{Noise.}
\todo{text}

\subsection{Artificial Augmentation}

Inspired by \cite{Howard2013, Redmon, Liu} the application of several artificial image transformations is studied. The overall goal is to generate more variations in the input signal and thus to make the model more invariant against changes in those properties. We do not use image scaling or translation for augmentation as it is easier to incorporate such variations using the motion model.

\paragraph{Brightness.} In order to obtain a model that is more robust against illumination changes image brightness is alternated. Therefore a scaling on the V-channel in HSV-colour space is applied. The scaling is drawn from a uniform distribution.
	
\paragraph{Grayscale.} By transforming a subset of samples into grayscale images, the model is forced to learn more color invariant features. 
	
\paragraph{Histogram Equalization.} Changes in the environmental conditions can also affect contrast. By applying a histogram equalization on a subset of images, variations in contrast are achieved. \todo{describe}
	
\paragraph{Flip.} By mirroring the image vertically, more variations in object locations are achieved. The operation is applied with a certain probability.


\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example}
\caption{Original Image.}
\label{fig:orig}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth]{fig/gate_example_chromatic}
	\caption{Chromatic Aberration.} 		
	\label{fig:chromatic}
\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_distorted}
	\caption{Lens Distortion. }		
\label{fig:distortion}
	\end{minipage}

	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_focusblur}
	\caption{Out-of-Focus blur.}
\label{fig:focusblur}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_motionblur_v}
	\caption{Vertical Motion Blur.}
\label{fig:motionblur}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\includegraphics[width=\textwidth]{fig/gate_example_exposure}
	\caption{Exposure.}
\label{fig:exposure}
	\end{minipage}
\end{figure}

\subsection{Hypothesis}
\label{sec:training:hypothesis}

Most of described approaches in \Cref{sec:training:related} focus on objects that have complex structures and therefore provide robust features that are independent of the rest of the scene. For example a face contains eyes and a nose, whose appearances are influenced to some extent by light conditions but relatively independent from image background. It has been shown in \todoref{visualizing cnns} how object detectors exploit this kind of structure in the learned representation. We hypothesize that a model to detect such complex objects is less domain dependent and can therefore be more easily transferred to domains with other environmental conditions. That is the performance drop $\Delta m$ of a model trained in $S$ and applied in $T$ where $S$ and $T$ are different in terms of environmental conditions will be larger for an \ac{EWFO} than for a more complex object.

If the environmental conditions have an high impact, their modelling is particularly important in the data generation process. We hypothesize that, as pasting the image on random backgrounds fails to capture these conditions, it is not a sufficient method to train a model for the detection of \acp{EWFO}. That is the performance drop $\Delta m$ of a model trained in $S$ and applied in $T$ where $S$ is generated by pasting a 3D-Mesh on random backgrounds and $T$ is modelled using full environment rendering will be prohibitively large.
 
From that it follows that in order to generate data for \ac{EWFO} Object Detection it is particularly important to address the domain shift. In literature two ways to approach this problem on the data level can be found: (1) providing a lot of variance in the training data to obtain a representation that is robust against domain shifts; (2) including target domain knowledge in the training data to obtain a representation that is tailored to the target domain. We hypothesize that there is a trade-off between these two approaches: a model that performs well across domains will perform poorer in a particular domain but better in other domains compared to a model that is trained for that particular domain. Hence, we hypothesize, if knowledge of $T$ is included when generating $S$ the performance in $T$ will improve.

In the data generation process this can be addressed on several levels:

\begin{enumerate}
	\item \textbf{Scene Generation.} A model trained for a particular room, with particular lightning conditions will perform better in that room but perform poorly in other rooms than a model trained on various rooms with various lightning conditions. By modelling the environment of the real data, the performance on the real data can be improved.
	
	\item \textbf{Camera Placement.} A model trained using the quad-rotor model will perform better on real data than a model trained using random camera locations.
	
	\item \textbf{Postprocessing.} A model trained using a post-processing pipeline that models the real-world sensor will perform better on the real data than a model that is trained on using varying parameters in the post-processing pipeline.  
\end{enumerate}

The hypotheses are summarized in the following:

\begin{enumerate}
	\item[$\mathcal{H}_1$] The performance drop $\Delta m$ of a model trained in $S$ and applied in $T$ where $S$ and $T$ are different in terms of environmental conditions will be larger for an \ac{EWFO} than for a more complex object.
	
	\item[$\mathcal{H}_2$] In contrast to a more complex object, the performance drop $\Delta m$ of a model for the detection of \acp{EWFO} trained in $S$ and applied in $T$ where $S$ is generated by pasting a \ac{CAD}-Model on real backgrounds and $T$ is modelled using full environment rendering will be prohibitively large.
	
	\item[$\mathcal{H}_3$] A model trained in $S_0$ where $S_0 \in S$ and applied in $T_0$, where $S_0 = T_0$ will perform better in $T_0$ than a model that is trained in $S$ but perform worse in $T_1$ where $T_1 \in S$ and $T_1 \neq S_0$. 

	\item[$\mathcal{H}_3$] By including properties of $T$ in $S$ where $S$ is an artificial set $T$ is the real data, the performance $m_T$ of a model can be improved. 

\end{enumerate}

\section{Experiments}
\label{sec:training:experiments}
In order to evaluate the formulated hypotheses several experiments are conducted. The model used is the TinyYoloV3-Architecture, further described in \Cref{sec:object_detection}. The reported metrics are described in \Cref{sec:metrics}. For all experiments mean and standard deviation of 5 runs are reported.

For the random view point generation the following parameters are used:

\begin{equation}
	x = \mathcal{U}(-30,30),\quad y = \mathcal{U}(-20,20),\quad z = \mathcal{N}(-4.5,0.5)),\quad
	\phi = \mathcal{U}(0,0.1\pi),\quad \theta = \mathcal{U}(0,0.1\pi),\quad \psi = \mathcal{N}(-\pi,\pi)
	\label{eq:distroexp}
\end{equation}
Where $ \mathcal{U}(a,b)$ is a uniform distribution between $a,b$ and $\mathcal{N}(\mu,\sigma^2)$ is a Gaussian distribution with mean $\mu$ and variance $\sigma^2$.

The parameters are chosen experimentally aiming to resemble common view points of a person standing in the room.

\subsection{Experiment I}

In order to evaluate $\mathcal{H}_1$ a model is trained in $S$ and applied in $T$. Three objects are investigated namely the \textit{Square-Gate}, \textit{Round-Gate} and \textit{Person.} The source environment is \textit{Basement}, the target domain is \textit{Daylight.} The training set consists of 20 000 samples, where for every batch of 2000 samples the objects are rearranged in the room. The test set consists of 1000 samples and fixed object arrangement. The view points are samples from the distributions described in \Cref{eq:distroexp}

\subsection{Experiment II}

In order to evaluate $\mathcal{H}_2$ a model is trained in $S$ and applied in $T$. Three objects are investigated namely the \textit{Square-Gate}, \textit{Round-Gate} and \textit{Person.}  The training set consists of 20 000 samples, where for every batch of 2000 samples the objects are rearranged in the room. The test set consists of 1000 samples and fixed object arrangement. The training set is generated by replacing the background with images samples from the Pascal VOC dataset. The test set is taken in \textit{Daylight} Environment. The view points are samples from the distributions described in \Cref{eq:distroexp}

\subsection{Experiment III}

In order to evaluate $\mathcal{H}_3$ the individual domain properties and their incorporation in the data generation process are studied. Each property is compared in terms of specialization and generalization. That is the property are varied between three configurations: (1) resembling the target domain, (2) generalizing across domains, (3) disabled (if applicable).

The test set is generated in the environment \textit{IROS}, using the quad-rotor model. In total 1000 images are sampled taken from one trajectory. The post-processing applies:
\begin{enumerate}
	\item Lense distortion
	\item Chromatic Abberration
	\item Motion Blur
	\item Out-of-Focus Blur
	\item Exposure
\end{enumerate}

\subsection{Experiment IV}

In order to evaluate $\mathcal{H}_4$ the individual domain properties are measured on the target domain and incorporated in the training set.


\section{Results}
\label{sec:training:results}
\begin{table}[htbp]
	\caption{Results of varying training sets on TODO}
	\small
	\begin{tabular}{|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{3cm}|p{2cm}|}
		\hline
		& General &  & Specific & General + Specific & Inactive \\ \hline
		Scene & Real Backgrounds & \textit{Basement, Daylight} & \textit{IROS2018} & Real Backgrounds, Synth. Environments & - \\ \hline
		&  &  &  &  & - \\ \hline
		Camera Placement & Heuristic Selection & Fitted on target set &  & Heuristic Selection + Target Set &  \\ \hline
		&  &  &  &  &  \\ \hline
		Lens Distortion & Varying parameters &  &  &  &  \\ \hline
		&  &  &  &  &  \\ \hline
		Chromatic Aberration &  &  &  &  &  \\ \hline
		Motion Noise &  &  &  &  &  \\ \hline
		Out-of-Focus Blur &  &  &  &  &  \\ \hline
		Exposure &  &  &  &  &  \\ \hline
		Sensor Noise &  &  &  &  &  \\ \hline
		Brightness &  &  &  &  &  \\ \hline
		Grayscale &  &  &  &  &  \\ \hline
		Histogram &  &  &  &  &  \\ \hline
		Flip &  &  &  &  &  \\ \hline
	\end{tabular}
	\label{genvsspec}
\end{table}


\section{Discussion}
\label{sec:training:discussion}

\section{Conclusion}
\label{sec:training:conclusion}


