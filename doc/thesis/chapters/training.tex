\chapter{Learning from Synthetic Data}
\label{sec:training}

Supervised machine learning relies on the assumption that a hypothesis $h$ can be learned from a limited set of samples $X$ with their corresponding set of labels $Y$. The goal is to learn $h$ from a source set ${X_{s},Y_{s}}$ such that it can be applied on a target set of unknown examples $X_{t}$. Usually this is evaluated by splitting the labeled set of ${X,Y}$ in a training set ${X_{train},Y_{train}}$ and test set ${X_{test},Y_{test}}$. The concept is learned using the full information of ${X_{train},Y_{train}}$ and applied on the ${X_{test},Y_{test}}$. By evaluating the performance metric $m_A$ on ${X_{train},Y_{train}}$ information about the representation strength of $h$ can be inferred. The value of the performance metric $m$ on ${X_{test},Y_{test}}$ gives an estimate of the performance of $h$ on $X_{t}$. Comparing $\epsilon_A$ and $\epsilon$ allows to evaluate whether a suitable model and suitable training data was chosen or is available.
			
This basic supervised machine learning setting relies on the assumption that ${X_s},{Y_s}$ follow the same i.i.d distribution in domain $D_s$ as $X_t$. However, this can not be assumed when learning from synthetic data. As the data is artificially created it will share properties with the real data but only to the extent that they can be modeled during creation. For example a graphic engine can not fully capture visual properties of all materials and light sources. When $D_s$ only shares a subset of properties with the target domain $D_t$ this is also referred to as a domain shift scenario.

Learning models in such an environment is concerned by the field of Transfer Learning. The goal is to find a concept $h$ that performs best in the target domain $D_t$. Similar to $m_A$ and $m$ the performance $m_s$ in $D_s$ and the performance $m_t$ in $D_t$ can be used to evaluate whether a suitable model/dataset has been chosen/is available.

The task of transfer learning can be summarized in:

$$
\text{arg}\max\limits_{h,D_s} m_t
$$

The goal is to create source domain $D_s$ and find a concept $h$ that maximizes the performance in the target domain $m_t$. The relevant question to be investigated in this chapter is the following:

\begin{center}
	\item How can a detection model for wire frame objects be learned from synthetic data?
\end{center}



\todo{The argument can be that because we want a realistic dataset we remove certain angles.}


\section{Background}

Domain Adaption/Synthetic Data:
 \cite{Chen2018c}, \cite{Xu2017}
\cite{Tremblay2018a} ,\cite{Inoue},\cite{Peng},
\cite{Rozantsev},  \cite{Le}, \cite{Liu2017}, \cite{Peng2017}

Incorporating Camera Effects:
\cite{Carlson2018},\cite{Vass}

Image Augmentation
 \cite{Bai2017},
 
\section{Hypothesis}

\todo{It should be possible to learn a model from synthetic data, incorporating background and camera effects should improve performance}

 
\section{Experiments}
\todo{Mesaure similarity between generated set and real set: label distribution (h,w, location, angles), domain(?)}
\todo{Compare performance from synthetic to real data}
\todo{How important is context in the training data?}
\todo{How much is the influence of camera effects?}

\section{Conclusion}

%As illustrated in the introduction the amount of training data is limited. This is critical for the performance of machine learning methods as they heavily depend on the available training data.



%Limited data can happen in multiple ways:
%\begin{enumerate}
%	\item The amount of data is too small thus the parameters can not be tuned effectively. Subsequently the model underfits or overfits the training data.
%	
%	\item The captured variety in the data set is limited. If the labeled examples do not sufficiently represent the objects in the real world, the model will fail in the wild.
%	
%	\item There is a domain shift between the training data and the test data.
%\end{enumerate}

%
%\section{Summaries}
%\subsection{Modeling Camera Effects to Improve Deep Vision for Real and Synthetic Data\cite{Carlson2018}}
%\begin{itemize}
%	\item heaps of references in how camera properties influence object detectors
%	\item introduces image augmentation pipeline based on physical model: chromatic aberration, blur, exposure, noise and color shift
%	\item input parameters are modelled by hand, then randomly selected within "realistic" range
%	\item no lense distortion
%	\item method seem to benefit for small objects and when oversaturation applies due to camera effects
%\end{itemize}
%\subsection{Domain Randomization\cite{Tremblay2018a}}
%\begin{itemize}
%	\item uses ue4 engine with custom plugin
%	\item model is supposed to learn independent of domain
%	\item textures are randomly applied
%	\item object models are randomly selected
%	\item approach cant capture patterns like how cars are parked (similar could happend when randomly placing the gates)
%	\item Tuning lower layers on simulated data fine tuning on real data
%	\item Effects of DR-parameters: light augmentation 0.1\%, light placement 10, \% texture 4\%, data augmentation 1.6\%, flying distractors 1.1 \%
%	\item pretraining on imagenet helps a lot, freezing weights when training on synthetic data does not help
%\end{itemize}