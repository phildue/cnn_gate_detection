\chapter{Transfer Learning}
\label{sec:training}

Supervised machine learning relies on the assumption that a hypothesis $h$ can be learned from a limited set of samples $X$ with their corresponding set of labels $Y$. The goal is to learn $h$ from a source set $D_s = \{X_{s},Y_{s}\}$ such that it can be applied on a target set of unknown examples $D_t = \{X_{t}\}$. Usually this is evaluated by splitting the labelled set of $D_s$ in a training set $D_{train}$ and test set $D_{test}$. The concept is learned using the full information of $D_{train}$ and applied on the $D_{test}$. By evaluating the performance metric $m_A$ on $D_{train}$ information about the representation strength of $h$ can be inferred. The value of the performance metric $m$ on $D_{test}$ gives an estimate of the performance of $h$ on $D_t$. Comparing $m_A$ and $m$ allows to evaluate whether model and dataset are suitable for the task.
			
This basic supervised machine learning setting relies on the assumption that $D_s$ follows the same i.i.d distribution as $D_t$. However, this can not be assumed for the application of this thesis. As most of the data is artificially created it will share properties with the real data but only to the extent that they can be modelled during creation. For example a graphic engine can not fully capture visual properties of all materials and light sources. Furthermore, is this model developed for application in locations with unknown light conditions and object variances that can be significantly different to the ones used at training time. When the source domain $S$ only shares a subset of properties with the target domain $T$ this is also referred to as a domain shift scenario. 


Learning models in such an environment is concerned by the field of Transfer Learning. The goal is to find a concept $h$ that performs best in the target domain $T$. Similar to $m_A$ and $m$ the performance $m_s$ in $S$ and the performance $m_t$ in $T$ can be used to evaluate whether a suitable model/dataset has been chosen/is available.

The task of transfer learning can be summarized in:

$$
\text{arg}\max\limits_{h,S} m_t
$$

The goal is to create source domain $D_s$ and find a concept $h$ that maximizes the performance in the target domain $m_t$. This formulation yields two levels at which the domain shift problem can be addressed: (1) focusing on generating data or (2) focusing on finding a hypothesis that maximizes the performance.

In this thesis two types of domain shifts are considered: (1) A semi-supervised domain shift between synthetic data and real images. This means $D_s$ is available in large quantity, namely it is generated by a graphical engine. For $D_t$ a considerably smaller amount of labels is available \todo{describe real datasets}. 
(2) An unsupervised domain shift when environment conditions at test time can be significantly different to the ones at training time. In this case information about $D_t$ is only available as graphical models of the object of interest and several images of the domain. \todo{double check is this really an unsupervised domain shift since we dont even know X}

The conducted research is limited to \ac{CNN}-based object detection as they are investigated in this thesis. The chapter focuses on domain adaption while the exact model is described in \autoref{sec:object_detection}.

The relevant question to be investigated in this chapter is the following:

\begin{center}
	\textbf{How can data be generated to train a model for wire frame object detection?}
	\textbf{How can the model be made robust against domain shifts?}
\end{center}

The first question will be answered by choosing a state of the art method for object detection and training it with varying properties in $D_s$. By evaluating $m_s$ on synthetic data and $m_t$ on real images the influence of these properties will be measured.  

The second question will be answered by simulating a domain with synthetic data. That is the model will be trained in a room with significant different environment properties $D_s$ from the room it is tested in $D_t$.

The remaining parts of this chapter are structured as follows: \autoref{sec:training:related} discusses relevant related work. Based on the gained insights \autoref{sec:training:hypothesis} formulates several hypotheses to be investigated. \autoref{sec:training:experiments} outlines the experiments conducted to evaluate the formulated hypotheses. \autoref{sec:training:results} describes the obtained results. \autoref{sec:training:conclusion} discusses the results and answers the research question.

\section{Related Work}
\label{sec:training:related}


\subsection{Domain Adaptation}

Domain Adaptation techniques focus on taking the domain shift into account when training a model.

\subsection{Domain Randomization}

\ac{DR} was initially introduced by \cite{Tobin2017} for object localization. In contrast to domain adaptation approaches the method does not try to model the domain shift. Instead large variability in the source domain shall enable the model to learn a robust representation that is domain agnostic. \cite{Tremblay2018a} extends the approach to object detection.

The data is generated by a graphical engine. The following properties of the scene and the object are varied \cite{Tremblay2018a}:

\begin{enumerate}
	\item number, type and texture of objects of interest
	\item number, types, colours, scales of irrelevant objects (distractors)
	\item background image
	\item camera pose
	\item light sources
	\item visibility of ground plane
\end{enumerate}

An advantage of \ac{DR} is its ease of implementation and use, no assumptions about the target distribution have to be made neither are samples or labels of the target domain required. However, the method can fail to capture important patterns if the randomization is too strong. For example the movement pattern of a \ac{MAV} is ignored when placing the camera randomly.


\section{Image Augmentation}


Domain Adaption/Synthetic Data:
 \cite{Chen2018c}, \cite{Xu2017}
,\cite{Inoue},\cite{Peng},
\cite{Rozantsev},  \cite{Le}, \cite{Liu2017}, \cite{Peng2017}

Incorporating Camera Effects:
\cite{Carlson2018},\cite{Vass}

Image Augmentation
 \cite{Bai2017},
 
\section{Hypothesis}
\label{sec:training:hypothesis}

\begin{enumerate}
	\item[H1] The more similar the properties between $D_s$ and $D_t$ the higher $m_t$.
	\item[H2] The graphical rendering effect should be of minor impact compared to environment properties.
\end{enumerate}

\todo{The argument can be that because we want a realistic dataset we remove certain angles.}


\todo{It should be possible to learn a model from synthetic data, incorporating background and camera effects should improve performance}

\section{Experiments}
\label{sec:training:experiments}

List of properties: Lightning conditions, Background, Context, Motion Model, Visual effects (motion blur)

\todo{Mesaure similarity between generated set and real set: label distribution (h,w, location, angles), domain(?)}
\todo{Compare performance from synthetic to real data}
\todo{How important is context in the training data?}
\todo{How much is the influence of camera effects?}

\section{Results}
\label{sec:training:results}

\section{Conclusion}
\label{sec:training:conclusion}
%As illustrated in the introduction the amount of training data is limited. This is critical for the performance of machine learning methods as they heavily depend on the available training data.



%Limited data can happen in multiple ways:
%\begin{enumerate}
%	\item The amount of data is too small thus the parameters can not be tuned effectively. Subsequently the model underfits or overfits the training data.
%	
%	\item The captured variety in the data set is limited. If the labeled examples do not sufficiently represent the objects in the real world, the model will fail in the wild.
%	
%	\item There is a domain shift between the training data and the test data.
%\end{enumerate}

%
%\section{Summaries}
%\subsection{Modeling Camera Effects to Improve Deep Vision for Real and Synthetic Data\cite{Carlson2018}}
%\begin{itemize}
%	\item heaps of references in how camera properties influence object detectors
%	\item introduces image augmentation pipeline based on physical model: chromatic aberration, blur, exposure, noise and color shift
%	\item input parameters are modelled by hand, then randomly selected within "realistic" range
%	\item no lense distortion
%	\item method seem to benefit for small objects and when oversaturation applies due to camera effects
%\end{itemize}
%\subsection{Domain Randomization\cite{Tremblay2018a}}
%\begin{itemize}
%	\item uses ue4 engine with custom plugin
%	\item model is supposed to learn independent of domain
%	\item textures are randomly applied
%	\item object models are randomly selected
%	\item approach cant capture patterns like how cars are parked (similar could happend when randomly placing the gates)
%	\item Tuning lower layers on simulated data fine tuning on real data
%	\item Effects of DR-parameters: light augmentation 0.1\%, light placement 10, \% texture 4\%, data augmentation 1.6\%, flying distractors 1.1 \%
%	\item pretraining on imagenet helps a lot, freezing weights when training on synthetic data does not help
%\end{itemize}