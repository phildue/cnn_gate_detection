\chapter{Investigating the Trade-Off between Detection Performance and Inference Time}
\label{sec:tradeoff}

A major drawback of \acp{CNN} is their huge computational requirements. For example a state-of-the-art Computer Vision model \cite{He2015} requires 11.3 billion floating point operations \cite{Tschannen2017}. 

Hence, given a baseline performance $m_0$ the goal of speed optimization must be to reduce the inference time $t$ while keeping the performance of the faster model $m_1$ as close as possible to $m_0$. This chapter explores the optimal trade-off between $t$ and $m$.


The research question of this chapter is stated as:

\begin{center}
	\textbf{What are the trade-off's between detection performance $m$ and inference time $t$ when a detection model is integrated on a embedded computing platform?}
\end{center}

The question is answered on a theoretical level by using the total number of \ac{Multiply-Adds} $N_O$ as an indication for the inference time of the model. However, as also stated by \todoref{others} $N_O$ is not necessarily directly related to $t$. On a computing platform $t$ also depends on:

\begin{enumerate}
	\item whether several operations can be executed in parallel,
	\item the memory usage of the operations, the kind of operation e.g. floating point or integer
	\item the particular low level implementation of the model
\end{enumerate} 

Hence, in addition to $N_0$ also the actual inference time of the model is measured on a particular computing platform.

The chosen hardware is a Jevois Smart Camera \todoref{jevois}. The platform is developed for vision applications and provides a 4 Core CPU, as well as a small GPU \todo{more info}. That's why it is perfectly suitable for integrating in lightweight \acp{MAV} or other robotic applications.

The rest of the chapter is organized as follows: \autoref{sec:tradeoff:related} discusses relevant related work. Based on the gained insights \autoref{sec:tradeoff:hypothesis} formulates several hypotheses to be investigated. \autoref{sec:tradeoff:experiments} outlines the experiments conducted to evaluate the formulated hypotheses. \autoref{sec:tradeoff:results} describes the obtained results. \autoref{sec:tradeoff:conclusion} discusses the results and answers the research question.


\section{Related Work}
\label{sec:tradeoff:related_work}

In recent years a lot of research has been conducted to reduce the inference time of \acp{CNN}. The publications address different levels for optimization:
\begin{enumerate}
	\item \textbf{Conceptual Level}
	\item \textbf{Architectural Level}
	\item \textbf{Operational Level}
\end{enumerate}

\section{Conceptual Level}
 On a conceptual level authors aimed to incorporate more steps of the object detection pipeline into one model to share computational load and thus reduce inference time.
 
 Overfeat\todoref{overfeat}, one the first \ac{CNN}-based object detectors ran a \ac{CNN} in sliding window manner across the image. As this led to redundant operations for feature extraction quickly two-stage approaches evolved. The consequent publications of R-CNN, Fast-RCNN and Faster-RCNN proposed a region proposal network that extracts features and proposes possible object locations, followed by a classification network that reuses the extracted features for classification. Thereby not only the number of regions that where classified was reduced but also the extracted features could be reused efficiently.
 
 Yolo and SSD proposed to combine the whole pipeline into one model. Although, this led to a bit of loss in performance, the inference time could be reduced significantly. The aforementioned models are further described in \autoref{sec:object_detection:related}.
 
 Using Time domain:
 \cite{Chen2018}
 
 \todo{put somewhere the overview of performance vs speed gained from object detection paper}
 
\section{Architectural Level}

Reducing the computational cost of \acp{CNN} has been addressed in two individual lines of research.

\subsection{Architectural Blocks}

\cite{YoungwanLee} and \cite{Zagoruyko2016} showed the performance of thin and deep architectures like \textit{ResNet} with more than 100 layers can equally be achieved by wider but shallower networks. At the same time the proposed \textit{Wide Residual Networks} use less parameters and can be executed more efficiently.

\textit{DenseNet} \cite{Huang2016} proposes the use of dense connections in \acp{CNN}. Thereby the input of each convolutional layer does not only consist if its direct previous layer but of a concatenation of the activations of all its previous layers. This enables feature reuse  and thus the reduction of the total amount of parameters \todo{is that really true since we need weights for much more filters}.

\textit{MobileNet} \cite{Howard2017} and \textit{QuickNet} \cite{Ghosh2017} make extensive use of \acp{DSC}. \acp{DSC} replace the original 3D-convolution by several 2D-convolutions followed by a pointwise convolution. \autoref{fig:dsc} illustrates the concept.

\textit{MobileNetV2} \cite{Sandler2018} further includes linear bottlenecks to reduce the total number of operations.

\cite{Zhang2017a} addresses the computational costs of pointwise convolutions. Instead of applying a pointwise convolution on the whole input volume, group convolutions are applied on by dividing the channels in subsets. These channels are shuffled to enable cross-channel information propagation. 

\subsection{Knowledge Distillation}

\todo{Knowledge Distillation}

\section{Operational Level}
Operational Level - Quantization:
\cite{TripathiSanDiego}, 



\todo{We choose one/two of the above because trying everything is a bit too much. So which one and why?}

\section{Experiments}

\todo{Evaluate effects on performance and accuracy}

\section{Conclusion}